{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import wandb\n",
    "import yaml\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from training import train, evaluate\n",
    "from config import Config\n",
    "from getters import *\n",
    "from models import *\n",
    "\n",
    "from utils import find_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def make_model(name, **kwargs):\n",
    "    Model = get_model(name)\n",
    "    return Model(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expe_name = 'copy_hadamssm_2'\n",
    "task2sweep = {'copy_hadamssm': 'test',\n",
    "              'copy_bjorck': 'sweep_ucvrt4w4',\n",
    "              'copy_projunn': 'sweep_a4d2sihj',\n",
    "             'pmnist_bjorck': 'sweep_bgckd728',\n",
    "             'smnist': 'sweep_a3af3t6v',\n",
    "             'ptb_bjorck': 'sweep_g0tjzjy7',\n",
    "}\n",
    "\n",
    "short2fullname = {\n",
    "    'copy_hadamssm_2': 'hadamSSM_copy',\n",
    "    'copy_bjorck_5' : 'fresh-sweep-19_qeh5ezkj',\n",
    "    'copy_bjorck_6' : 'rich-sweep-25_eihml4ya',\n",
    "    'copy_bjorck_8' : 'woven-sweep-33_dfdq24vp',\n",
    "    'copy_projunn_5' : 'sweet-sweep-16_fvzfdbx1',\n",
    "    'copy_projunn_6' : 'rose-sweep-22_e3ftelec',\n",
    "    'copy_projunn_8' : 'resilient-sweep-32_fze6523n',\n",
    "    'pmnist_bjorck_3' : 'rare-sweep-1_uyv83nl9',\n",
    "    'pmnist_bjorck_4' : 'rare4-sweep-1_uyv83nl9',\n",
    "    'pmnist_bjorck_5' : 'fresh-sweep-2_c9nwwvj4',\n",
    "    'pmnist_bjorck_6' : 'skilled-sweep-3_1pi0p71d',\n",
    "    'pmnist_bjorck_8' : 'denim-sweep-4_27qjq5oa',\n",
    "    'smnist_bjorck_3' : 'cerulean-sweep-1_1k08ik92',\n",
    "    'smnist_bjorck_4' : 'sleek-sweep-2_275j9jq7',\n",
    "    'smnist_bjorck_5' : 'wandering-sweep-3_1tbbz1et',\n",
    "    'smnist_bjorck_6' : 'distinctive-sweep-4_c0gyg62r',\n",
    "    'smnist_bjorck_8' : 'rich-sweep-5_ypocgzgv',\n",
    "    'ptb_bjorck_3' : 'decent-sweep-1_5tay03df',\n",
    "    'ptb_bjorck_4' : 'earnest-sweep-2_rb6cg3gh',\n",
    "    'ptb_bjorck_5' : 'eternal-sweep-3_5vss2or6',\n",
    "    'ptb_bjorck_6' : 'polished-sweep-4_qa3yj567',\n",
    "    'ptb_bjorck_8' : 'eager-sweep-5_whv7y4xu',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_task_value(expe_name,task2value):\n",
    "    value = None\n",
    "    for kk in task2value.keys():\n",
    "        if kk in expe_name:\n",
    "            value = task2value[kk]\n",
    "            break\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path2yaml = {'copy': 'copy_task',\n",
    "             'pmnist': 'permuted_mnist',\n",
    "             'smnist': 'seq_mnist',\n",
    "             'ptb': 'ptb',}\n",
    "\n",
    "config_path = get_task_value(expe_name,path2yaml)\n",
    "print(config_path)\n",
    "\n",
    "sweep_name = get_task_value(expe_name,task2sweep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file_path = \"config/\"+config_path+\"/\"+short2fullname[expe_name]+\".yaml\"\n",
    "weight_file_path = \"results/\"+sweep_name+\"_\"+short2fullname[expe_name]+\".pth\"\n",
    "result_file_path = \"results/\"+sweep_name+\"_\"+short2fullname[expe_name]+\".txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file_path = \"config/\"+config_path+\"/\"+\"copy_task_hadamSSM_config_test.yaml\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu' #'cpu' #'cpu' #'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verification of model load an performance with FP activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "conf_file = find_file(config_file_path)\n",
    "config = Config(conf_file=conf_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.model['num_bits_feat'] = None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = make_model(**config.model).to(device)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(weight_file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(config.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset = get_dataset(config.dataset['name'])\n",
    "\n",
    "dataset = Dataset(**config.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if dataset.test_ds is not None:\n",
    "    test_batch_size = dataset.te_size // 10\n",
    "    test_ds = dataset.test_ds\n",
    "else:\n",
    "    test_batch_size = dataset.va_size // 10\n",
    "    test_ds = dataset.val_ds\n",
    "stat_test = evaluate(test_ds, test_batch_size, model, loss_fn=config.train['loss_fn'], metrics=config.train['metrics'], kind='test', torch_device=device)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stat_test)\n",
    "#ref  {'test_loss': 9.112985571846366e-05}   #in cuda {'test_loss': 0.00011134522355860099} ?????\n",
    "# open and print result_file_path\n",
    "with open(result_file_path, 'r') as f:\n",
    "    print(f.read())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare PTQ of activations:\n",
    "Compute max value of activation on training set\n",
    "\n",
    "TO BE DONE ONCE ONLY => CELLS IN MARKDOWN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.load(weight_file_path)\n",
    "new_weights = {}\n",
    "new_weights[\"input_layer.weight\"] = model.input_layer.qweight  #NO_PARAM model.input_layer.weight\n",
    "new_weights['recurrent_layer.weight'] = model.recurrent_layer.weight\n",
    "if 'activation.b' in weights:\n",
    "    new_weights['activation.b'] = weights['activation.b']\n",
    "if 'recurrent_layer.bias' in weights:\n",
    "    new_weights['recurrent_layer.bias'] = weights['recurrent_layer.bias']\n",
    "#new_weights['output_layer.weight'] = weights['output_layer.weight']\n",
    "\n",
    "for kk in weights.keys():\n",
    "    if (kk not in new_weights) and ('input_layer' not in kk) and ('recurrent_layer' not in kk):\n",
    "        new_weights[kk] = weights[kk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(torch.unique(new_weights[\"input_layer.weight\"]).numpy()))\n",
    "print(list(torch.unique(new_weights[\"recurrent_layer.weight\"]).detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_scaled = copy.deepcopy(config)\n",
    "\n",
    "config_scaled.model['num_bits']  = None\n",
    "config_scaled.model['num_bits_feat'] = None\n",
    "config_scaled.model['name'] = 'QSSM'\n",
    "#config_scaled.model['bias'] = False\n",
    "if 'activation.b' in new_weights:\n",
    "    config_scaled.model['activation'] = layers.modrelu(**config_scaled.model['activation_config'])\n",
    "else:\n",
    "    config_scaled.model['activation'] = torch.nn.ReLU()\n",
    "config_scaled.model['num_bits_feat'] = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_stat = 'cuda'\n",
    "model_stat = make_model(**config_scaled.model).to(device_stat)\n",
    "model_stat.load_state_dict(new_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute max_val on training set\n",
    "train_batch_size = dataset.tr_size // 1000\n",
    "stat_test = evaluate(dataset.train_ds, train_batch_size, model_stat, loss_fn=config.train['loss_fn'], metrics=config.train['metrics'], kind='test', torch_device=device_stat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"stats on training set\")\n",
    "print(stat_test)\n",
    "print(\"input\")\n",
    "print(model_stat.quant_input.stat_max_absolute)\n",
    "print(\"feat\")\n",
    "print(model_stat.quant_feat.stat_max_absolute)\n",
    "\n",
    "#stats on training set\n",
    "#input\n",
    "#1.0\n",
    "#feat\n",
    "#19.80474853515625"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stat_test = evaluate(test_ds, test_batch_size, model_stat, loss_fn=config.train['loss_fn'], metrics=config.train['metrics'], kind='test', torch_device=device_stat)\n",
    "#print(stat_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expe_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "task2input_max_absolute = {'copy': 2.0,\n",
    "             'pmnist': 1.0,\n",
    "             'smnist': 1.0,\n",
    "             'ptb': 2.0,}\n",
    "\n",
    "short2feat_max_absolute  = {\n",
    "    'copy_hadamssm_2': 170.156494140625, #21.454788208007812,\n",
    "    'copy_bjorck_5' : 24.19596290588379,\n",
    "    'copy_bjorck_6' : 42.483619689941406,\n",
    "    'copy_bjorck_8' : 30.833498001098633, #65.3037109375,\n",
    "    'copy_projunn_5' : 'sweet-sweep-16_fvzfdbx1',\n",
    "    'copy_projunn_6' : 1902.8502197265625,\n",
    "    'copy_projunn_8' : 219.551513671875,\n",
    "    'pmnist_bjorck_3' : 22.018991470336914,\n",
    "    'pmnist_bjorck_4' : 26.795150756835938,\n",
    "    'pmnist_bjorck_5' : 25.203166961669922,\n",
    "    'pmnist_bjorck_6' : 25.342670440673828,\n",
    "    'pmnist_bjorck_8' : 26.046796798706055,\n",
    "    'smnist_bjorck_3' : 'cerulean-sweep-1_1k08ik92',\n",
    "    'smnist_bjorck_4' : 17.66891860961914,\n",
    "    'smnist_bjorck_5' : 29.19618034362793,\n",
    "    'smnist_bjorck_6' : 30.9416561126709,\n",
    "    'smnist_bjorck_8' : 30.775388717651367,\n",
    "    'ptb_bjorck_3' : 'decent-sweep-1_5tay03df',\n",
    "    'ptb_bjorck_4' : 30.078453063964844,\n",
    "    'ptb_bjorck_5' : 6.935123443603516,\n",
    "    'ptb_bjorck_6' : 6.048137187957764,\n",
    "    'ptb_bjorck_8' : 9.994182586669922,\n",
    "}\n",
    "\n",
    "task2prescale = {'copy': 1.0,\n",
    "             'pmnist': 255.0/256.0,\n",
    "             'smnist': 255.0/256.0,\n",
    "             'ptb': 1.0,}\n",
    "\n",
    "task2shiftinput = {'copy': 0,\n",
    "             'pmnist': 8,\n",
    "             'smnist': 8,\n",
    "             'ptb':0,}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "saved_quant_input_max_absolute = get_task_value(expe_name,task2input_max_absolute) #short2input_max_absolute[expe_name] #2. # 2.  ## set it twice to get 1 as max\n",
    "saved_quant_feat_max_absolute = short2feat_max_absolute[expe_name] #19.80474853515625   #32 #\n",
    "\n",
    "input_pre_scaling_factor = get_task_value(expe_name,task2prescale)\n",
    "#input_pre_scaling_factor = 1.0 # to get true Int\n",
    "#input_pre_scaling_factor = 255.0/256.0\n",
    "shift_input = get_task_value(expe_name,task2shiftinput)\n",
    "#shift_input = 0\n",
    "#shift_input = 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(saved_quant_input_max_absolute)\n",
    "print(saved_quant_feat_max_absolute)\n",
    "print(input_pre_scaling_factor)\n",
    "print(shift_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get $\\alpha_W$ and $\\alpha_U$ values\n",
    "\n",
    "TO BE DONE ONCE ONLY => CELLS IN MARKDOWN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.recurrent_layer.parametrizations.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''model.recurrent_layer.parametrizations.weight.original\n",
    "aa = model.recurrent_layer.parametrizations.weight[0](model.recurrent_layer.parametrizations.weight.original)\n",
    "model.recurrent_layer.parametrizations.weight[-2](aa)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeParametrization(params,originaW,until_step=2):\n",
    "    for kk in range(until_step):\n",
    "        originaW = params[kk](originaW)\n",
    "    return originaW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get floating point weights\n",
    "import torch.nn.utils.parametrize as parametrize\n",
    "\n",
    "print(model.recurrent_layer.parametrizations.weight)\n",
    "if len(model.recurrent_layer.parametrizations.weight) > 1: #1 is only Quantize, SpectralLinear has 3 paramtrizations\n",
    "    Wfp = computeParametrization(model.recurrent_layer.parametrizations.weight,\n",
    "                                 model.recurrent_layer.parametrizations.weight.original,\n",
    "                                 until_step=2) #model.recurrent_layer.parametrizations.weight[-2](model.recurrent_layer.parametrizations.weight.original)\n",
    "else:\n",
    "    Wfp = model.recurrent_layer.weight\n",
    "print(\"W\",Wfp.min().detach().numpy(),Wfp.max().detach().numpy(),np.max(np.abs(Wfp.detach().numpy())))\n",
    "if parametrize.is_parametrized(model.input_layer):\n",
    "    print(model.input_layer.parametrizations.weight)\n",
    "    if len(model.input_layer.parametrizations.weight) > 1:\n",
    "        Ufp = computeParametrization(model.input_layer.parametrizations.weight,\n",
    "                                    model.input_layer.parametrizations.weight.original,\n",
    "                                    until_step=2) #model.input_layer.parametrizations.weight[-2](model.input_layer.parametrizations.weight.original)\n",
    "    else:\n",
    "        Ufp = model.input_layer.parametrizations.weight.original\n",
    "else:\n",
    "    Ufp = model.input_layer.qweight #QLinear implementation of Armand \n",
    "print(\"U\",Ufp.min().detach().numpy(),Ufp.max().detach().numpy(),np.max(np.abs(Ufp.detach().numpy())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "short2alpha_w  = {\n",
    "    'copy_hadamssm_2': 0.25,\n",
    "    'copy_bjorck_5' : 0.26512054,\n",
    "    'copy_bjorck_6' : 0.28184956,\n",
    "    'copy_bjorck_8' : 0.26092893, #0.26838166,\n",
    "    'copy_projunn_5' : 'sweet-sweep-16_fvzfdbx1',\n",
    "    'copy_projunn_6' : 0.9988483,\n",
    "    'copy_projunn_8' : 0.9998472,\n",
    "    'pmnist_bjorck_3' : 0.32128334,\n",
    "    'pmnist_bjorck_4' : 0.3660844,\n",
    "    'pmnist_bjorck_5' : 0.34444174,\n",
    "    'pmnist_bjorck_6' : 0.38659847,\n",
    "    'pmnist_bjorck_8' : 0.6006793,\n",
    "    'smnist_bjorck_3' : 'cerulean-sweep-1_1k08ik92',\n",
    "    'smnist_bjorck_4' : 0.43379927,\n",
    "    'smnist_bjorck_5' : 0.36560023,\n",
    "    'smnist_bjorck_6' : 0.40944064,\n",
    "    'smnist_bjorck_8' : 0.40728217,\n",
    "    'ptb_bjorck_3' : 'decent-sweep-1_5tay03df',\n",
    "    'ptb_bjorck_4' : 0.19523722,\n",
    "    'ptb_bjorck_5' : 0.23500842,\n",
    "    'ptb_bjorck_6' : 0.16609415,\n",
    "    'ptb_bjorck_8' : 0.48273855,\n",
    "}\n",
    "\n",
    "short2alpha_u  = {\n",
    "    'copy_hadamssm_2': 5.5423894,\n",
    "    'copy_bjorck_5' : 2.2110918,\n",
    "    'copy_bjorck_6' : 2.2872915,\n",
    "    'copy_bjorck_8' : 1.733867, #1.9520738,\n",
    "    'copy_projunn_5' : 'sweet-sweep-16_fvzfdbx1',\n",
    "    'copy_projunn_6' : 7.8092656,\n",
    "    'copy_projunn_8' : 1.9943593,\n",
    "    'pmnist_bjorck_3' : 16.959444,\n",
    "    'pmnist_bjorck_4' : 12.96517,\n",
    "    'pmnist_bjorck_5' : 8.487175,\n",
    "    'pmnist_bjorck_6' : 10.693445,\n",
    "    'pmnist_bjorck_8' : 7.431476,\n",
    "    'smnist_bjorck_3' : 'cerulean-sweep-1_1k08ik92',\n",
    "    'smnist_bjorck_4' : 6.031232,\n",
    "    'smnist_bjorck_5' : 8.092017,\n",
    "    'smnist_bjorck_6' : 5.021599,\n",
    "    'smnist_bjorck_8' : 6.9251447,\n",
    "    'ptb_bjorck_3' : 'decent-sweep-1_5tay03df',\n",
    "    'ptb_bjorck_4' : 15.941487,\n",
    "    'ptb_bjorck_5' : 2.4994867,\n",
    "    'ptb_bjorck_6' : 2.4208195,\n",
    "    'ptb_bjorck_8' : 7.541183,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_alpha_u = short2alpha_u[expe_name] #np.max(np.abs([-0.6072238087654114, 0.607637882232666]))\n",
    "saved_alpha_w = short2alpha_w[expe_name] #nnp.max(np.abs([-0.24883395433425903, 0.24531713128089905]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(saved_alpha_u,saved_alpha_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rounding functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_values(vv, num_bits):\n",
    "    vv = vv*2**num_bits\n",
    "    assert torch.max(torch.abs(vv.round() - vv))<1e-4   # ?????\n",
    "    vv = vv.round()\n",
    "    vv = vv/2**num_bits\n",
    "    return vv\n",
    "\n",
    "def verify_round_max(vv, num_bits):\n",
    "    eval_numbits = 0\n",
    "    for ss in range(num_bits):\n",
    "        vv = vv*2\n",
    "        if torch.max(torch.abs(vv.round() - vv))<1e-5:\n",
    "            eval_numbits = ss+1\n",
    "            break\n",
    "    assert eval_numbits > 0\n",
    "    print(eval_numbits,\"max\",num_bits)\n",
    "    return eval_numbits\n",
    "    \n",
    "  \n",
    "def verify_and_round(vv, num_bits):  \n",
    "    eval_numbits = verify_round_max(vv, num_bits)\n",
    "    return round_values(vv, eval_numbits)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully Quantized RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.load(weight_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vv = (2**config.model['num_bits'])*model.recurrent_layer.weight/ saved_alpha_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.max(torch.abs(vv.round()-vv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(torch.unique(model.input_layer.qweight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_weights = {}\n",
    "requant_input_weight = round_values(model.input_layer.qweight/ saved_alpha_u, config.model['num_bits'])#NO_PARAM model.input_layer.weight\n",
    "new_weights[\"input_layer.weight\"] = requant_input_weight #model.input_layer.weight/ (saved_quant_input_max_absolute*saved_alpha_u)\n",
    "float_recurrent_weight = model.recurrent_layer.weight\n",
    "requant_recurrent_weight = round_values(model.recurrent_layer.weight/ saved_alpha_w, config.model['num_bits'])\n",
    "new_weights['recurrent_layer.weight'] = requant_recurrent_weight #model.recurrent_layer.weight / saved_alpha_w ## compensate alpha_w in hidden quantization\n",
    "#new_weights['recurrent_layer.weight'] = model.recurrent_layer.weight  ## compensate alpha_w in hidden quantization\n",
    "scaled_quant_feat_max_absolute  = saved_quant_feat_max_absolute/saved_alpha_u\n",
    "print(scaled_quant_feat_max_absolute) #saved_quant_feat_max_absolute*saved_alpha_w/(saved_quant_input_max_absolute*saved_alpha_u))\n",
    "print(\"scaled hidden quantiz\",scaled_quant_feat_max_absolute*saved_alpha_w)  #4.055123978191649\n",
    "#new_quant_feat_max_absolute  = scaled_quant_feat_max_absolute #8/saved_alpha_w #saved_alpha_w*saved_quant_feat_max_absolute/(saved_quant_input_max_absolute*saved_alpha_u)\n",
    "if 'activation.b' in weights:\n",
    "    new_weights['activation.b'] = weights['activation.b']/ saved_alpha_u\n",
    "    print(\"activation.b max abs\",torch.abs(new_weights['activation.b']).max().cpu().detach().numpy())\n",
    "if 'recurrent_layer.bias' in weights:\n",
    "    new_weights['recurrent_layer.bias'] = weights['recurrent_layer.bias']/ saved_alpha_u\n",
    "    print(\"'recurrent_layer.bias' max abs\",torch.abs(new_weights['recurrent_layer.bias']).max().cpu().detach().numpy())\n",
    "new_weights['output_layer.weight'] = saved_alpha_u*weights['output_layer.weight']/input_pre_scaling_factor\n",
    "\n",
    "for kk in weights.keys():\n",
    "    if (kk not in new_weights) and ('input_layer' not in kk) and ('recurrent_layer' not in kk):\n",
    "        new_weights[kk] = weights[kk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(saved_quant_feat_max_absolute,saved_alpha_u,saved_quant_feat_max_absolute/saved_alpha_u)\n",
    "print(scaled_quant_feat_max_absolute,saved_alpha_w,scaled_quant_feat_max_absolute*saved_alpha_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "short2scaled_feat_max_absolute  = {  # if possible power of two\n",
    "    'copy_hadamssm_2': 8.0,\n",
    "    'copy_bjorck_5' : 4.0,\n",
    "    'copy_bjorck_6' : 4.0,  \n",
    "    'copy_bjorck_8' : 4.0, #12.0, #4.417504669503896\n",
    "    'copy_projunn_5' : 'sweet-sweep-16_fvzfdbx1',\n",
    "    'copy_projunn_6' : 256.0,\n",
    "    'copy_projunn_8' : 128.0,\n",
    "    'pmnist_bjorck_3' : 1.0,    #0.20856624553910355\n",
    "    'pmnist_bjorck_4' : 1.0,    #0.7565875871836489\n",
    "    'pmnist_bjorck_5' : 1.0,\n",
    "    'pmnist_bjorck_6' : 1.0,\n",
    "    'pmnist_bjorck_8' : 2.0,\n",
    "    'smnist_bjorck_3' : 'cerulean-sweep-1_1k08ik92',\n",
    "    'smnist_bjorck_4' : 2.0,\n",
    "    'smnist_bjorck_5' : 2.0,\n",
    "    'smnist_bjorck_6' : 4.0,\n",
    "    'smnist_bjorck_8' : 2.0,\n",
    "    'ptb_bjorck_3' : 'decent-sweep-1_5tay03df',\n",
    "    'ptb_bjorck_4' : 1.0,\n",
    "    'ptb_bjorck_5' : 1.0,\n",
    "    'ptb_bjorck_6' : 1.0,\n",
    "    'ptb_bjorck_8' : 1.0,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_quant_feat_max_absolute  = short2scaled_feat_max_absolute[expe_name]  #saved_alpha_w*saved_quant_feat_max_absolute/(2*saved_alpha_u)\n",
    "\n",
    "new_bias_max_absolute = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check rounding of weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iww = 2**(config.model['num_bits']-1)*2*new_weights[\"input_layer.weight\"].unique()\n",
    "print(\"input \",torch.norm(iww.round() - iww))\n",
    "\n",
    "iww = 2**(config.model['num_bits']-1)*requant_recurrent_weight\n",
    "#print(iww)\n",
    "print(\"recurrent \",torch.norm(iww.round() - iww))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate with several bitwidth for activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(config.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "config_scaled = copy.deepcopy(config)\n",
    "\n",
    "config_scaled.model['num_bits']  = None\n",
    "config_scaled.model['num_bits_feat'] = None\n",
    "config_scaled.model['name'] = 'QSSM'\n",
    "#config_scaled.model['bias'] = False\n",
    "if 'activation.b' in new_weights:\n",
    "    config_scaled.model['activation'] = layers.modified4q_modrelu(**config_scaled.model['activation_config'])\n",
    "else:\n",
    "    config_scaled.model['activation'] = torch.nn.ReLU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from extra_layers import _Quantize_stats\n",
    "\n",
    "qb = None\n",
    "models = []\n",
    "for qb in [None,12]: #6,8,10,12]:\n",
    "    config_scaled.model['num_bits_feat'] = qb #10 #12\n",
    "    model_scaled = make_model(**config_scaled.model).to(device)\n",
    "    if qb is None:   # No rescaling of h with qb=None\n",
    "        new_weights['recurrent_layer.weight'] = float_recurrent_weight\n",
    "        #model_scaled.load_state_dict(new_weights_normalphaU)\n",
    "    else:\n",
    "        new_weights['recurrent_layer.weight'] = requant_recurrent_weight\n",
    "    model_scaled.load_state_dict(new_weights)\n",
    "    if qb is not None:      \n",
    "        model_scaled.quant_input.pre_scaling_factor = input_pre_scaling_factor     \n",
    "        model_scaled.quant_input.max_absolute = saved_quant_input_max_absolute #saved_quant_input_max_absolute  ## set it twice to get 1 as max\n",
    "        model_scaled.quant_feat.max_absolute = new_quant_feat_max_absolute       \n",
    "        model_scaled.quant_feat.pre_scaling_factor = saved_alpha_w     \n",
    "        #print(model_scaled.activation.b)\n",
    "        #print(quant_bias(model_scaled.activation.b))\n",
    "        if ('activation.b' in new_weights) or ('recurrent_layer.bias' in new_weights):\n",
    "            #quantize biases\n",
    "            quant_bias = _Quantize_stats(weight=None,num_bits= model_scaled.quant_feat.num_bits, max_absolute= new_bias_max_absolute)\n",
    "            if 'activation.b' in new_weights:\n",
    "                model_scaled.activation.b.data = quant_bias(model_scaled.activation.b) \n",
    "            if 'recurrent_layer.bias' in new_weights:\n",
    "                model_scaled.recurrent_layer.bias.data = quant_bias(model_scaled.recurrent_layer.bias)\n",
    "    models.append(model_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(iter(test_ds))\n",
    "x0 = x[None,:] #No batch_size [0:1]\n",
    "x0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_float = models[0]\n",
    "model_float.eval()\n",
    "model_float(x0.to(device))\n",
    "\n",
    "print(\"input\")\n",
    "#print(model_float.quant_input.stat_max_absolute)\n",
    "print(\"feat\")\n",
    "print(model_float.quant_feat.stat_max_absolute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.unique(model_float.input_layer.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scaled.quant_feat.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h0 = hidden_scaled[-1][0,220:230]\n",
    "h1 = h0*model_scaled.quant_feat.pre_scaling_factor\n",
    "h2 = h1/model_scaled.quant_feat.max_absolute\n",
    "h3 = h2*2**(model_scaled.quant_feat.num_bits-1)\n",
    "h4 = h3.round()\n",
    "h5 = h4/2**(model_scaled.quant_feat.num_bits-1)\n",
    "h6 = h5*model_scaled.quant_feat.max_absolute\n",
    "h7 = h6/model_scaled.quant_feat.pre_scaling_factor\n",
    "print(h0,\"\\n\",h1,\"\\n\",h2,\"\\n\",h3,\"\\n\",h4,\"\\n\",h5,\"\\n\",h6,\"\\n\",h7)\n",
    "print(torch.abs(h0-h7))\n",
    "print(model_scaled.quant_feat(hidden_scaled[-1][0,220:230])/model_scaled.quant_feat.pre_scaling_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "model_float = models[0]\n",
    "model_scaled = models[1]\n",
    "hidden = []\n",
    "hidden_scaled = []\n",
    "hidden_scaled_input = []\n",
    "hidden_scaled_quant = []\n",
    "hidden_scaled_rec = []\n",
    "for ii in range(1000):\n",
    "    print(\"input\",x0[0:1,ii])\n",
    "    print(\"qinput\",model_scaled.quant_input(x0[0:1,ii])/input_pre_scaling_factor)\n",
    "    #print(model.input_layer(x0[0:1,ii]))\n",
    "    #print(model_scaled.input_layer(x0[0:1,ii])*saved_quant_input_max_absolute*saved_alpha_u)\n",
    "    print(\"diff U.x\",torch.norm(model_float.input_layer(x0[0:1,ii])-model_scaled.input_layer(model_scaled.quant_input(x0[0:1,ii]))/input_pre_scaling_factor))\n",
    "    \n",
    "    if len(hidden)==0:\n",
    "        hidden.append(model_float.input_layer(x0[0:1,ii]))\n",
    "        hidden_scaled.append(model_scaled.input_layer(model_scaled.quant_input(x0[0:1,ii])))\n",
    "        #print(hidden_scaled[0])\n",
    "        #print(hidden_scaled[0]*2**(config.model['num_bits']))\n",
    "        hidden_scaled_quant.append(torch.zeros_like(hidden_scaled[0])) # fake\n",
    "        hidden_scaled_rec.append(torch.zeros_like(hidden_scaled[0])) # fake\n",
    "    else:\n",
    "        #print(\"hidden\",hidden[-1][0,0:10])\n",
    "        #print(\"hidden_rec\",model.recurrent_layer(hidden[-1])[0,0:10])\n",
    "        hidden.append(model_float.input_layer(x0[0:1,ii])+model_float.recurrent_layer(hidden[-1]))\n",
    "        #print(\"hidden_scaled\",hidden_scaled[-1][0,0:10])\n",
    "        #print(\"hidden_scaled\",hidden_scaled[-1][0,0:10]*saved_alpha_u/input_pre_scaling_factor)\n",
    "        hidden_scaled_quant.append(model_scaled.quant_feat(hidden_scaled[-1]))\n",
    "        #print(\"hidden_scaled_quant\",hidden_scaled_quant[-1][0,0:10])\n",
    "        #print(\"hidden_scaled_quant\",hidden_scaled_quant[-1][0,0:10]*saved_alpha_u/(input_pre_scaling_factor*model_scaled.quant_feat.pre_scaling_factor))\n",
    "        print(\"diff_hidden_scaled\",torch.norm(hidden[-2]-hidden_scaled[-1]/input_pre_scaling_factor))\n",
    "        print(\"diff_hidden_quant\",torch.norm(hidden[-2]-hidden_scaled_quant[-1]/(input_pre_scaling_factor*model_scaled.quant_feat.pre_scaling_factor)))\n",
    "        diff_hidden_scaled = (hidden_scaled_quant[-1]/model_scaled.quant_feat.pre_scaling_factor-hidden_scaled[-1])*saved_alpha_u/input_pre_scaling_factor\n",
    "        print(\"quant diff hidden\",torch.norm(diff_hidden_scaled).detach().numpy(),torch.min(torch.abs(diff_hidden_scaled)).detach().numpy(),torch.mean(torch.abs(diff_hidden_scaled)).detach().numpy(),torch.max(torch.abs(diff_hidden_scaled)).detach().numpy(),diff_hidden_scaled.shape)\n",
    "        #print(\"hidden_scaled_reccccc\",model.recurrent_layer(hidden_scaled_quant[-1]*saved_alpha_u/(input_pre_scaling_factor*model_scaled.quant_feat.pre_scaling_factor))[0,0:10])\n",
    "        #print(\"hidden_scaled_reccccc1\",model.recurrent_layer(hidden_scaled_quant[-1]/model_scaled.quant_feat.pre_scaling_factor)[0,0:10]*saved_alpha_u/input_pre_scaling_factor)\n",
    "        #print(\"hidden_scaled_rec\",model_scaled.recurrent_layer(hidden_scaled_quant[-1])[0,0:10]*saved_alpha_u/input_pre_scaling_factor)\n",
    "        #print(\"hidden_scaled_rec\",model_scaled.recurrent_layer(model_scaled.quant_feat(hidden_scaled[-1]))[0,0:10]*saved_alpha_u/input_pre_scaling_factor)\n",
    "        hidden_scaled.append(model_scaled.input_layer(model_scaled.quant_input(x0[0:1,ii]))+model_scaled.recurrent_layer(model_scaled.quant_feat(hidden_scaled[-1])))\n",
    "        hidden_scaled_rec.append(model_scaled.recurrent_layer(model_scaled.quant_feat(hidden_scaled[-1]))) \n",
    "    \n",
    "    hidden_scaled_input.append(model_scaled.input_layer(model_scaled.quant_input(x0[0:1,ii])))\n",
    "    #print(\"hidden_scaled_rec\",hidden_scaled_rec[-1]*saved_alpha_u/input_pre_scaling_factor)\n",
    "    print(\"diff h_t\",torch.norm(hidden[-1]-hidden_scaled[-1]/input_pre_scaling_factor))\n",
    "    print(\"h_t, min,mean,max,norm\",hidden[-1].min().detach().numpy(),hidden[-1].mean().detach().numpy(),hidden[-1].max().detach().numpy(),torch.norm(hidden[-1]).detach().numpy())\n",
    "    #print(\"diff h_t\",hidden[-1][0,0:10])\n",
    "    #print(\"diff h_t\",hidden_scaled[-1][0,0:10])\n",
    "    #print(\"diff h_t\",hidden_scaled[-1][0,0:10]*saved_alpha_u/input_pre_scaling_factor)\n",
    "    #if ii>2:\n",
    "    #    aaa\n",
    "    if 'activation.b' in new_weights:\n",
    "        print(\"diff h_t+bias\",torch.norm(hidden[-1]+model.activation.b-(hidden_scaled[-1]+model_scaled.activation.b)*saved_alpha_u/input_pre_scaling_factor))\n",
    "    #NO ACTIV IN SSM hidden[-1] = model.activation(hidden[-1])\n",
    "    #NO ACTIV IN SSM hidden_scaled[-1] = model_scaled.activation(hidden_scaled[-1])\n",
    "    print(\"activ\",torch.norm(hidden[-1]-hidden_scaled[-1]/input_pre_scaling_factor),torch.norm(hidden[-1]-hidden_scaled[-1]/input_pre_scaling_factor)/torch.norm(hidden[-1]),torch.max(torch.abs(hidden[-1]-hidden_scaled[-1]/input_pre_scaling_factor)))\n",
    "    #print(\"activ value\",hidden[-1],hidden_scaled[-1]*saved_alpha_u/input_pre_scaling_factor)\n",
    "    #print(\"activ valueq\",hidden_scaled[-1],model_scaled.quant_feat(hidden_scaled[-1])/model_scaled.quant_feat.pre_scaling_factor)\n",
    "    print(\"activ quantif\",torch.norm(hidden[-1]-model_scaled.quant_feat(hidden_scaled[-1])/model_scaled.quant_feat.pre_scaling_factor/input_pre_scaling_factor))\n",
    "    '''if torch.max(torch.abs(hidden[-1]-model_scaled.quant_feat(hidden_scaled[-1])/model_scaled.quant_feat.pre_scaling_factor/input_pre_scaling_factor))>0.1:\n",
    "        print(hidden[-1][0,290:300])\n",
    "        print(hidden_scaled[-1][0,290:300]/input_pre_scaling_factor)\n",
    "        print(model_scaled.quant_feat(hidden_scaled[-1])[0,290:300]/model_scaled.quant_feat.pre_scaling_factor/input_pre_scaling_factor)\n",
    "        diff = hidden[-1]-model_scaled.quant_feat(hidden_scaled[-1])/model_scaled.quant_feat.pre_scaling_factor/input_pre_scaling_factor\n",
    "        print(torch.argmax(torch.abs(diff)))\n",
    "        print(\"diff\",diff[0,290:300],diff.min().detach().numpy(),diff.mean().detach().numpy(),diff.max().detach().numpy(),torch.norm(diff).detach().numpy())\n",
    "        break'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scaled.quant_feat.__dict__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from extra_layers import _Quantize_stats\n",
    "results = {}\n",
    "for qb in [None,12]:\n",
    "    config_scaled.model['num_bits_feat'] = qb #10 #12\n",
    "    model_scaled = make_model(**config_scaled.model).to(device)\n",
    "    if qb is None:   # No rescaling of h with qb=None\n",
    "        new_weights['recurrent_layer.weight'] = float_recurrent_weight\n",
    "        #model_scaled.load_state_dict(new_weights_normalphaU)\n",
    "    else:\n",
    "        new_weights['recurrent_layer.weight'] = requant_recurrent_weight\n",
    "    model_scaled.load_state_dict(new_weights)\n",
    "    if qb is not None:      \n",
    "        model_scaled.quant_input.pre_scaling_factor = input_pre_scaling_factor     \n",
    "        model_scaled.quant_input.max_absolute = saved_quant_input_max_absolute #saved_quant_input_max_absolute  ## set it twice to get 1 as max\n",
    "        model_scaled.quant_feat.max_absolute = new_quant_feat_max_absolute       \n",
    "        model_scaled.quant_feat.pre_scaling_factor = saved_alpha_w     \n",
    "        #print(model_scaled.activation.b)\n",
    "        #print(quant_bias(model_scaled.activation.b))\n",
    "        if ('activation.b' in new_weights) or ('recurrent_layer.bias' in new_weights):\n",
    "            #quantize biases\n",
    "            quant_bias = _Quantize_stats(weight=None,num_bits= model_scaled.quant_feat.num_bits, max_absolute= new_bias_max_absolute)\n",
    "            if 'activation.b' in new_weights:\n",
    "                model_scaled.activation.b.data = quant_bias(model_scaled.activation.b) \n",
    "            if 'recurrent_layer.bias' in new_weights:\n",
    "                model_scaled.recurrent_layer.bias.data = quant_bias(model_scaled.recurrent_layer.bias)\n",
    "    stat_test = evaluate(test_ds, test_batch_size, model_scaled, loss_fn=config.train['loss_fn'], metrics=config.train['metrics'], kind='test', torch_device=device)\n",
    "    print(qb, stat_test)\n",
    "    results[qb] =stat_test.copy()\n",
    "\n",
    "for q in results:\n",
    "    print(q,results[q])\n",
    "#save perf in txt file\n",
    "with open(\"results/\"+expe_name+\"_\"+sweep_name+\"_\"+short2fullname[expe_name]+\"_quant.txt\", \"w\") as f:\n",
    "    for q in results:\n",
    "        for kk in results[q]:\n",
    "            f.write(str(q)+\";\"+str(kk)+\";\"+str(results[q][kk])+\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verification of the intermediate computation: chek for quantization\n",
    "\n",
    "need to simulate the QRNN computation and save intermediate values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scaled.quant_feat.num_bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(iter(test_ds))\n",
    "x0 = x[None,:] #No batch_size [0:1]\n",
    "x0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'activation.b' in new_weights:\n",
    "    model.activation.b/ saved_alpha_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if 'activation.b' in new_weights:\n",
    "    model_scaled.activation.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_quant_input_max_absolute\n",
    "input_pre_scaling_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn.functional as F\n",
    "hidden = []\n",
    "hidden_scaled = []\n",
    "hidden_scaled_input = []\n",
    "hidden_scaled_quant = []\n",
    "hidden_scaled_rec = []\n",
    "for ii in range(1000):\n",
    "    print(\"input\",x0[0:1,ii])\n",
    "    print(\"qinput\",model_scaled.quant_input(x0[0:1,ii])/input_pre_scaling_factor)\n",
    "    #print(model.input_layer(x0[0:1,ii]))\n",
    "    #print(model_scaled.input_layer(x0[0:1,ii])*saved_quant_input_max_absolute*saved_alpha_u)\n",
    "    print(\"diff U.x\",torch.norm(model.input_layer(x0[0:1,ii])-model_scaled.input_layer(model_scaled.quant_input(x0[0:1,ii]))*saved_alpha_u/input_pre_scaling_factor))\n",
    "    \n",
    "    if len(hidden)==0:\n",
    "        if 'recurrent_layer.bias' in new_weights:\n",
    "                hidden.append(model.input_layer(x0[0:1,ii])+model.recurrent_layer.bias)\n",
    "                hidden_scaled.append(model_scaled.input_layer(model_scaled.quant_input(x0[0:1,ii]))+model_scaled.recurrent_layer.bias)\n",
    "                hidden_scaled_rec.append(torch.zeros_like(hidden_scaled[0])+model_scaled.recurrent_layer.bias) # fake\n",
    "        else:\n",
    "            hidden.append(model.input_layer(x0[0:1,ii]))\n",
    "            hidden_scaled.append(model_scaled.input_layer(model_scaled.quant_input(x0[0:1,ii])))\n",
    "            hidden_scaled_rec.append(torch.zeros_like(hidden_scaled[0])) # fake\n",
    "        #print(hidden_scaled[0])\n",
    "        #print(hidden_scaled[0]*2**(config.model['num_bits']))\n",
    "        hidden_scaled_quant.append(torch.zeros_like(hidden_scaled[0])) # fake\n",
    "    else:\n",
    "        #print(\"hidden\",hidden[-1][0,0:10])\n",
    "        #print(\"hidden_rec\",model.recurrent_layer(hidden[-1])[0,0:10])\n",
    "        hidden.append(model.input_layer(x0[0:1,ii])+model.recurrent_layer(hidden[-1]))\n",
    "        #print(\"hidden_scaled\",hidden_scaled[-1][0,0:10])\n",
    "        #print(\"hidden_scaled\",hidden_scaled[-1][0,0:10]*saved_alpha_u/input_pre_scaling_factor)\n",
    "        hidden_scaled_quant.append(model_scaled.quant_feat(hidden_scaled[-1]))\n",
    "        #print(\"hidden_scaled_quant\",hidden_scaled_quant[-1][0,0:10])\n",
    "        #print(\"hidden_scaled_quant\",hidden_scaled_quant[-1][0,0:10]*saved_alpha_u/(input_pre_scaling_factor*model_scaled.quant_feat.pre_scaling_factor))\n",
    "        print(\"diff_hidden_scaled\",torch.norm(hidden[-2]-hidden_scaled[-1]*saved_alpha_u/input_pre_scaling_factor))\n",
    "        print(\"diff_hidden_quant\",torch.norm(hidden[-2]-hidden_scaled_quant[-1]*saved_alpha_u/(input_pre_scaling_factor*model_scaled.quant_feat.pre_scaling_factor)))\n",
    "        diff_hidden_scaled = (hidden_scaled_quant[-1]/model_scaled.quant_feat.pre_scaling_factor-hidden_scaled[-1])*saved_alpha_u/input_pre_scaling_factor\n",
    "        print(\"quant diff hidden\",torch.norm(diff_hidden_scaled).detach().numpy(),torch.min(torch.abs(diff_hidden_scaled)).detach().numpy(),torch.mean(torch.abs(diff_hidden_scaled)).detach().numpy(),torch.max(torch.abs(diff_hidden_scaled)).detach().numpy(),diff_hidden_scaled.shape)\n",
    "        #print(\"hidden_scaled_reccccc\",model.recurrent_layer(hidden_scaled_quant[-1]*saved_alpha_u/(input_pre_scaling_factor*model_scaled.quant_feat.pre_scaling_factor))[0,0:10])\n",
    "        #print(\"hidden_scaled_reccccc1\",model.recurrent_layer(hidden_scaled_quant[-1]/model_scaled.quant_feat.pre_scaling_factor)[0,0:10]*saved_alpha_u/input_pre_scaling_factor)\n",
    "        #print(\"hidden_scaled_rec\",model_scaled.recurrent_layer(hidden_scaled_quant[-1])[0,0:10]*saved_alpha_u/input_pre_scaling_factor)\n",
    "        #print(\"hidden_scaled_rec\",model_scaled.recurrent_layer(model_scaled.quant_feat(hidden_scaled[-1]))[0,0:10]*saved_alpha_u/input_pre_scaling_factor)\n",
    "        hidden_scaled.append(model_scaled.input_layer(model_scaled.quant_input(x0[0:1,ii]))+model_scaled.recurrent_layer(model_scaled.quant_feat(hidden_scaled[-1])))\n",
    "        hidden_scaled_rec.append(model_scaled.recurrent_layer(model_scaled.quant_feat(hidden_scaled[-1]))) \n",
    "    \n",
    "    hidden_scaled_input.append(model_scaled.input_layer(model_scaled.quant_input(x0[0:1,ii])))\n",
    "    #print(\"hidden_scaled_rec\",hidden_scaled_rec[-1]*saved_alpha_u/input_pre_scaling_factor)\n",
    "    print(\"diff h_t\",torch.norm(hidden[-1]-hidden_scaled[-1]*saved_alpha_u/input_pre_scaling_factor))\n",
    "    print(\"h_t, min,mean,max,norm\",hidden[-1].min().detach().numpy(),hidden[-1].mean().detach().numpy(),hidden[-1].max().detach().numpy(),torch.norm(hidden[-1]).detach().numpy())\n",
    "    #print(\"diff h_t\",hidden[-1][0,0:10])\n",
    "    #print(\"diff h_t\",hidden_scaled[-1][0,0:10])\n",
    "    #print(\"diff h_t\",hidden_scaled[-1][0,0:10]*saved_alpha_u/input_pre_scaling_factor)\n",
    "    #if ii>2:\n",
    "    #    aaa\n",
    "    if 'activation.b' in new_weights:\n",
    "        print(\"diff h_t+bias\",torch.norm(hidden[-1]+model.activation.b-(hidden_scaled[-1]+model_scaled.activation.b)*saved_alpha_u/input_pre_scaling_factor))\n",
    "    #NO ACTIV IN SSM hidden[-1] = model.activation(hidden[-1])\n",
    "    #NO ACTIV IN SSM hidden_scaled[-1] = model_scaled.activation(hidden_scaled[-1])\n",
    "    print(\"activ\",torch.norm(hidden[-1]-hidden_scaled[-1]*saved_alpha_u/input_pre_scaling_factor),torch.norm(hidden[-1]-hidden_scaled[-1]*saved_alpha_u/input_pre_scaling_factor)/torch.norm(hidden[-1]),torch.max(torch.abs(hidden[-1]-hidden_scaled[-1]*saved_alpha_u/input_pre_scaling_factor)))\n",
    "    #print(\"activ value\",hidden[-1],hidden_scaled[-1]*saved_alpha_u/input_pre_scaling_factor)\n",
    "    #print(\"activ valueq\",hidden_scaled[-1],model_scaled.quant_feat(hidden_scaled[-1])/model_scaled.quant_feat.pre_scaling_factor)\n",
    "    print(\"activ quantif\",torch.norm(hidden[-1]-model_scaled.quant_feat(hidden_scaled[-1])/model_scaled.quant_feat.pre_scaling_factor*saved_alpha_u/input_pre_scaling_factor))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "preds = []\n",
    "if config.model['manytomany']:\n",
    "    for ii,hh in enumerate(hidden_scaled): #zip(hidden_scaled):\n",
    "        #print(hh.shape)\n",
    "        #print(hh[:,0:10])\n",
    "        preds.append(model_scaled.output_layer(model_scaled.activation(hh)))\n",
    "    nsmples = 10\n",
    "else:\n",
    "    preds.append(model_scaled.output_layer(model_scaled.activation(hidden_scaled[-1])))\n",
    "    nsmples = 1\n",
    "    '''if ii==2:\n",
    "        break'''\n",
    "    #preds.append(model_scaled.output_layer(hh[0]))\n",
    "ypred = model_scaled(x0.to(device))\n",
    "preds = torch.concat(preds,0)\n",
    "print(ypred[-1:])    \n",
    "print(\"preds\",preds[-nsmples:])\n",
    "print(torch.argmax(ypred[-nsmples:],dim=-1))    \n",
    "print(\"preds\",torch.argmax(preds[-nsmples:],dim=-1))\n",
    "print(ypred.shape,preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vvin = verify_and_round(model_scaled.recurrent_layer.weight, config.model['num_bits'])\n",
    "\n",
    "for hh, hhin, hhq, hhrec in zip(hidden_scaled,hidden_scaled_input, hidden_scaled_quant,hidden_scaled_rec):\n",
    "    vvin = hhin*(2**(config.model['num_bits']-1+shift_input)) #+model_scaled.quant_input.num_bits-1)) \n",
    "    print(\"hhin \",torch.norm(vvin.round() - vvin), torch.min(vvin.round() - vvin), torch.max(vvin.round() - vvin))\n",
    "    vvin = verify_and_round(hhin, config.model['num_bits']-1+shift_input)\n",
    "    vvq = hhq*(2**(model_scaled.quant_input.num_bits-1)) \n",
    "    #vvq = hhq*(2**(shift_input+model_scaled.quant_input.num_bits-1)*model_scaled.quant_input.max_absolute) \n",
    "    print(\"hhq \",torch.norm(vvq.round() - vvq), torch.min(vvq.round() - vvq), torch.max(vvq.round() - vvq))\n",
    "    vvq = verify_and_round(hhq, model_scaled.quant_feat.num_bits-1)  # +1 for multiply by 2 in model_scaled.quant_input.max_absolute\n",
    "    vvrec = hhrec*(2**(config.model['num_bits']-1+model_scaled.quant_feat.num_bits-1)) \n",
    "    print(\"hhrec \",torch.norm(vvrec.round() - vvrec), torch.min(vvrec.round() - vvrec), torch.max(vvrec.round() - vvrec))\n",
    "    vvrec = verify_and_round(hhrec, config.model['num_bits']-1+model_scaled.quant_feat.num_bits-1)\n",
    "    vv = hh*(2**(config.model['num_bits']-1+model_scaled.quant_feat.num_bits-1)) \n",
    "    print(\"hh \",torch.norm(vv.round() - vv), torch.min(vv.round() - vv), torch.max(vv.round() - vv))\n",
    "    vv = verify_and_round(hh, config.model['num_bits']-1+model_scaled.quant_feat.num_bits-1)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old stuf "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$U = \\alpha_U.U_k$, $U_k\\in[[-2^{k-1},2^{k-1}-1]]$\n",
    "\n",
    "$W = \\alpha_W.W_k$, $W_k\\in[[-2^{k-1},2^{k-1}-1]]$\n",
    "\n",
    "$x = x_{ka}/2^{ka-2}$  , $x_{ka}\\in{0,2^{ka-2}}$ #$[-2^{ka-1},2^{ka-1}-1]]$\n",
    "\n",
    "$h = \\alpha_h.h_{ka}$, $h_{ka}\\in[[-2^{ka-1},2^{ka-1}-1]]$\n",
    "\n",
    "$h_{t+1} = W.h + U.x = (\\alpha_h*\\alpha_W)*(W_k.h_{ka}) + (\\alpha_U/2^{ka-2})*(U_k*x_{ka})$\n",
    "\n",
    "$h_{t+1} = ReLU(h_{t+1})$\n",
    "\n",
    "On peut eviter la multiplaction par $\\alpha_U$\n",
    "\n",
    "Simplification si \n",
    "\n",
    "$1/\\alpha_h*\\alpha_W = 2^k'$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reminder fixed point notation  $Q_{l,k}$\n",
    "\n",
    "$Q_{l,k} = {p/2^{k},p\\in[[-2^{l-1},2^{l-1}-1]]}\\in [-2^{l-k-1},2^{l-k-1}[$ with $l$ bits\n",
    "\n",
    "in particular : $Q_k = Q_{k+1,k} = {p/2^{k},p\\in[[-2^k,2^k-1]]}\\in [-1,1[$ with $k+1$ bits\n",
    "\n",
    "multiplication $Q_{l,k}.Q_{l',k'} -> Q_{l+l'-1,k+k'}$\n",
    "\n",
    "$Q_{k}.Q_{k'} -> Q_{k+k'+1,k+k'}=Q_{k+k'}$\n",
    "\n",
    "Addition (has to be with the same $k$)\n",
    "\n",
    "$Q_{k} + Q_{k} -> Q_{k+2,k}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Formulation with fixed point variables\n",
    "## matrices and vectors\n",
    "$U = \\alpha_U/2^{k-1}.U_k= \\alpha_U.U'_k$, with $U'_k\\in Q_{k-1}$\n",
    "\n",
    "$W = \\alpha_W/2^{k-1}.W_k= \\alpha_W.W'_k$, with $W'_k\\in Q_{k-1}$\n",
    "\n",
    "$x(t) = 2*x_{ka}(t)$, with $x\\in\\{0,1\\}$ , $x_{ka}\\in\\{0,0.5\\}$ could be $Q_1$ (2 bits), but easier to keep it in $Q_{ka-1}$\n",
    "\n",
    "$h(t) = \\alpha_h/2^{ka-1}.h_{ka}(t)=\\alpha_h.h'_{ka}(t)$ with $h'_{ka}(t)\\in Q_{ka-1}$\n",
    "\n",
    "## recurrent computation\n",
    "\n",
    "$h(t+1) = W.h(t) + U.x(t) = (\\alpha_h*\\alpha_W)*(W'_k.h'_{ka}(t)) + (2\\alpha_U)*(U'_k*x_{ka}(t))$ with $W'_k.h'_{ka}(t)$ and $U'_k*x_{ka}(t)\\in Q_{k+ka-2}$\n",
    "\n",
    "$h(t+1) = ReLU(h(t+1))$\n",
    "\n",
    "or $h(t+1) = ModReLU(h(t+1),b) = sign(h(t+1))*ReLU(|h(t+1)|+b)$ => Could be quantized back to $\\alpha_h.h'_{ka}(t+1)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Scaling hidden vector to reduce floating point multiplications ($2\\alpha_U$)\n",
    "\n",
    "To avoid the multiplication by  par $2\\alpha_U$\n",
    "\n",
    "scaled h : $sh(t) = h(t)/(2\\alpha_U)$, $sh(t) = \\alpha_{sh}.sh'_{ka}(t)$ with $\\alpha_{sh} = \\alpha_h/(2\\alpha_U)$, and $sh'_{ka}(t)=h'_{ka}(t)$\n",
    "\n",
    "$sh(t+1) = (\\alpha_{sh}*\\alpha_W)*(W'_k.sh'_{ka}(t)) + U'_k*x_{ka}(t)$  -> Floating point\n",
    "\n",
    "$sh(t+1) = ReLU(h(t+1))$\n",
    "\n",
    "$sh(t+1) = ModReLU(h(t+1),sb), with $sb = b/(2\\alpha_U)$ => Could be quantized back to $\\alpha_{sh}.sh'_{ka}(t+1)$ (division by $\\alpha_sh$)\n",
    "\n",
    "# Removing the  floating point multiplication  $(\\alpha_{sh}*\\alpha_W)$\n",
    "set an upper bound for quantization: choose $\\alpha_h$ such as \n",
    "$(\\alpha_{sh}*\\alpha_W) = \\alpha_h\\alpha_W/(2\\alpha_U)= 2^{kal}$\n",
    "\n",
    "\n",
    " $U'_k*x_{ka}(t)$ in $Q_{k+ka-2} = Q_{k+ka-1,k+ka-2}$\n",
    "\n",
    " $W'_k.sh'_{ka}(t)$ in $Q_{k+ka-2} = Q_{k+ka-1,k+ka-2}$\n",
    "\n",
    " $2^{kal}W'_k.sh'_{ka}(t)$ in $Q_{k+ka+kal-1,k+ka-2}$\n",
    "\n",
    "$sh(t+1) \\in Q_{k+ka+kal-1,k+ka-2}$ \n",
    "ReLU or ModReLU -> **BUT** division by $\\alpha_{sh}=2^{kal}/\\alpha_W$ before quantizing  to $Q_{ka}$\n",
    "\n",
    "# Removing the  floating point division  $\\alpha_{sh}$ for quantization\n",
    "require to set  $\\alpha_{sh}=2^{kal}$\n",
    "quantize $\\alpha_W \\in Q_{kw,kw'}$\n",
    "\n",
    "\n",
    " $U'_k*x_{ka}(t)$ in $Q_{k+ka-2} = Q_{k+ka-1,k+ka-2}$\n",
    "\n",
    " $W'_k.sh'_{ka}(t)$ in $Q_{k+ka-2} = Q_{k+ka-1,k+ka-2}$\n",
    "\n",
    " $\\alpha_W.W'_k.sh'_{ka}(t)$ in $Q_{k+ka+kw-2,k+ka+kw'-2}$\n",
    "\n",
    "$sh(t+1) \\in Q_{k+ka+kw-2,k+ka+kw'-2}$ \n",
    "ReLU or ModReLU ->  shift by $\\alpha_{sh}=2^{kal}$ before quantizing  to $Q_{ka}$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deel-pt1.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
