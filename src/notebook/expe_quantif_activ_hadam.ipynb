{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/franck.mamalet/.conda/envs/deel-pt1.10/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import wandb\n",
    "import yaml\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from training import train, evaluate\n",
    "from config import Config\n",
    "from getters import *\n",
    "from models import *\n",
    "\n",
    "from utils import find_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def make_model(name, **kwargs):\n",
    "    Model = get_model(name)\n",
    "    return Model(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "expe_name = 'copy_hadamssm_2'\n",
    "task2sweep = {'copy_hadamssm': 'test',\n",
    "              'copy_bjorck': 'sweep_ucvrt4w4',\n",
    "              'copy_projunn': 'sweep_a4d2sihj',\n",
    "             'pmnist_bjorck': 'sweep_bgckd728',\n",
    "             'smnist': 'sweep_a3af3t6v',\n",
    "             'ptb_bjorck': 'sweep_g0tjzjy7',\n",
    "}\n",
    "\n",
    "short2fullname = {\n",
    "    'copy_hadamssm_2': 'hadamSSM_copy',\n",
    "    'copy_bjorck_5' : 'fresh-sweep-19_qeh5ezkj',\n",
    "    'copy_bjorck_6' : 'rich-sweep-25_eihml4ya',\n",
    "    'copy_bjorck_8' : 'woven-sweep-33_dfdq24vp',\n",
    "    'copy_projunn_5' : 'sweet-sweep-16_fvzfdbx1',\n",
    "    'copy_projunn_6' : 'rose-sweep-22_e3ftelec',\n",
    "    'copy_projunn_8' : 'resilient-sweep-32_fze6523n',\n",
    "    'pmnist_bjorck_3' : 'rare-sweep-1_uyv83nl9',\n",
    "    'pmnist_bjorck_4' : 'rare4-sweep-1_uyv83nl9',\n",
    "    'pmnist_bjorck_5' : 'fresh-sweep-2_c9nwwvj4',\n",
    "    'pmnist_bjorck_6' : 'skilled-sweep-3_1pi0p71d',\n",
    "    'pmnist_bjorck_8' : 'denim-sweep-4_27qjq5oa',\n",
    "    'smnist_bjorck_3' : 'cerulean-sweep-1_1k08ik92',\n",
    "    'smnist_bjorck_4' : 'sleek-sweep-2_275j9jq7',\n",
    "    'smnist_bjorck_5' : 'wandering-sweep-3_1tbbz1et',\n",
    "    'smnist_bjorck_6' : 'distinctive-sweep-4_c0gyg62r',\n",
    "    'smnist_bjorck_8' : 'rich-sweep-5_ypocgzgv',\n",
    "    'ptb_bjorck_3' : 'decent-sweep-1_5tay03df',\n",
    "    'ptb_bjorck_4' : 'earnest-sweep-2_rb6cg3gh',\n",
    "    'ptb_bjorck_5' : 'eternal-sweep-3_5vss2or6',\n",
    "    'ptb_bjorck_6' : 'polished-sweep-4_qa3yj567',\n",
    "    'ptb_bjorck_8' : 'eager-sweep-5_whv7y4xu',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_task_value(expe_name,task2value):\n",
    "    value = None\n",
    "    for kk in task2value.keys():\n",
    "        if kk in expe_name:\n",
    "            value = task2value[kk]\n",
    "            break\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copy_task\n"
     ]
    }
   ],
   "source": [
    "path2yaml = {'copy': 'copy_task',\n",
    "             'pmnist': 'permuted_mnist',\n",
    "             'smnist': 'seq_mnist',\n",
    "             'ptb': 'ptb',}\n",
    "\n",
    "config_path = get_task_value(expe_name,path2yaml)\n",
    "print(config_path)\n",
    "\n",
    "sweep_name = get_task_value(expe_name,task2sweep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sweep_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file_path = \"conf/\"+config_path+\"/\"+short2fullname[expe_name]+\".yaml\"\n",
    "weight_file_path = \"results/\"+sweep_name+\"_\"+short2fullname[expe_name]+\".pth\"\n",
    "result_file_path = \"results/\"+sweep_name+\"_\"+short2fullname[expe_name]+\".txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file_path = \"conf/\"+config_path+\"/\"+\"copy_task_hadamSSM_config_test.yaml\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu' #'cpu' #'cpu' #'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'conf/copy_task/copy_task_hadamSSM_config_test.yaml'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_file_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verification of model load an performance with FP activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "conf_file = find_file(config_file_path)\n",
    "config = Config(conf_file=conf_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.model['num_bits_feat'] = None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BinadamSSM(\n",
      "  (activation): ReLU()\n",
      "  (input_layer): QLinear(in_features=10, out_features=512, bias=False)\n",
      "  (recurrent_layer): ParametrizedBinadamard(\n",
      "    in_features=512, out_features=512, bias=True\n",
      "    (parametrizations): ModuleDict(\n",
      "      (weight): ParametrizationList(\n",
      "        (0): binadamard_parametrization()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (output_layer): Linear(in_features=512, out_features=9, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = make_model(**config.model).to(device)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(weight_file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'CopyMemory', 'sent_length': 10, 'seq_length': 1000, 'test_size': 2000, 'train_size': 512000, 'val_size': 2000, 'vocabulary': 10}\n"
     ]
    }
   ],
   "source": [
    "print(config.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------Loading CopyMemory------------------------------------------------------------\n",
      "------------------------------------------------------------CopyMemory loaded------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "Dataset = get_dataset(config.dataset['name'])\n",
    "\n",
    "dataset = Dataset(**config.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if dataset.test_ds is not None:\n",
    "    test_batch_size = dataset.te_size // 10\n",
    "    test_ds = dataset.test_ds\n",
    "else:\n",
    "    test_batch_size = dataset.va_size // 10\n",
    "    test_ds = dataset.val_ds\n",
    "stat_test = evaluate(test_ds, test_batch_size, model, loss_fn=config.train['loss_fn'], metrics=config.train['metrics'], kind='test', torch_device=device)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_loss': 9.904126585524864e-08}\n",
      "test_loss: 9.904285747097674e-08\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(stat_test)\n",
    "#ref  {'test_loss': 9.112985571846366e-05}   #in cuda {'test_loss': 0.00011134522355860099} ?????\n",
    "# open and print result_file_path\n",
    "with open(result_file_path, 'r') as f:\n",
    "    print(f.read())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare PTQ of activations:\n",
    "Compute max value of activation on training set\n",
    "\n",
    "TO BE DONE ONCE ONLY => CELLS IN MARKDOWN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.load(weight_file_path)\n",
    "new_weights = {}\n",
    "new_weights[\"input_layer.weight\"] = model.input_layer.qweight  #NO_PARAM model.input_layer.weight\n",
    "new_weights['recurrent_layer.weight'] = model.recurrent_layer.weight\n",
    "if 'activation.b' in weights:\n",
    "    new_weights['activation.b'] = weights['activation.b']\n",
    "if 'recurrent_layer.bias' in weights:\n",
    "    new_weights['recurrent_layer.bias'] = weights['recurrent_layer.bias']\n",
    "#new_weights['output_layer.weight'] = weights['output_layer.weight']\n",
    "\n",
    "for kk in weights.keys():\n",
    "    if (kk not in new_weights) and ('input_layer' not in kk) and ('recurrent_layer' not in kk):\n",
    "        new_weights[kk] = weights[kk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-5.5423894, -2.7711947, 0.0, 2.7711947]\n",
      "[-0.25, 0.0, 0.25]\n"
     ]
    }
   ],
   "source": [
    "print(list(torch.unique(new_weights[\"input_layer.weight\"]).numpy()))\n",
    "print(list(torch.unique(new_weights[\"recurrent_layer.weight\"]).detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_scaled = copy.deepcopy(config)\n",
    "\n",
    "config_scaled.model['num_bits']  = None\n",
    "config_scaled.model['num_bits_feat'] = None\n",
    "config_scaled.model['name'] = 'QSSM'\n",
    "#config_scaled.model['bias'] = False\n",
    "if 'activation.b' in new_weights:\n",
    "    config_scaled.model['activation'] = layers.modrelu(**config_scaled.model['activation_config'])\n",
    "else:\n",
    "    config_scaled.model['activation'] = torch.nn.ReLU()\n",
    "config_scaled.model['num_bits_feat'] = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device_stat = 'cuda'\n",
    "model_stat = make_model(**config_scaled.model).to(device_stat)\n",
    "model_stat.load_state_dict(new_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QSSM(\n",
       "  (activation): ReLU()\n",
       "  (input_layer): QLinear(in_features=10, out_features=512, bias=False)\n",
       "  (recurrent_layer): QLinear(in_features=512, out_features=512, bias=True)\n",
       "  (output_layer): Linear(in_features=512, out_features=9, bias=True)\n",
       "  (quant_input): _Quantize_stats()\n",
       "  (quant_feat): _Quantize_stats()\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute max_val on training set\n",
    "train_batch_size = dataset.tr_size // 1000\n",
    "stat_test = evaluate(dataset.train_ds, train_batch_size, model_stat, loss_fn=config.train['loss_fn'], metrics=config.train['metrics'], kind='test', torch_device=device_stat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stats on training set\n",
      "{'test_loss': 2.2125034604414395e-07}\n",
      "input\n",
      "1.0\n",
      "feat\n",
      "170.156494140625\n"
     ]
    }
   ],
   "source": [
    "print(\"stats on training set\")\n",
    "print(stat_test)\n",
    "print(\"input\")\n",
    "print(model_stat.quant_input.stat_max_absolute)\n",
    "print(\"feat\")\n",
    "print(model_stat.quant_feat.stat_max_absolute)\n",
    "\n",
    "#stats on training set\n",
    "#input\n",
    "#1.0\n",
    "#feat\n",
    "#19.80474853515625"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stat_test = evaluate(test_ds, test_batch_size, model_stat, loss_fn=config.train['loss_fn'], metrics=config.train['metrics'], kind='test', torch_device=device_stat)\n",
    "#print(stat_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'copy_hadamssm_2'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expe_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "task2input_max_absolute = {'copy': 2.0,\n",
    "             'pmnist': 1.0,\n",
    "             'smnist': 1.0,\n",
    "             'ptb': 2.0,}\n",
    "\n",
    "short2feat_max_absolute  = {\n",
    "    'copy_hadamssm_2': 170.156494140625, #21.454788208007812,\n",
    "    'copy_bjorck_5' : 24.19596290588379,\n",
    "    'copy_bjorck_6' : 42.483619689941406,\n",
    "    'copy_bjorck_8' : 30.833498001098633, #65.3037109375,\n",
    "    'copy_projunn_5' : 'sweet-sweep-16_fvzfdbx1',\n",
    "    'copy_projunn_6' : 1902.8502197265625,\n",
    "    'copy_projunn_8' : 219.551513671875,\n",
    "    'pmnist_bjorck_3' : 22.018991470336914,\n",
    "    'pmnist_bjorck_4' : 26.795150756835938,\n",
    "    'pmnist_bjorck_5' : 25.203166961669922,\n",
    "    'pmnist_bjorck_6' : 25.342670440673828,\n",
    "    'pmnist_bjorck_8' : 26.046796798706055,\n",
    "    'smnist_bjorck_3' : 'cerulean-sweep-1_1k08ik92',\n",
    "    'smnist_bjorck_4' : 17.66891860961914,\n",
    "    'smnist_bjorck_5' : 29.19618034362793,\n",
    "    'smnist_bjorck_6' : 30.9416561126709,\n",
    "    'smnist_bjorck_8' : 30.775388717651367,\n",
    "    'ptb_bjorck_3' : 'decent-sweep-1_5tay03df',\n",
    "    'ptb_bjorck_4' : 30.078453063964844,\n",
    "    'ptb_bjorck_5' : 6.935123443603516,\n",
    "    'ptb_bjorck_6' : 6.048137187957764,\n",
    "    'ptb_bjorck_8' : 9.994182586669922,\n",
    "}\n",
    "\n",
    "task2prescale = {'copy': 1.0,\n",
    "             'pmnist': 255.0/256.0,\n",
    "             'smnist': 255.0/256.0,\n",
    "             'ptb': 1.0,}\n",
    "\n",
    "task2shiftinput = {'copy': 0,\n",
    "             'pmnist': 8,\n",
    "             'smnist': 8,\n",
    "             'ptb':0,}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "saved_quant_input_max_absolute = get_task_value(expe_name,task2input_max_absolute) #short2input_max_absolute[expe_name] #2. # 2.  ## set it twice to get 1 as max\n",
    "saved_quant_feat_max_absolute = short2feat_max_absolute[expe_name] #19.80474853515625   #32 #\n",
    "\n",
    "input_pre_scaling_factor = get_task_value(expe_name,task2prescale)\n",
    "#input_pre_scaling_factor = 1.0 # to get true Int\n",
    "#input_pre_scaling_factor = 255.0/256.0\n",
    "shift_input = get_task_value(expe_name,task2shiftinput)\n",
    "#shift_input = 0\n",
    "#shift_input = 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0\n",
      "170.156494140625\n",
      "1.0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(saved_quant_input_max_absolute)\n",
    "print(saved_quant_feat_max_absolute)\n",
    "print(input_pre_scaling_factor)\n",
    "print(shift_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get $\\alpha_W$ and $\\alpha_U$ values\n",
    "\n",
    "TO BE DONE ONCE ONLY => CELLS IN MARKDOWN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParametrizationList(\n",
       "  (0): binadamard_parametrization()\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.recurrent_layer.parametrizations.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model.recurrent_layer.parametrizations.weight.original\\naa = model.recurrent_layer.parametrizations.weight[0](model.recurrent_layer.parametrizations.weight.original)\\nmodel.recurrent_layer.parametrizations.weight[-2](aa)'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''model.recurrent_layer.parametrizations.weight.original\n",
    "aa = model.recurrent_layer.parametrizations.weight[0](model.recurrent_layer.parametrizations.weight.original)\n",
    "model.recurrent_layer.parametrizations.weight[-2](aa)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeParametrization(params,originaW,until_step=2):\n",
    "    for kk in range(until_step):\n",
    "        originaW = params[kk](originaW)\n",
    "    return originaW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParametrizationList(\n",
      "  (0): binadamard_parametrization()\n",
      ")\n",
      "W -0.25 0.25 0.25\n",
      "U -5.5423894 2.7711947 5.5423894\n"
     ]
    }
   ],
   "source": [
    "#Get floating point weights\n",
    "import torch.nn.utils.parametrize as parametrize\n",
    "\n",
    "print(model.recurrent_layer.parametrizations.weight)\n",
    "if len(model.recurrent_layer.parametrizations.weight) > 1: #1 is only Quantize, SpectralLinear has 3 paramtrizations\n",
    "    Wfp = computeParametrization(model.recurrent_layer.parametrizations.weight,\n",
    "                                 model.recurrent_layer.parametrizations.weight.original,\n",
    "                                 until_step=2) #model.recurrent_layer.parametrizations.weight[-2](model.recurrent_layer.parametrizations.weight.original)\n",
    "else:\n",
    "    Wfp = model.recurrent_layer.weight\n",
    "print(\"W\",Wfp.min().detach().numpy(),Wfp.max().detach().numpy(),np.max(np.abs(Wfp.detach().numpy())))\n",
    "if parametrize.is_parametrized(model.input_layer):\n",
    "    print(model.input_layer.parametrizations.weight)\n",
    "    if len(model.input_layer.parametrizations.weight) > 1:\n",
    "        Ufp = computeParametrization(model.input_layer.parametrizations.weight,\n",
    "                                    model.input_layer.parametrizations.weight.original,\n",
    "                                    until_step=2) #model.input_layer.parametrizations.weight[-2](model.input_layer.parametrizations.weight.original)\n",
    "    else:\n",
    "        Ufp = model.input_layer.parametrizations.weight.original\n",
    "else:\n",
    "    Ufp = model.input_layer.qweight #QLinear implementation of Armand \n",
    "print(\"U\",Ufp.min().detach().numpy(),Ufp.max().detach().numpy(),np.max(np.abs(Ufp.detach().numpy())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "short2alpha_w  = {\n",
    "    'copy_hadamssm_2': 0.25,\n",
    "    'copy_bjorck_5' : 0.26512054,\n",
    "    'copy_bjorck_6' : 0.28184956,\n",
    "    'copy_bjorck_8' : 0.26092893, #0.26838166,\n",
    "    'copy_projunn_5' : 'sweet-sweep-16_fvzfdbx1',\n",
    "    'copy_projunn_6' : 0.9988483,\n",
    "    'copy_projunn_8' : 0.9998472,\n",
    "    'pmnist_bjorck_3' : 0.32128334,\n",
    "    'pmnist_bjorck_4' : 0.3660844,\n",
    "    'pmnist_bjorck_5' : 0.34444174,\n",
    "    'pmnist_bjorck_6' : 0.38659847,\n",
    "    'pmnist_bjorck_8' : 0.6006793,\n",
    "    'smnist_bjorck_3' : 'cerulean-sweep-1_1k08ik92',\n",
    "    'smnist_bjorck_4' : 0.43379927,\n",
    "    'smnist_bjorck_5' : 0.36560023,\n",
    "    'smnist_bjorck_6' : 0.40944064,\n",
    "    'smnist_bjorck_8' : 0.40728217,\n",
    "    'ptb_bjorck_3' : 'decent-sweep-1_5tay03df',\n",
    "    'ptb_bjorck_4' : 0.19523722,\n",
    "    'ptb_bjorck_5' : 0.23500842,\n",
    "    'ptb_bjorck_6' : 0.16609415,\n",
    "    'ptb_bjorck_8' : 0.48273855,\n",
    "}\n",
    "\n",
    "short2alpha_u  = {\n",
    "    'copy_hadamssm_2': 5.5423894,\n",
    "    'copy_bjorck_5' : 2.2110918,\n",
    "    'copy_bjorck_6' : 2.2872915,\n",
    "    'copy_bjorck_8' : 1.733867, #1.9520738,\n",
    "    'copy_projunn_5' : 'sweet-sweep-16_fvzfdbx1',\n",
    "    'copy_projunn_6' : 7.8092656,\n",
    "    'copy_projunn_8' : 1.9943593,\n",
    "    'pmnist_bjorck_3' : 16.959444,\n",
    "    'pmnist_bjorck_4' : 12.96517,\n",
    "    'pmnist_bjorck_5' : 8.487175,\n",
    "    'pmnist_bjorck_6' : 10.693445,\n",
    "    'pmnist_bjorck_8' : 7.431476,\n",
    "    'smnist_bjorck_3' : 'cerulean-sweep-1_1k08ik92',\n",
    "    'smnist_bjorck_4' : 6.031232,\n",
    "    'smnist_bjorck_5' : 8.092017,\n",
    "    'smnist_bjorck_6' : 5.021599,\n",
    "    'smnist_bjorck_8' : 6.9251447,\n",
    "    'ptb_bjorck_3' : 'decent-sweep-1_5tay03df',\n",
    "    'ptb_bjorck_4' : 15.941487,\n",
    "    'ptb_bjorck_5' : 2.4994867,\n",
    "    'ptb_bjorck_6' : 2.4208195,\n",
    "    'ptb_bjorck_8' : 7.541183,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_alpha_u = short2alpha_u[expe_name] #np.max(np.abs([-0.6072238087654114, 0.607637882232666]))\n",
    "saved_alpha_w = short2alpha_w[expe_name] #nnp.max(np.abs([-0.24883395433425903, 0.24531713128089905]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.5423894 0.25\n"
     ]
    }
   ],
   "source": [
    "print(saved_alpha_u,saved_alpha_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rounding functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_values(vv, num_bits):\n",
    "    vv = vv*2**num_bits\n",
    "    assert torch.max(torch.abs(vv.round() - vv))<1e-4   # ?????\n",
    "    vv = vv.round()\n",
    "    vv = vv/2**num_bits\n",
    "    return vv\n",
    "\n",
    "def verify_round_max(vv, num_bits):\n",
    "    eval_numbits = 0\n",
    "    for ss in range(num_bits):\n",
    "        vv = vv*2\n",
    "        if torch.max(torch.abs(vv.round() - vv))<1e-5:\n",
    "            eval_numbits = ss+1\n",
    "            break\n",
    "    assert eval_numbits > 0\n",
    "    print(eval_numbits,\"max\",num_bits)\n",
    "    return eval_numbits\n",
    "    \n",
    "  \n",
    "def verify_and_round(vv, num_bits):  \n",
    "    eval_numbits = verify_round_max(vv, num_bits)\n",
    "    return round_values(vv, eval_numbits)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully Quantized RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.load(weight_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "vv = (2**config.model['num_bits'])*model.recurrent_layer.weight/ saved_alpha_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., grad_fn=<MaxBackward1>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(torch.abs(vv.round()-vv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(torch.unique(model.input_layer.qweight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.7009273185722\n",
      "scaled hidden quantiz 7.67523182964305\n",
      "'recurrent_layer.bias' max abs 0.19227177\n"
     ]
    }
   ],
   "source": [
    "new_weights = {}\n",
    "requant_input_weight = round_values(model.input_layer.qweight/ saved_alpha_u, config.model['num_bits'])#NO_PARAM model.input_layer.weight\n",
    "new_weights[\"input_layer.weight\"] = requant_input_weight #model.input_layer.weight/ (saved_quant_input_max_absolute*saved_alpha_u)\n",
    "float_recurrent_weight = model.recurrent_layer.weight\n",
    "requant_recurrent_weight = round_values(model.recurrent_layer.weight/ saved_alpha_w, config.model['num_bits'])\n",
    "new_weights['recurrent_layer.weight'] = requant_recurrent_weight #model.recurrent_layer.weight / saved_alpha_w ## compensate alpha_w in hidden quantization\n",
    "#new_weights['recurrent_layer.weight'] = model.recurrent_layer.weight  ## compensate alpha_w in hidden quantization\n",
    "scaled_quant_feat_max_absolute  = saved_quant_feat_max_absolute/saved_alpha_u\n",
    "print(scaled_quant_feat_max_absolute) #saved_quant_feat_max_absolute*saved_alpha_w/(saved_quant_input_max_absolute*saved_alpha_u))\n",
    "print(\"scaled hidden quantiz\",scaled_quant_feat_max_absolute*saved_alpha_w)  #4.055123978191649\n",
    "#new_quant_feat_max_absolute  = scaled_quant_feat_max_absolute #8/saved_alpha_w #saved_alpha_w*saved_quant_feat_max_absolute/(saved_quant_input_max_absolute*saved_alpha_u)\n",
    "if 'activation.b' in weights:\n",
    "    new_weights['activation.b'] = weights['activation.b']/ saved_alpha_u\n",
    "    print(\"activation.b max abs\",torch.abs(new_weights['activation.b']).max().cpu().detach().numpy())\n",
    "if 'recurrent_layer.bias' in weights:\n",
    "    new_weights['recurrent_layer.bias'] = weights['recurrent_layer.bias']/ saved_alpha_u\n",
    "    print(\"'recurrent_layer.bias' max abs\",torch.abs(new_weights['recurrent_layer.bias']).max().cpu().detach().numpy())\n",
    "new_weights['output_layer.weight'] = saved_alpha_u*weights['output_layer.weight']/input_pre_scaling_factor\n",
    "\n",
    "for kk in weights.keys():\n",
    "    if (kk not in new_weights) and ('input_layer' not in kk) and ('recurrent_layer' not in kk):\n",
    "        new_weights[kk] = weights[kk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170.156494140625 5.5423894 30.7009273185722\n",
      "30.7009273185722 0.25 7.67523182964305\n"
     ]
    }
   ],
   "source": [
    "print(saved_quant_feat_max_absolute,saved_alpha_u,saved_quant_feat_max_absolute/saved_alpha_u)\n",
    "print(scaled_quant_feat_max_absolute,saved_alpha_w,scaled_quant_feat_max_absolute*saved_alpha_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "short2scaled_feat_max_absolute  = {  # if possible power of two\n",
    "    'copy_hadamssm_2': 8.0,\n",
    "    'copy_bjorck_5' : 4.0,\n",
    "    'copy_bjorck_6' : 4.0,  \n",
    "    'copy_bjorck_8' : 4.0, #12.0, #4.417504669503896\n",
    "    'copy_projunn_5' : 'sweet-sweep-16_fvzfdbx1',\n",
    "    'copy_projunn_6' : 256.0,\n",
    "    'copy_projunn_8' : 128.0,\n",
    "    'pmnist_bjorck_3' : 1.0,    #0.20856624553910355\n",
    "    'pmnist_bjorck_4' : 1.0,    #0.7565875871836489\n",
    "    'pmnist_bjorck_5' : 1.0,\n",
    "    'pmnist_bjorck_6' : 1.0,\n",
    "    'pmnist_bjorck_8' : 2.0,\n",
    "    'smnist_bjorck_3' : 'cerulean-sweep-1_1k08ik92',\n",
    "    'smnist_bjorck_4' : 2.0,\n",
    "    'smnist_bjorck_5' : 2.0,\n",
    "    'smnist_bjorck_6' : 4.0,\n",
    "    'smnist_bjorck_8' : 2.0,\n",
    "    'ptb_bjorck_3' : 'decent-sweep-1_5tay03df',\n",
    "    'ptb_bjorck_4' : 1.0,\n",
    "    'ptb_bjorck_5' : 1.0,\n",
    "    'ptb_bjorck_6' : 1.0,\n",
    "    'ptb_bjorck_8' : 1.0,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_quant_feat_max_absolute  = short2scaled_feat_max_absolute[expe_name]  #saved_alpha_w*saved_quant_feat_max_absolute/(2*saved_alpha_u)\n",
    "\n",
    "new_bias_max_absolute = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check rounding of weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input  tensor(0.)\n",
      "recurrent  tensor(0., grad_fn=<CopyBackwards>)\n"
     ]
    }
   ],
   "source": [
    "iww = 2**(config.model['num_bits']-1)*2*new_weights[\"input_layer.weight\"].unique()\n",
    "print(\"input \",torch.norm(iww.round() - iww))\n",
    "\n",
    "iww = 2**(config.model['num_bits']-1)*requant_recurrent_weight\n",
    "#print(iww)\n",
    "print(\"recurrent \",torch.norm(iww.round() - iww))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate with several bitwidth for activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': ReLU(), 'activation_config': None, 'activation_final': None, 'activation_final_config': None, 'basic_block': 4, 'bias': True, 'bias_final': True, 'givens': 2, 'hidden_size': 512, 'input_size': 10, 'manytomany': True, 'multiparameters': False, 'name': 'BinadamSSM', 'num_bits': 2, 'output_size': 9, 'seed': 171, 'single_layer': True, 'num_bits_feat': None}\n"
     ]
    }
   ],
   "source": [
    "print(config.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "config_scaled = copy.deepcopy(config)\n",
    "\n",
    "config_scaled.model['num_bits']  = None\n",
    "config_scaled.model['num_bits_feat'] = None\n",
    "config_scaled.model['name'] = 'QSSM'\n",
    "#config_scaled.model['bias'] = False\n",
    "if 'activation.b' in new_weights:\n",
    "    config_scaled.model['activation'] = layers.modified4q_modrelu(**config_scaled.model['activation_config'])\n",
    "else:\n",
    "    config_scaled.model['activation'] = torch.nn.ReLU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from extra_layers import _Quantize_stats\n",
    "\n",
    "qb = None\n",
    "models = []\n",
    "for qb in [None,12]: #6,8,10,12]:\n",
    "    config_scaled.model['num_bits_feat'] = qb #10 #12\n",
    "    model_scaled = make_model(**config_scaled.model).to(device)\n",
    "    if qb is None:   # No rescaling of h with qb=None\n",
    "        new_weights['recurrent_layer.weight'] = float_recurrent_weight\n",
    "        #model_scaled.load_state_dict(new_weights_normalphaU)\n",
    "    else:\n",
    "        new_weights['recurrent_layer.weight'] = requant_recurrent_weight\n",
    "    model_scaled.load_state_dict(new_weights)\n",
    "    if qb is not None:      \n",
    "        model_scaled.quant_input.pre_scaling_factor = input_pre_scaling_factor     \n",
    "        model_scaled.quant_input.max_absolute = saved_quant_input_max_absolute #saved_quant_input_max_absolute  ## set it twice to get 1 as max\n",
    "        model_scaled.quant_feat.max_absolute = new_quant_feat_max_absolute       \n",
    "        model_scaled.quant_feat.pre_scaling_factor = saved_alpha_w     \n",
    "        #print(model_scaled.activation.b)\n",
    "        #print(quant_bias(model_scaled.activation.b))\n",
    "        if ('activation.b' in new_weights) or ('recurrent_layer.bias' in new_weights):\n",
    "            #quantize biases\n",
    "            quant_bias = _Quantize_stats(weight=None,num_bits= model_scaled.quant_feat.num_bits, max_absolute= new_bias_max_absolute)\n",
    "            if 'activation.b' in new_weights:\n",
    "                model_scaled.activation.b.data = quant_bias(model_scaled.activation.b) \n",
    "            if 'recurrent_layer.bias' in new_weights:\n",
    "                model_scaled.recurrent_layer.bias.data = quant_bias(model_scaled.recurrent_layer.bias)\n",
    "    models.append(model_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1000, 10])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,y = next(iter(test_ds))\n",
    "x0 = x[None,:] #No batch_size [0:1]\n",
    "x0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input\n",
      "feat\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'stat_max_absolute'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [61]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#print(model_float.quant_input.stat_max_absolute)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeat\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mmodel_float\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquant_feat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstat_max_absolute\u001b[49m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'stat_max_absolute'"
     ]
    }
   ],
   "source": [
    "\n",
    "model_float = models[0]\n",
    "model_float.eval()\n",
    "model_float(x0.to(device))\n",
    "\n",
    "print(\"input\")\n",
    "#print(model_float.quant_input.stat_max_absolute)\n",
    "print(\"feat\")\n",
    "print(model_float.quant_feat.stat_max_absolute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_float' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [52]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m torch\u001b[38;5;241m.\u001b[39munique(\u001b[43mmodel_float\u001b[49m\u001b[38;5;241m.\u001b[39minput_layer\u001b[38;5;241m.\u001b[39mweight)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_float' is not defined"
     ]
    }
   ],
   "source": [
    "torch.unique(model_float.input_layer.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'training': True,\n",
       " '_parameters': OrderedDict(),\n",
       " '_buffers': OrderedDict(),\n",
       " '_non_persistent_buffers_set': set(),\n",
       " '_backward_hooks': OrderedDict(),\n",
       " '_is_full_backward_hook': None,\n",
       " '_forward_hooks': OrderedDict(),\n",
       " '_forward_pre_hooks': OrderedDict(),\n",
       " '_state_dict_hooks': OrderedDict(),\n",
       " '_load_state_dict_pre_hooks': OrderedDict(),\n",
       " '_modules': OrderedDict(),\n",
       " 'num_bits': 12,\n",
       " 'enforce_true_quantized': True,\n",
       " 'delta_m': None,\n",
       " 'max_absolute': 8.0,\n",
       " 'stat_max_absolute': 1e-10,\n",
       " 'pre_scaling_factor': 0.25}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_scaled.quant_feat.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.1191,  0.1860,  0.4380, -0.0894,  0.1797, -0.0747,  0.1880, -0.3149,\n",
      "        -0.3154,  0.1699], grad_fn=<SliceBackward0>) \n",
      " tensor([ 0.0298,  0.0465,  0.1095, -0.0223,  0.0449, -0.0187,  0.0470, -0.0787,\n",
      "        -0.0789,  0.0425], grad_fn=<MulBackward0>) \n",
      " tensor([ 0.0037,  0.0058,  0.0137, -0.0028,  0.0056, -0.0023,  0.0059, -0.0098,\n",
      "        -0.0099,  0.0053], grad_fn=<DivBackward0>) \n",
      " tensor([  7.6250,  11.9062,  28.0312,  -5.7188,  11.5000,  -4.7812,  12.0312,\n",
      "        -20.1562, -20.1875,  10.8750], grad_fn=<MulBackward0>) \n",
      " tensor([  8.,  12.,  28.,  -6.,  12.,  -5.,  12., -20., -20.,  11.],\n",
      "       grad_fn=<RoundBackward0>) \n",
      " tensor([ 0.0039,  0.0059,  0.0137, -0.0029,  0.0059, -0.0024,  0.0059, -0.0098,\n",
      "        -0.0098,  0.0054], grad_fn=<DivBackward0>) \n",
      " tensor([ 0.0312,  0.0469,  0.1094, -0.0234,  0.0469, -0.0195,  0.0469, -0.0781,\n",
      "        -0.0781,  0.0430], grad_fn=<MulBackward0>) \n",
      " tensor([ 0.1250,  0.1875,  0.4375, -0.0938,  0.1875, -0.0781,  0.1875, -0.3125,\n",
      "        -0.3125,  0.1719], grad_fn=<DivBackward0>)\n",
      "tensor([0.0059, 0.0015, 0.0005, 0.0044, 0.0078, 0.0034, 0.0005, 0.0024, 0.0029,\n",
      "        0.0020], grad_fn=<AbsBackward0>)\n",
      "tensor([ 0.1250,  0.1875,  0.4375, -0.0938,  0.1875, -0.0781,  0.1875, -0.3125,\n",
      "        -0.3125,  0.1719], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "h0 = hidden_scaled[-1][0,220:230]\n",
    "h1 = h0*model_scaled.quant_feat.pre_scaling_factor\n",
    "h2 = h1/model_scaled.quant_feat.max_absolute\n",
    "h3 = h2*2**(model_scaled.quant_feat.num_bits-1)\n",
    "h4 = h3.round()\n",
    "h5 = h4/2**(model_scaled.quant_feat.num_bits-1)\n",
    "h6 = h5*model_scaled.quant_feat.max_absolute\n",
    "h7 = h6/model_scaled.quant_feat.pre_scaling_factor\n",
    "print(h0,\"\\n\",h1,\"\\n\",h2,\"\\n\",h3,\"\\n\",h4,\"\\n\",h5,\"\\n\",h6,\"\\n\",h7)\n",
    "print(torch.abs(h0-h7))\n",
    "print(model_scaled.quant_feat(hidden_scaled[-1][0,220:230])/model_scaled.quant_feat.pre_scaling_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff h_t tensor(0., grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -0.5 -0.0048828125 0.5 2.7838821\n",
      "activ tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<DivBackward0>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(0., grad_fn=<CopyBackwards>)\n",
      "input tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]])\n",
      "qinput tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(0., grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.0 0.0 0.0 0.0 torch.Size([1, 512])\n",
      "diff h_t tensor(0.0032, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -0.7978067 -0.0029966324 0.92874837 4.1673536\n",
      "activ tensor(0.0032, grad_fn=<CopyBackwards>) tensor(0.0008, grad_fn=<DivBackward0>) tensor(0.0002, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(0.1055, grad_fn=<CopyBackwards>)\n",
      "input tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(0.0032, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(0.1055, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5847806 0.0 0.022479804 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(0.1055, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -0.9369043 -0.0019862228 0.8898971 5.5120354\n",
      "activ tensor(0.1055, grad_fn=<CopyBackwards>) tensor(0.0191, grad_fn=<DivBackward0>) tensor(0.0148, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(0.1459, grad_fn=<CopyBackwards>)\n",
      "input tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(0.1055, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(0.1459, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.571505 0.0 0.021824388 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(0.1462, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -1.0497028 -0.0010631271 1.4901263 6.7118125\n",
      "activ tensor(0.1462, grad_fn=<CopyBackwards>) tensor(0.0218, grad_fn=<DivBackward0>) tensor(0.0194, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(0.1802, grad_fn=<CopyBackwards>)\n",
      "input tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]])\n",
      "qinput tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(0.1462, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(0.1802, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.54473555 0.0 0.020672116 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(0.1804, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -1.1025491 0.0024138067 1.3498691 7.566179\n",
      "activ tensor(0.1804, grad_fn=<CopyBackwards>) tensor(0.0238, grad_fn=<DivBackward0>) tensor(0.0260, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(0.2082, grad_fn=<CopyBackwards>)\n",
      "input tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]])\n",
      "qinput tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(0.1804, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(0.2082, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5626191 0.0 0.021369822 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(0.2083, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -1.330509 0.003939669 1.5961567 8.490381\n",
      "activ tensor(0.2083, grad_fn=<CopyBackwards>) tensor(0.0245, grad_fn=<DivBackward0>) tensor(0.0333, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(0.2325, grad_fn=<CopyBackwards>)\n",
      "input tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(0.2083, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(0.2325, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55559814 0.0 0.021105539 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(0.2324, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -1.4652116 -0.0018047527 1.8289661 9.297817\n",
      "activ tensor(0.2324, grad_fn=<CopyBackwards>) tensor(0.0250, grad_fn=<DivBackward0>) tensor(0.0426, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(0.2534, grad_fn=<CopyBackwards>)\n",
      "input tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(0.2324, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(0.2534, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55464816 0.0 0.021137254 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(0.2534, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -1.4298495 0.0037845676 1.5364542 9.908284\n",
      "activ tensor(0.2534, grad_fn=<CopyBackwards>) tensor(0.0256, grad_fn=<DivBackward0>) tensor(0.0344, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(0.2724, grad_fn=<CopyBackwards>)\n",
      "input tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(0.2534, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(0.2724, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5720171 0.0 0.022088667 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(0.2724, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -1.7026452 0.0048121335 1.7098846 10.268659\n",
      "activ tensor(0.2724, grad_fn=<CopyBackwards>) tensor(0.0265, grad_fn=<DivBackward0>) tensor(0.0389, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(0.2974, grad_fn=<CopyBackwards>)\n",
      "input tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(0.2724, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(0.2974, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5717098 0.0 0.021813814 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(0.2976, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -2.3756475 0.024524845 1.7269738 10.706489\n",
      "activ tensor(0.2976, grad_fn=<CopyBackwards>) tensor(0.0278, grad_fn=<DivBackward0>) tensor(0.0507, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(0.3115, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(0.2976, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(0.3115, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55990505 0.0 0.021443821 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(0.3115, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -1.9743384 0.05800301 1.9032886 10.685889\n",
      "activ tensor(0.3115, grad_fn=<CopyBackwards>) tensor(0.0291, grad_fn=<DivBackward0>) tensor(0.0429, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(0.3272, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(0.3115, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(0.3272, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56313974 0.0 0.021390963 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(0.3271, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -2.2818253 0.017031854 1.7464433 10.564379\n",
      "activ tensor(0.3271, grad_fn=<CopyBackwards>) tensor(0.0310, grad_fn=<DivBackward0>) tensor(0.0396, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(0.3445, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(0.3271, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(0.3445, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56945086 0.0 0.021803243 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(0.3444, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -2.3558278 0.006810195 1.7247537 10.418955\n",
      "activ tensor(0.3444, grad_fn=<CopyBackwards>) tensor(0.0331, grad_fn=<DivBackward0>) tensor(0.0440, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(0.3589, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(0.3444, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(0.3589, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5671827 0.0 0.021739816 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(0.3591, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -1.7827753 0.02417855 1.5736954 10.207693\n",
      "activ tensor(0.3591, grad_fn=<CopyBackwards>) tensor(0.0352, grad_fn=<DivBackward0>) tensor(0.0528, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(0.3779, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(0.3591, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(0.3779, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57017064 0.0 0.021982953 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(0.3779, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -1.6194159 0.012716828 1.5471666 10.040307\n",
      "activ tensor(0.3779, grad_fn=<CopyBackwards>) tensor(0.0376, grad_fn=<DivBackward0>) tensor(0.0501, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(0.3895, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(0.3779, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(0.3895, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5697593 0.0 0.022035811 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(0.3895, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -1.5512719 0.0060403757 2.0552995 9.871609\n",
      "activ tensor(0.3895, grad_fn=<CopyBackwards>) tensor(0.0395, grad_fn=<DivBackward0>) tensor(0.0652, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(0.4017, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(0.3895, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(0.4017, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57620144 0.0 0.022268381 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(0.4019, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -1.5116172 0.0020402786 1.7997917 9.727656\n",
      "activ tensor(0.4019, grad_fn=<CopyBackwards>) tensor(0.0413, grad_fn=<DivBackward0>) tensor(0.0576, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(0.4095, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(0.4019, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(0.4095, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5662524 0.0 0.021612959 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(0.4096, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -2.1170633 0.006005386 2.0031464 9.629144\n",
      "activ tensor(0.4096, grad_fn=<CopyBackwards>) tensor(0.0425, grad_fn=<DivBackward0>) tensor(0.0594, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(0.4229, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(0.4096, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(0.4229, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5629315 0.0 0.021570675 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(0.4228, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -1.5403672 0.0036608116 1.4758068 9.611383\n",
      "activ tensor(0.4228, grad_fn=<CopyBackwards>) tensor(0.0440, grad_fn=<DivBackward0>) tensor(0.0737, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(0.4432, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(0.4228, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(0.4432, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.59006697 0.0 0.022870941 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(0.4432, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -1.62178 0.0033401113 1.427072 9.616917\n",
      "activ tensor(0.4432, grad_fn=<CopyBackwards>) tensor(0.0461, grad_fn=<DivBackward0>) tensor(0.0623, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(0.4566, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(0.4432, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(0.4566, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57365376 0.0 0.022056956 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(0.4566, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -1.3474298 0.015745796 1.6557845 9.625899\n",
      "activ tensor(0.4566, grad_fn=<CopyBackwards>) tensor(0.0474, grad_fn=<DivBackward0>) tensor(0.0781, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(0.4679, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(0.4566, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(0.4679, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55210716 0.0 0.020872971 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(0.4678, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -1.5461849 -0.030308016 1.7656525 9.623306\n",
      "activ tensor(0.4678, grad_fn=<CopyBackwards>) tensor(0.0486, grad_fn=<DivBackward0>) tensor(0.0762, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(0.4777, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(0.4678, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(0.4777, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.557388 0.0 0.02126411 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(0.4777, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -1.6871405 -0.0072399904 1.6651479 9.637448\n",
      "activ tensor(0.4777, grad_fn=<CopyBackwards>) tensor(0.0496, grad_fn=<DivBackward0>) tensor(0.0718, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(0.4943, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(0.4777, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(0.4943, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56428295 0.0 0.021486105 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(0.4942, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -1.5641022 0.0003408905 1.6681074 9.702273\n",
      "activ tensor(0.4942, grad_fn=<CopyBackwards>) tensor(0.0509, grad_fn=<DivBackward0>) tensor(0.0739, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(0.5122, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(0.4942, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(0.5122, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.54998064 0.0 0.020883543 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(0.5120, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -1.2698251 -0.0012185131 1.2873693 9.801039\n",
      "activ tensor(0.5120, grad_fn=<CopyBackwards>) tensor(0.0522, grad_fn=<DivBackward0>) tensor(0.0838, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(0.5244, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(0.5120, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(0.5244, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5762015 0.0 0.02210981 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(0.5244, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -1.4934314 0.023691986 1.8512707 9.884598\n",
      "activ tensor(0.5244, grad_fn=<CopyBackwards>) tensor(0.0530, grad_fn=<DivBackward0>) tensor(0.0841, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(0.5337, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(0.5244, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(0.5337, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5641792 0.0 0.021665817 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(0.5336, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -1.6930739 0.016296927 2.3249118 9.95881\n",
      "activ tensor(0.5336, grad_fn=<CopyBackwards>) tensor(0.0536, grad_fn=<DivBackward0>) tensor(0.0769, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(0.5529, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(0.5336, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(0.5529, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56334764 0.0 0.021570675 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(0.5530, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -1.6178393 0.013165427 1.7850748 10.037961\n",
      "activ tensor(0.5530, grad_fn=<CopyBackwards>) tensor(0.0551, grad_fn=<DivBackward0>) tensor(0.0849, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(0.5612, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(0.5530, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(0.5612, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5564411 0.0 0.02127468 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(0.5614, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -1.717316 0.0014625369 2.2223804 10.14283\n",
      "activ tensor(0.5614, grad_fn=<CopyBackwards>) tensor(0.0553, grad_fn=<DivBackward0>) tensor(0.0850, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(0.5743, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(0.5614, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(0.5743, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5649057 0.0 0.021443818 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(0.5744, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -1.6780025 0.018835656 1.9471734 10.247715\n",
      "activ tensor(0.5744, grad_fn=<CopyBackwards>) tensor(0.0561, grad_fn=<DivBackward0>) tensor(0.0928, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(0.5885, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(0.5744, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(0.5885, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.53748095 0.0 0.020481836 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(0.5884, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -1.6445472 0.018851742 1.6341617 10.378549\n",
      "activ tensor(0.5884, grad_fn=<CopyBackwards>) tensor(0.0567, grad_fn=<DivBackward0>) tensor(0.1308, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(0.6013, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(0.5884, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(0.6013, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56230676 0.0 0.021602388 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(0.6013, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -2.250686 -0.031208823 1.5045626 10.491222\n",
      "activ tensor(0.6013, grad_fn=<CopyBackwards>) tensor(0.0573, grad_fn=<DivBackward0>) tensor(0.0858, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(0.6106, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(0.6013, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(0.6106, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5624109 0.0 0.021475535 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(0.6107, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -1.9807597 -0.00919533 1.8197821 10.640898\n",
      "activ tensor(0.6107, grad_fn=<CopyBackwards>) tensor(0.0574, grad_fn=<DivBackward0>) tensor(0.0944, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(0.6167, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(0.6107, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(0.6167, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5634515 0.0 0.021634104 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(0.6168, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -1.9866724 -0.0016178582 2.1794522 10.776163\n",
      "activ tensor(0.6168, grad_fn=<CopyBackwards>) tensor(0.0572, grad_fn=<DivBackward0>) tensor(0.0857, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(0.6301, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(0.6168, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(0.6301, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57129985 0.0 0.021993525 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(0.6302, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -2.457773 0.013973394 1.6094437 10.891209\n",
      "activ tensor(0.6302, grad_fn=<CopyBackwards>) tensor(0.0579, grad_fn=<DivBackward0>) tensor(0.1150, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(0.6396, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(0.6302, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(0.6396, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57731885 0.0 0.022342378 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(0.6395, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -1.991704 -0.0055382075 1.7067004 10.9270935\n",
      "activ tensor(0.6395, grad_fn=<CopyBackwards>) tensor(0.0585, grad_fn=<DivBackward0>) tensor(0.1003, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(0.6454, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(0.6395, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(0.6454, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5650093 0.0 0.021729244 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(0.6454, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -1.9203084 -0.018035725 1.6942117 10.893783\n",
      "activ tensor(0.6454, grad_fn=<CopyBackwards>) tensor(0.0592, grad_fn=<DivBackward0>) tensor(0.0919, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(0.6603, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(0.6454, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(0.6603, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57721746 0.0 0.022162668 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(0.6603, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -1.619853 -0.014079854 1.8119113 10.802664\n",
      "activ tensor(0.6603, grad_fn=<CopyBackwards>) tensor(0.0611, grad_fn=<DivBackward0>) tensor(0.1056, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(0.6592, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(0.6603, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(0.6592, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55019355 0.0 0.020798972 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(0.6593, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -1.7617359 -0.024603244 1.6565806 10.688942\n",
      "activ tensor(0.6593, grad_fn=<CopyBackwards>) tensor(0.0617, grad_fn=<DivBackward0>) tensor(0.1051, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(0.6668, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(0.6593, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(0.6668, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5885756 0.0 0.02294494 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(0.6668, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -1.8166306 0.005926339 1.5259922 10.571052\n",
      "activ tensor(0.6668, grad_fn=<CopyBackwards>) tensor(0.0631, grad_fn=<DivBackward0>) tensor(0.0990, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(0.6735, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(0.6668, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(0.6735, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5683178 0.0 0.021665815 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(0.6733, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -1.9776915 0.0064800363 1.6097443 10.404406\n",
      "activ tensor(0.6733, grad_fn=<CopyBackwards>) tensor(0.0647, grad_fn=<DivBackward0>) tensor(0.1313, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(0.6811, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(0.6733, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(0.6811, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56355584 0.0 0.021708103 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(0.6811, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -1.5653149 -0.0111989025 1.5535048 10.2443495\n",
      "activ tensor(0.6811, grad_fn=<CopyBackwards>) tensor(0.0665, grad_fn=<DivBackward0>) tensor(0.0912, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(0.6918, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(0.6811, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(0.6918, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55422556 0.0 0.020894114 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(0.6918, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -1.4428318 0.010187036 1.4959638 10.122386\n",
      "activ tensor(0.6918, grad_fn=<CopyBackwards>) tensor(0.0683, grad_fn=<DivBackward0>) tensor(0.1015, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(0.6987, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(0.6918, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(0.6987, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5718123 0.0 0.021792673 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(0.6987, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -1.730917 -0.0046386113 1.594756 10.026213\n",
      "activ tensor(0.6987, grad_fn=<CopyBackwards>) tensor(0.0697, grad_fn=<DivBackward0>) tensor(0.0981, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(0.7089, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(0.6987, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(0.7089, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5563357 0.0 0.021412108 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(0.7091, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -1.5501404 -0.0021794923 1.4212275 10.023826\n",
      "activ tensor(0.7091, grad_fn=<CopyBackwards>) tensor(0.0707, grad_fn=<DivBackward0>) tensor(0.1264, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(0.7157, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(0.7091, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(0.7157, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5547539 0.0 0.021327537 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(0.7159, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -1.6947374 -0.008249456 1.9283133 10.0236435\n",
      "activ tensor(0.7159, grad_fn=<CopyBackwards>) tensor(0.0714, grad_fn=<DivBackward0>) tensor(0.1173, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(0.7275, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(0.7159, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(0.7275, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57314295 0.0 0.022025239 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(0.7275, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -1.4544523 -0.012838274 1.5161186 10.102646\n",
      "activ tensor(0.7275, grad_fn=<CopyBackwards>) tensor(0.0720, grad_fn=<DivBackward0>) tensor(0.1064, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(0.7341, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(0.7275, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(0.7341, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56334764 0.0 0.021422679 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(0.7342, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -1.7000638 -0.020673094 1.5585535 10.246278\n",
      "activ tensor(0.7342, grad_fn=<CopyBackwards>) tensor(0.0717, grad_fn=<DivBackward0>) tensor(0.1102, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(0.7462, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(0.7342, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(0.7462, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57273376 0.0 0.022067524 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(0.7462, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -2.187049 -0.030423494 2.1312375 10.466739\n",
      "activ tensor(0.7462, grad_fn=<CopyBackwards>) tensor(0.0713, grad_fn=<DivBackward0>) tensor(0.1183, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(0.7489, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(0.7462, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(0.7489, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5612636 0.0 0.021581247 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(0.7489, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -1.6066475 -0.0040494176 2.1424205 10.728613\n",
      "activ tensor(0.7489, grad_fn=<CopyBackwards>) tensor(0.0698, grad_fn=<DivBackward0>) tensor(0.0934, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(0.7586, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(0.7489, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(0.7586, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5628272 0.0 0.021581246 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(0.7585, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -1.8558059 0.005767446 2.1426413 10.977311\n",
      "activ tensor(0.7585, grad_fn=<CopyBackwards>) tensor(0.0691, grad_fn=<DivBackward0>) tensor(0.1394, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(0.7677, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(0.7585, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(0.7677, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57772475 0.0 0.022183808 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(0.7679, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -1.9222575 -0.022519436 2.1682014 11.188744\n",
      "activ tensor(0.7679, grad_fn=<CopyBackwards>) tensor(0.0686, grad_fn=<DivBackward0>) tensor(0.1351, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(0.7776, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(0.7679, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(0.7776, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56625235 0.0 0.021644672 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(0.7776, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -1.6993431 -0.0031587537 1.7275176 11.352872\n",
      "activ tensor(0.7776, grad_fn=<CopyBackwards>) tensor(0.0685, grad_fn=<DivBackward0>) tensor(0.1089, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(0.7867, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(0.7776, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(0.7867, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57497996 0.0 0.022226095 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(0.7870, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -2.1744473 -0.017327633 1.9845626 11.469578\n",
      "activ tensor(0.7870, grad_fn=<CopyBackwards>) tensor(0.0686, grad_fn=<DivBackward0>) tensor(0.1087, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(0.7889, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(0.7870, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(0.7889, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.58947104 0.0 0.022807512 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(0.7890, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -2.2639072 -0.0125879245 1.6965306 11.5785675\n",
      "activ tensor(0.7890, grad_fn=<CopyBackwards>) tensor(0.0681, grad_fn=<DivBackward0>) tensor(0.1520, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(0.7998, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(0.7890, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(0.7998, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56532025 0.0 0.02149668 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(0.7997, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -2.6313438 -0.016380258 2.0875773 11.63744\n",
      "activ tensor(0.7997, grad_fn=<CopyBackwards>) tensor(0.0687, grad_fn=<DivBackward0>) tensor(0.1233, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(0.8057, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(0.7997, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(0.8057, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5639713 0.0 0.021676388 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(0.8057, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -2.4465384 -0.02713429 1.7534826 11.661704\n",
      "activ tensor(0.8057, grad_fn=<CopyBackwards>) tensor(0.0691, grad_fn=<DivBackward0>) tensor(0.1205, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(0.8161, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(0.8057, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(0.8161, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56769896 0.0 0.021581247 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(0.8162, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -2.2525976 0.0017566998 1.957896 11.657221\n",
      "activ tensor(0.8162, grad_fn=<CopyBackwards>) tensor(0.0700, grad_fn=<DivBackward0>) tensor(0.1190, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(0.8227, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(0.8162, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(0.8227, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5650093 0.0 0.021782104 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(0.8226, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -2.1764138 -0.008359401 1.8496566 11.61443\n",
      "activ tensor(0.8226, grad_fn=<CopyBackwards>) tensor(0.0708, grad_fn=<DivBackward0>) tensor(0.1228, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(0.8300, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(0.8226, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(0.8300, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5593816 0.0 0.021221824 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(0.8302, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -2.1106155 -0.00914746 2.2325082 11.557793\n",
      "activ tensor(0.8302, grad_fn=<CopyBackwards>) tensor(0.0718, grad_fn=<DivBackward0>) tensor(0.1221, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(0.8400, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(0.8302, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(0.8400, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56635576 0.0 0.021644674 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(0.8400, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -2.3705068 -0.014749936 1.9606562 11.504055\n",
      "activ tensor(0.8400, grad_fn=<CopyBackwards>) tensor(0.0730, grad_fn=<DivBackward0>) tensor(0.1704, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(0.8488, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(0.8400, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(0.8488, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56074154 0.0 0.021211252 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(0.8487, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -2.591066 -0.03180786 1.8865829 11.379993\n",
      "activ tensor(0.8487, grad_fn=<CopyBackwards>) tensor(0.0746, grad_fn=<DivBackward0>) tensor(0.1187, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(0.8632, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(0.8487, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(0.8632, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5794463 0.0 0.022215521 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(0.8633, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -2.1710353 -0.014445638 1.9386665 11.281401\n",
      "activ tensor(0.8633, grad_fn=<CopyBackwards>) tensor(0.0765, grad_fn=<DivBackward0>) tensor(0.1471, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(0.8661, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(0.8633, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(0.8661, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5723244 0.0 0.022078095 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(0.8662, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -2.2409303 -0.03992435 1.8601313 11.21393\n",
      "activ tensor(0.8662, grad_fn=<CopyBackwards>) tensor(0.0772, grad_fn=<DivBackward0>) tensor(0.1361, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(0.8758, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(0.8662, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(0.8758, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.567802 0.0 0.021792673 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(0.8759, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -2.1480331 -0.016930513 2.037012 11.178474\n",
      "activ tensor(0.8759, grad_fn=<CopyBackwards>) tensor(0.0784, grad_fn=<DivBackward0>) tensor(0.1730, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(0.8848, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(0.8759, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(0.8848, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56542397 0.0 0.02134868 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(0.8847, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -2.082333 -0.02570928 1.9011648 11.185626\n",
      "activ tensor(0.8847, grad_fn=<CopyBackwards>) tensor(0.0791, grad_fn=<DivBackward0>) tensor(0.1437, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(0.8888, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(0.8847, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(0.8888, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5603234 0.0 0.02145439 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(0.8888, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -2.0872421 -0.013903909 1.6818633 11.231354\n",
      "activ tensor(0.8888, grad_fn=<CopyBackwards>) tensor(0.0791, grad_fn=<DivBackward0>) tensor(0.1415, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(0.8994, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(0.8888, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(0.8994, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57569283 0.0 0.022120379 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(0.8995, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -2.086276 -0.013709871 1.7569258 11.250582\n",
      "activ tensor(0.8995, grad_fn=<CopyBackwards>) tensor(0.0800, grad_fn=<DivBackward0>) tensor(0.1428, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(0.9066, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(0.8995, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(0.9066, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5871803 0.0 0.022553805 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(0.9066, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -2.5209506 -0.039212514 1.9668663 11.292991\n",
      "activ tensor(0.9066, grad_fn=<CopyBackwards>) tensor(0.0803, grad_fn=<DivBackward0>) tensor(0.1814, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(0.9190, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(0.9066, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(0.9190, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5827735 0.0 0.02252209 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(0.9193, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -2.6462054 -0.00783575 2.0553207 11.39615\n",
      "activ tensor(0.9193, grad_fn=<CopyBackwards>) tensor(0.0807, grad_fn=<DivBackward0>) tensor(0.1384, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(0.9241, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(0.9193, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(0.9241, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5509386 0.0 0.020999828 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(0.9241, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -1.9880319 -0.02359632 1.6234342 11.527019\n",
      "activ tensor(0.9241, grad_fn=<CopyBackwards>) tensor(0.0802, grad_fn=<DivBackward0>) tensor(0.1697, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(0.9328, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(0.9241, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(0.9328, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5787379 0.0 0.02235295 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(0.9328, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -2.2424529 -0.034593306 2.0051265 11.679858\n",
      "activ tensor(0.9328, grad_fn=<CopyBackwards>) tensor(0.0799, grad_fn=<DivBackward0>) tensor(0.1481, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(0.9447, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(0.9328, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(0.9447, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56021875 0.0 0.02128525 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(0.9450, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -2.4402156 -0.02694177 1.9250585 11.875514\n",
      "activ tensor(0.9450, grad_fn=<CopyBackwards>) tensor(0.0796, grad_fn=<DivBackward0>) tensor(0.1508, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(0.9554, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(0.9450, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(0.9554, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57314295 0.0 0.022162665 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(0.9554, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -2.2293718 -0.030458655 2.2853725 12.066747\n",
      "activ tensor(0.9554, grad_fn=<CopyBackwards>) tensor(0.0792, grad_fn=<DivBackward0>) tensor(0.1423, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(0.9605, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(0.9554, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(0.9605, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5904639 0.0 0.022648945 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(0.9601, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -2.205913 -0.007372906 2.8073354 12.285693\n",
      "activ tensor(0.9601, grad_fn=<CopyBackwards>) tensor(0.0781, grad_fn=<DivBackward0>) tensor(0.1953, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(0.9714, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(0.9601, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(0.9714, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5707867 0.0 0.021824386 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(0.9713, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -2.5722585 -0.0241943 2.094291 12.508541\n",
      "activ tensor(0.9713, grad_fn=<CopyBackwards>) tensor(0.0777, grad_fn=<DivBackward0>) tensor(0.1518, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(0.9702, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(0.9713, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(0.9702, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.54408973 0.0 0.020883543 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(0.9703, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -3.2536907 -0.012895995 1.9479978 12.689418\n",
      "activ tensor(0.9703, grad_fn=<CopyBackwards>) tensor(0.0765, grad_fn=<DivBackward0>) tensor(0.1560, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(0.9794, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(0.9703, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(0.9794, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.58005255 0.0 0.022363521 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(0.9795, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -3.0390646 -0.04097874 2.2768338 12.776263\n",
      "activ tensor(0.9795, grad_fn=<CopyBackwards>) tensor(0.0767, grad_fn=<DivBackward0>) tensor(0.1466, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(0.9847, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(0.9795, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(0.9847, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5567569 0.0 0.021116111 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(0.9846, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -2.7729373 -0.046872336 2.42176 12.847447\n",
      "activ tensor(0.9846, grad_fn=<CopyBackwards>) tensor(0.0766, grad_fn=<DivBackward0>) tensor(0.1776, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(0.9980, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(0.9846, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(0.9980, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5796485 0.0 0.02251152 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(0.9981, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -2.785772 -0.025812238 2.730985 12.906046\n",
      "activ tensor(0.9981, grad_fn=<CopyBackwards>) tensor(0.0773, grad_fn=<DivBackward0>) tensor(0.1602, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.0181, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(0.9981, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.0181, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5732452 0.0 0.021898385 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.0180, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -2.7979267 -0.033643894 2.568811 12.925429\n",
      "activ tensor(1.0180, grad_fn=<CopyBackwards>) tensor(0.0788, grad_fn=<DivBackward0>) tensor(0.1664, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.0221, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.0180, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.0221, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5667695 0.0 0.021718673 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.0222, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -3.0709288 -0.015830552 2.3491576 12.905086\n",
      "activ tensor(1.0222, grad_fn=<CopyBackwards>) tensor(0.0792, grad_fn=<DivBackward0>) tensor(0.1638, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.0271, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.0222, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.0271, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56157684 0.0 0.02136982 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.0271, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -3.1920774 -0.0039355503 1.9817233 12.826248\n",
      "activ tensor(1.0271, grad_fn=<CopyBackwards>) tensor(0.0801, grad_fn=<DivBackward0>) tensor(0.1607, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.0355, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.0271, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.0355, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5589625 0.0 0.021306394 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.0354, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -2.7686353 -0.033274762 2.4869952 12.67078\n",
      "activ tensor(1.0354, grad_fn=<CopyBackwards>) tensor(0.0817, grad_fn=<DivBackward0>) tensor(0.1495, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.0373, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.0354, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.0373, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5847808 0.0 0.02243752 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.0372, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -2.7313013 -0.030310217 2.7152324 12.536761\n",
      "activ tensor(1.0372, grad_fn=<CopyBackwards>) tensor(0.0827, grad_fn=<DivBackward0>) tensor(0.2077, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.0443, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.0372, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.0443, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5514699 0.0 0.021137252 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.0444, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -2.7952068 -0.019549072 2.3911653 12.417006\n",
      "activ tensor(1.0444, grad_fn=<CopyBackwards>) tensor(0.0841, grad_fn=<DivBackward0>) tensor(0.1489, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.0420, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.0444, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.0420, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55917203 0.0 0.021401534 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.0421, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -2.5525858 -0.024450753 2.4126475 12.330537\n",
      "activ tensor(1.0421, grad_fn=<CopyBackwards>) tensor(0.0845, grad_fn=<DivBackward0>) tensor(0.1728, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.0497, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.0421, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.0497, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5726315 0.0 0.021972382 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.0498, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -2.7691557 -0.058328263 1.8415865 12.278398\n",
      "activ tensor(1.0498, grad_fn=<CopyBackwards>) tensor(0.0855, grad_fn=<DivBackward0>) tensor(0.1601, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.0509, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.0498, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.0509, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57171 0.0 0.022056954 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.0510, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -2.8774662 -0.06496301 2.240252 12.305141\n",
      "activ tensor(1.0510, grad_fn=<CopyBackwards>) tensor(0.0854, grad_fn=<DivBackward0>) tensor(0.2015, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.0593, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.0510, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.0593, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55316746 0.0 0.020925825 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.0595, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -3.110207 -0.037856672 2.794724 12.433187\n",
      "activ tensor(1.0595, grad_fn=<CopyBackwards>) tensor(0.0852, grad_fn=<DivBackward0>) tensor(0.1486, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.0669, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.0595, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.0669, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55717754 0.0 0.02110554 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.0666, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -3.3482604 -0.032708675 2.3907228 12.622356\n",
      "activ tensor(1.0666, grad_fn=<CopyBackwards>) tensor(0.0845, grad_fn=<DivBackward0>) tensor(0.1619, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.0719, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.0666, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.0719, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5722221 0.0 0.02219438 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.0719, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -2.7027009 -0.040000968 2.4072506 12.847753\n",
      "activ tensor(1.0719, grad_fn=<CopyBackwards>) tensor(0.0834, grad_fn=<DivBackward0>) tensor(0.1913, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.0767, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.0719, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.0767, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5557034 0.0 0.021306396 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.0769, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -3.041828 -0.037748806 2.2476895 13.076947\n",
      "activ tensor(1.0769, grad_fn=<CopyBackwards>) tensor(0.0824, grad_fn=<DivBackward0>) tensor(0.1684, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.0847, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.0769, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.0847, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5653203 0.0 0.021517819 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.0849, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -2.95486 -0.06341311 2.3835635 13.258829\n",
      "activ tensor(1.0849, grad_fn=<CopyBackwards>) tensor(0.0818, grad_fn=<DivBackward0>) tensor(0.1864, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.0913, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.0849, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.0913, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55369663 0.0 0.020989256 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.0912, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -2.9206567 -0.0453083 2.703277 13.472854\n",
      "activ tensor(1.0912, grad_fn=<CopyBackwards>) tensor(0.0810, grad_fn=<DivBackward0>) tensor(0.1780, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.0940, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.0912, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.0940, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5725292 0.0 0.022046382 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.0942, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -2.9470816 -0.031981234 3.1925945 13.687675\n",
      "activ tensor(1.0942, grad_fn=<CopyBackwards>) tensor(0.0799, grad_fn=<DivBackward0>) tensor(0.1726, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.0968, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.0942, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.0968, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57201713 0.0 0.02228952 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.0968, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -3.186557 -0.017768871 2.8938766 13.853838\n",
      "activ tensor(1.0968, grad_fn=<CopyBackwards>) tensor(0.0792, grad_fn=<DivBackward0>) tensor(0.2041, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.1028, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.0968, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.1028, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57355136 0.0 0.02208867 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.1027, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -3.8602011 -0.02266326 2.3054366 13.950338\n",
      "activ tensor(1.1027, grad_fn=<CopyBackwards>) tensor(0.0790, grad_fn=<DivBackward0>) tensor(0.1852, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.1029, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.1027, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.1029, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5640752 0.0 0.021570675 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.1028, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -3.6147811 -0.026039988 3.0292704 14.014871\n",
      "activ tensor(1.1028, grad_fn=<CopyBackwards>) tensor(0.0787, grad_fn=<DivBackward0>) tensor(0.2117, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.1137, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.1028, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.1137, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55454266 0.0 0.020925831 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.1136, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -3.3139048 -0.014594467 3.0887642 14.0338335\n",
      "activ tensor(1.1136, grad_fn=<CopyBackwards>) tensor(0.0793, grad_fn=<DivBackward0>) tensor(0.1644, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.1184, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.1136, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.1184, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.53954804 0.0 0.020418407 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.1183, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -3.372884 -0.02629157 3.2166576 14.030389\n",
      "activ tensor(1.1183, grad_fn=<CopyBackwards>) tensor(0.0797, grad_fn=<DivBackward0>) tensor(0.1718, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.1256, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.1183, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.1256, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.562515 0.0 0.021475533 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.1257, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -3.5125816 -0.03225857 3.3550477 14.007218\n",
      "activ tensor(1.1257, grad_fn=<CopyBackwards>) tensor(0.0804, grad_fn=<DivBackward0>) tensor(0.1713, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.1302, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.1257, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.1302, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5468825 0.0 0.020841256 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.1302, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -3.741007 -0.037776038 2.678976 13.976067\n",
      "activ tensor(1.1302, grad_fn=<CopyBackwards>) tensor(0.0809, grad_fn=<DivBackward0>) tensor(0.1856, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.1399, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.1302, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.1399, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56021893 0.0 0.021412106 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.1398, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -3.800874 -0.06789798 2.843392 13.937591\n",
      "activ tensor(1.1398, grad_fn=<CopyBackwards>) tensor(0.0818, grad_fn=<DivBackward0>) tensor(0.1689, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.1443, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.1398, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.1443, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57528573 0.0 0.022236666 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.1443, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -3.3536575 -0.07028943 3.0692108 13.961208\n",
      "activ tensor(1.1443, grad_fn=<CopyBackwards>) tensor(0.0820, grad_fn=<DivBackward0>) tensor(0.1996, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.1495, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.1443, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.1495, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5633477 0.0 0.021369822 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.1495, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -3.3803103 -0.045307763 3.303539 14.034311\n",
      "activ tensor(1.1495, grad_fn=<CopyBackwards>) tensor(0.0819, grad_fn=<DivBackward0>) tensor(0.1920, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.1628, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.1495, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.1628, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5624107 0.0 0.02161296 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.1627, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -3.1577244 -0.0244853 3.143145 14.095989\n",
      "activ tensor(1.1627, grad_fn=<CopyBackwards>) tensor(0.0825, grad_fn=<DivBackward0>) tensor(0.2003, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.1742, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.1627, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.1742, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55327326 0.0 0.02085183 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.1742, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -3.39398 -0.065746576 2.7682254 14.146907\n",
      "activ tensor(1.1742, grad_fn=<CopyBackwards>) tensor(0.0830, grad_fn=<DivBackward0>) tensor(0.2042, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.1838, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.1742, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.1838, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5738578 0.0 0.02202524 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.1839, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -3.4963264 -0.06545826 2.64852 14.196568\n",
      "activ tensor(1.1839, grad_fn=<CopyBackwards>) tensor(0.0834, grad_fn=<DivBackward0>) tensor(0.2023, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.1845, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.1839, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.1845, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5727337 0.0 0.022152096 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.1848, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -3.3852947 -0.055484656 2.7404456 14.222703\n",
      "activ tensor(1.1848, grad_fn=<CopyBackwards>) tensor(0.0833, grad_fn=<DivBackward0>) tensor(0.1774, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.1890, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.1848, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.1890, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55623055 0.0 0.021168968 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.1889, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -3.7465878 -0.064904496 3.4853873 14.288281\n",
      "activ tensor(1.1889, grad_fn=<CopyBackwards>) tensor(0.0832, grad_fn=<DivBackward0>) tensor(0.1920, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.1924, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.1889, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.1924, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5664591 0.0 0.021686958 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.1926, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -3.8327174 -0.055606753 3.0602815 14.375059\n",
      "activ tensor(1.1926, grad_fn=<CopyBackwards>) tensor(0.0830, grad_fn=<DivBackward0>) tensor(0.1776, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.1970, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.1926, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.1970, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.582069 0.0 0.022331806 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.1971, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -3.511279 -0.038956583 2.7472844 14.442183\n",
      "activ tensor(1.1971, grad_fn=<CopyBackwards>) tensor(0.0829, grad_fn=<DivBackward0>) tensor(0.2442, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.2128, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.1971, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.2128, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57263154 0.0 0.021940669 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.2128, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -3.651861 -0.042458847 3.101202 14.490556\n",
      "activ tensor(1.2128, grad_fn=<CopyBackwards>) tensor(0.0837, grad_fn=<DivBackward0>) tensor(0.1839, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.2245, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.2128, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.2245, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5430119 0.0 0.020492408 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.2245, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -3.4428303 -0.030581865 2.8097038 14.574818\n",
      "activ tensor(1.2245, grad_fn=<CopyBackwards>) tensor(0.0840, grad_fn=<DivBackward0>) tensor(0.1881, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.2310, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.2245, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.2310, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5788392 0.0 0.022162665 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.2310, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -3.5813296 -0.028561156 3.4623117 14.651019\n",
      "activ tensor(1.2310, grad_fn=<CopyBackwards>) tensor(0.0840, grad_fn=<DivBackward0>) tensor(0.2008, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.2430, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.2310, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.2430, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57027334 0.0 0.021887813 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.2430, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -3.5351176 -0.039270237 3.7560203 14.756314\n",
      "activ tensor(1.2430, grad_fn=<CopyBackwards>) tensor(0.0842, grad_fn=<DivBackward0>) tensor(0.2080, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.2503, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.2430, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.2503, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56790525 0.0 0.021898383 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.2505, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -3.7944493 -0.050121825 3.3245273 14.8737955\n",
      "activ tensor(1.2505, grad_fn=<CopyBackwards>) tensor(0.0841, grad_fn=<DivBackward0>) tensor(0.1904, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.2582, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.2505, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.2582, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5711974 0.0 0.02177153 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.2583, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -4.460607 -0.048109885 3.106174 14.992\n",
      "activ tensor(1.2583, grad_fn=<CopyBackwards>) tensor(0.0839, grad_fn=<DivBackward0>) tensor(0.2254, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.2627, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.2583, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.2627, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5661488 0.0 0.021528391 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.2627, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -4.1760483 -0.05625952 3.417809 15.130336\n",
      "activ tensor(1.2627, grad_fn=<CopyBackwards>) tensor(0.0835, grad_fn=<DivBackward0>) tensor(0.1982, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.2613, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.2627, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.2613, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57140225 0.0 0.021930099 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.2613, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -3.858024 -0.06517872 3.9379125 15.3068905\n",
      "activ tensor(1.2613, grad_fn=<CopyBackwards>) tensor(0.0824, grad_fn=<DivBackward0>) tensor(0.2322, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.2628, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.2613, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.2628, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5491277 0.0 0.020798974 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.2628, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -3.974978 -0.05517718 3.744094 15.494982\n",
      "activ tensor(1.2628, grad_fn=<CopyBackwards>) tensor(0.0815, grad_fn=<DivBackward0>) tensor(0.2075, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.2685, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.2628, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.2685, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5782317 0.0 0.022310663 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.2684, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -4.2035437 -0.04197575 3.8622046 15.656052\n",
      "activ tensor(1.2684, grad_fn=<CopyBackwards>) tensor(0.0810, grad_fn=<DivBackward0>) tensor(0.2213, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.2644, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.2684, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.2644, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56862706 0.0 0.02177153 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.2643, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -4.431564 -0.06493656 3.436142 15.786182\n",
      "activ tensor(1.2643, grad_fn=<CopyBackwards>) tensor(0.0801, grad_fn=<DivBackward0>) tensor(0.2132, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.2652, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.2643, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.2652, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5649056 0.0 0.021760957 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.2652, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -4.3728914 -0.05671178 3.264667 15.869919\n",
      "activ tensor(1.2652, grad_fn=<CopyBackwards>) tensor(0.0797, grad_fn=<DivBackward0>) tensor(0.1865, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.2762, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.2652, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.2762, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5797494 0.0 0.022215523 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.2763, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -3.958615 -0.06170278 3.9898782 15.88639\n",
      "activ tensor(1.2763, grad_fn=<CopyBackwards>) tensor(0.0803, grad_fn=<DivBackward0>) tensor(0.2171, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.2886, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.2763, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.2886, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5571776 0.0 0.021380393 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.2886, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -4.076165 -0.06171364 3.751246 15.914195\n",
      "activ tensor(1.2886, grad_fn=<CopyBackwards>) tensor(0.0810, grad_fn=<DivBackward0>) tensor(0.2317, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.3004, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.2886, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.3004, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5723243 0.0 0.022099238 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.3003, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -3.6180832 -0.06283139 3.7782083 15.911248\n",
      "activ tensor(1.3003, grad_fn=<CopyBackwards>) tensor(0.0817, grad_fn=<DivBackward0>) tensor(0.1904, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.3084, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.3003, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.3084, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.561994 0.0 0.021750387 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.3084, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -3.9354048 -0.059066653 3.448565 15.877795\n",
      "activ tensor(1.3084, grad_fn=<CopyBackwards>) tensor(0.0824, grad_fn=<DivBackward0>) tensor(0.2958, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.3160, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.3084, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.3160, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57150483 0.0 0.022078095 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.3159, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -4.020591 -0.037322387 3.0882182 15.835444\n",
      "activ tensor(1.3159, grad_fn=<CopyBackwards>) tensor(0.0831, grad_fn=<DivBackward0>) tensor(0.2291, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.3232, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.3159, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.3232, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5630355 0.0 0.02152839 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.3232, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -3.7929564 -0.03836144 3.6455312 15.797267\n",
      "activ tensor(1.3232, grad_fn=<CopyBackwards>) tensor(0.0838, grad_fn=<DivBackward0>) tensor(0.2040, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.3263, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.3232, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.3263, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57792765 0.0 0.022247236 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.3264, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -4.540292 -0.058858566 3.910254 15.759535\n",
      "activ tensor(1.3264, grad_fn=<CopyBackwards>) tensor(0.0842, grad_fn=<DivBackward0>) tensor(0.2276, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.3355, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.3264, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.3355, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.58277357 0.0 0.022458663 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.3357, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -4.262503 -0.05860995 3.7595096 15.767549\n",
      "activ tensor(1.3357, grad_fn=<CopyBackwards>) tensor(0.0847, grad_fn=<DivBackward0>) tensor(0.1993, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.3457, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.3357, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.3457, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5813643 0.0 0.022331806 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.3458, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -4.131127 -0.070918806 3.2978425 15.800299\n",
      "activ tensor(1.3458, grad_fn=<CopyBackwards>) tensor(0.0852, grad_fn=<DivBackward0>) tensor(0.2332, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.3491, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.3458, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.3491, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56365967 0.0 0.021464963 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.3493, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -4.1574874 -0.0759371 3.575558 15.8720875\n",
      "activ tensor(1.3493, grad_fn=<CopyBackwards>) tensor(0.0850, grad_fn=<DivBackward0>) tensor(0.2275, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.3566, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.3493, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.3566, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5640751 0.0 0.021517817 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.3567, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -3.9552722 -0.07923627 3.6940265 15.99998\n",
      "activ tensor(1.3567, grad_fn=<CopyBackwards>) tensor(0.0848, grad_fn=<DivBackward0>) tensor(0.2439, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.3569, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.3567, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.3569, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5885758 0.0 0.022511518 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.3570, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -4.4535365 -0.072216436 3.8287272 16.177952\n",
      "activ tensor(1.3570, grad_fn=<CopyBackwards>) tensor(0.0839, grad_fn=<DivBackward0>) tensor(0.2765, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.3621, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.3570, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.3621, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56594175 0.0 0.021782102 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.3623, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -3.9894748 -0.07345864 4.519282 16.384205\n",
      "activ tensor(1.3623, grad_fn=<CopyBackwards>) tensor(0.0831, grad_fn=<DivBackward0>) tensor(0.2255, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.3691, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.3623, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.3691, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5650094 0.0 0.021538962 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.3692, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -4.3933854 -0.060250364 3.784876 16.593176\n",
      "activ tensor(1.3692, grad_fn=<CopyBackwards>) tensor(0.0825, grad_fn=<DivBackward0>) tensor(0.2094, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.3688, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.3692, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.3688, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5734495 0.0 0.02209924 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.3687, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -5.0443745 -0.05547533 3.659583 16.78325\n",
      "activ tensor(1.3687, grad_fn=<CopyBackwards>) tensor(0.0816, grad_fn=<DivBackward0>) tensor(0.2624, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.3690, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.3687, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.3690, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.569142 0.0 0.021792674 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.3688, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -4.7371707 -0.07275316 4.213758 16.956995\n",
      "activ tensor(1.3688, grad_fn=<CopyBackwards>) tensor(0.0807, grad_fn=<DivBackward0>) tensor(0.2145, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.3744, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.3688, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.3744, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5727338 0.0 0.022014668 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.3745, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -4.39977 -0.068655565 4.352033 17.087322\n",
      "activ tensor(1.3745, grad_fn=<CopyBackwards>) tensor(0.0804, grad_fn=<DivBackward0>) tensor(0.2142, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.3806, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.3745, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.3806, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.568524 0.0 0.021655247 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.3806, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -4.5851355 -0.041154787 4.5828657 17.206837\n",
      "activ tensor(1.3806, grad_fn=<CopyBackwards>) tensor(0.0802, grad_fn=<DivBackward0>) tensor(0.2440, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.3893, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.3806, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.3893, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5644905 0.0 0.021634102 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.3892, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -4.894038 -0.06949961 4.2549453 17.290495\n",
      "activ tensor(1.3892, grad_fn=<CopyBackwards>) tensor(0.0803, grad_fn=<DivBackward0>) tensor(0.2262, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.3965, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.3892, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.3965, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5858819 0.0 0.022564376 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.3964, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -5.099716 -0.064899266 4.0686836 17.341381\n",
      "activ tensor(1.3964, grad_fn=<CopyBackwards>) tensor(0.0805, grad_fn=<DivBackward0>) tensor(0.3044, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.3970, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.3964, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.3970, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.54944766 0.0 0.020756688 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.3968, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -4.960604 -0.040079195 4.027148 17.377335\n",
      "activ tensor(1.3968, grad_fn=<CopyBackwards>) tensor(0.0804, grad_fn=<DivBackward0>) tensor(0.2314, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.4101, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.3968, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.4101, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55717766 0.0 0.021084398 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.4099, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -4.5336556 -0.042898558 4.463412 17.413761\n",
      "activ tensor(1.4099, grad_fn=<CopyBackwards>) tensor(0.0810, grad_fn=<DivBackward0>) tensor(0.2617, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.4213, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.4099, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.4213, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5553872 0.0 0.02103154 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.4213, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -4.4996448 -0.066992864 4.614944 17.422953\n",
      "activ tensor(1.4213, grad_fn=<CopyBackwards>) tensor(0.0816, grad_fn=<DivBackward0>) tensor(0.2413, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.4295, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.4213, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.4295, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5700678 0.0 0.02184553 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.4296, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -4.428434 -0.061452243 4.1293807 17.43474\n",
      "activ tensor(1.4296, grad_fn=<CopyBackwards>) tensor(0.0820, grad_fn=<DivBackward0>) tensor(0.2124, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.4411, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.4296, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.4411, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5578083 0.0 0.021221824 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.4410, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -4.5645204 -0.063180655 4.1892447 17.436047\n",
      "activ tensor(1.4410, grad_fn=<CopyBackwards>) tensor(0.0826, grad_fn=<DivBackward0>) tensor(0.2712, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.4495, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.4410, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.4495, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56749237 0.0 0.021676388 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.4496, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -4.640528 -0.09611585 3.7241426 17.450169\n",
      "activ tensor(1.4496, grad_fn=<CopyBackwards>) tensor(0.0831, grad_fn=<DivBackward0>) tensor(0.2714, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.4565, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.4496, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.4565, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5760999 0.0 0.022141522 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.4567, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -4.524137 -0.08270024 4.1828074 17.499168\n",
      "activ tensor(1.4567, grad_fn=<CopyBackwards>) tensor(0.0832, grad_fn=<DivBackward0>) tensor(0.2298, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.4563, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.4567, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.4563, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5631397 0.0 0.021549532 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.4563, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -5.022196 -0.07759549 4.761804 17.585974\n",
      "activ tensor(1.4563, grad_fn=<CopyBackwards>) tensor(0.0828, grad_fn=<DivBackward0>) tensor(0.2428, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.4655, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.4563, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.4655, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57047886 0.0 0.021655247 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.4654, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -4.924576 -0.102408856 4.087641 17.676847\n",
      "activ tensor(1.4654, grad_fn=<CopyBackwards>) tensor(0.0829, grad_fn=<DivBackward0>) tensor(0.2531, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.4770, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.4654, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.4770, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5610548 0.0 0.021475535 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.4773, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -4.5873284 -0.07385846 4.0445952 17.764803\n",
      "activ tensor(1.4773, grad_fn=<CopyBackwards>) tensor(0.0832, grad_fn=<DivBackward0>) tensor(0.2124, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.4824, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.4773, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.4824, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57252914 0.0 0.021813814 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.4824, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -4.849646 -0.079223126 4.1051626 17.867239\n",
      "activ tensor(1.4824, grad_fn=<CopyBackwards>) tensor(0.0830, grad_fn=<DivBackward0>) tensor(0.2622, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.4916, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.4824, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.4916, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.58777857 0.0 0.02295551 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.4917, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -4.6842422 -0.08086769 4.306116 17.961088\n",
      "activ tensor(1.4917, grad_fn=<CopyBackwards>) tensor(0.0831, grad_fn=<DivBackward0>) tensor(0.2568, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.4937, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.4917, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.4937, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55833316 0.0 0.021264108 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.4939, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -5.0559497 -0.09255411 4.6127753 18.039572\n",
      "activ tensor(1.4939, grad_fn=<CopyBackwards>) tensor(0.0828, grad_fn=<DivBackward0>) tensor(0.3081, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.5056, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.4939, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.5056, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5496607 0.0 0.020714402 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.5057, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -4.6931376 -0.0744428 4.827822 18.151386\n",
      "activ tensor(1.5057, grad_fn=<CopyBackwards>) tensor(0.0830, grad_fn=<DivBackward0>) tensor(0.2653, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.5140, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.5057, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.5140, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5692452 0.0 0.021708101 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.5141, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -5.014155 -0.077694625 4.5859857 18.251385\n",
      "activ tensor(1.5141, grad_fn=<CopyBackwards>) tensor(0.0830, grad_fn=<DivBackward0>) tensor(0.2255, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.5166, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.5141, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.5166, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.554965 0.0 0.021179538 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.5165, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -5.598255 -0.0877862 4.088589 18.35282\n",
      "activ tensor(1.5165, grad_fn=<CopyBackwards>) tensor(0.0826, grad_fn=<DivBackward0>) tensor(0.3247, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.5230, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.5165, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.5230, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5688331 0.0 0.021581246 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.5232, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -5.296053 -0.03838118 4.9034715 18.470356\n",
      "activ tensor(1.5232, grad_fn=<CopyBackwards>) tensor(0.0825, grad_fn=<DivBackward0>) tensor(0.2629, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.5268, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.5232, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.5268, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56635565 0.0 0.021951241 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.5267, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -4.9530396 -0.047355495 5.073727 18.604118\n",
      "activ tensor(1.5267, grad_fn=<CopyBackwards>) tensor(0.0821, grad_fn=<DivBackward0>) tensor(0.2344, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.5309, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.5267, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.5309, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5672861 0.0 0.02162353 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.5306, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -5.190941 -0.082492456 4.9870534 18.710695\n",
      "activ tensor(1.5306, grad_fn=<CopyBackwards>) tensor(0.0818, grad_fn=<DivBackward0>) tensor(0.2446, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.5369, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.5306, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.5369, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5527435 0.0 0.020809542 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.5371, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -5.5852118 -0.07194465 5.0540233 18.832546\n",
      "activ tensor(1.5371, grad_fn=<CopyBackwards>) tensor(0.0816, grad_fn=<DivBackward0>) tensor(0.2446, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.5456, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.5371, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.5456, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5382435 0.0 0.020217553 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.5456, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -5.757289 -0.06784092 4.446155 18.945684\n",
      "activ tensor(1.5456, grad_fn=<CopyBackwards>) tensor(0.0816, grad_fn=<DivBackward0>) tensor(0.2679, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.5498, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.5456, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.5498, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5467754 0.0 0.020798976 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.5498, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -5.534439 -0.0685271 4.8275633 19.03028\n",
      "activ tensor(1.5498, grad_fn=<CopyBackwards>) tensor(0.0814, grad_fn=<DivBackward0>) tensor(0.2802, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.5588, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.5498, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.5588, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56449056 0.0 0.021475531 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.5589, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -5.1257467 -0.092828184 5.1388154 19.12126\n",
      "activ tensor(1.5589, grad_fn=<CopyBackwards>) tensor(0.0815, grad_fn=<DivBackward0>) tensor(0.2749, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.5595, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.5589, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.5595, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.58176726 0.0 0.022416377 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.5595, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -5.1296115 -0.08375902 5.085029 19.219648\n",
      "activ tensor(1.5595, grad_fn=<CopyBackwards>) tensor(0.0811, grad_fn=<DivBackward0>) tensor(0.2777, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.5701, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.5595, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.5701, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5778259 0.0 0.022173237 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.5698, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -5.1245084 -0.09616795 4.9305167 19.298887\n",
      "activ tensor(1.5698, grad_fn=<CopyBackwards>) tensor(0.0813, grad_fn=<DivBackward0>) tensor(0.2649, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.5742, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.5698, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.5742, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5696566 0.0 0.022004098 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.5743, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -5.2786736 -0.08160964 4.5488257 19.364237\n",
      "activ tensor(1.5743, grad_fn=<CopyBackwards>) tensor(0.0813, grad_fn=<DivBackward0>) tensor(0.2657, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.5883, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.5743, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.5883, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5664591 0.0 0.021634102 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.5884, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -5.2692494 -0.06061349 4.5802393 19.415405\n",
      "activ tensor(1.5884, grad_fn=<CopyBackwards>) tensor(0.0818, grad_fn=<DivBackward0>) tensor(0.2592, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.5964, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.5884, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.5964, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5725292 0.0 0.021729244 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.5963, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -5.295256 -0.08198828 4.745542 19.435925\n",
      "activ tensor(1.5963, grad_fn=<CopyBackwards>) tensor(0.0821, grad_fn=<DivBackward0>) tensor(0.2500, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.6050, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.5963, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.6050, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5776233 0.0 0.022215523 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.6051, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -5.657364 -0.100325584 5.3499985 19.431862\n",
      "activ tensor(1.6051, grad_fn=<CopyBackwards>) tensor(0.0826, grad_fn=<DivBackward0>) tensor(0.2689, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.6135, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.6051, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.6135, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57426614 0.0 0.022046383 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.6135, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -5.304067 -0.09615363 4.817989 19.475134\n",
      "activ tensor(1.6135, grad_fn=<CopyBackwards>) tensor(0.0828, grad_fn=<DivBackward0>) tensor(0.2859, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.6109, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.6135, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.6109, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56759566 0.0 0.0218561 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.6109, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -5.251373 -0.09584737 4.370909 19.527498\n",
      "activ tensor(1.6109, grad_fn=<CopyBackwards>) tensor(0.0825, grad_fn=<DivBackward0>) tensor(0.2681, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.6166, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.6109, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.6166, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57528573 0.0 0.021982955 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.6166, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -5.482806 -0.0966072 4.9627414 19.5938\n",
      "activ tensor(1.6166, grad_fn=<CopyBackwards>) tensor(0.0825, grad_fn=<DivBackward0>) tensor(0.2944, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.6284, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.6166, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.6284, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5531674 0.0 0.020767257 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.6285, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -5.215124 -0.0867185 4.8039045 19.69146\n",
      "activ tensor(1.6285, grad_fn=<CopyBackwards>) tensor(0.0827, grad_fn=<DivBackward0>) tensor(0.3223, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.6390, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.6285, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.6390, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5555979 0.0 0.021211252 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.6391, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -5.701761 -0.075658366 5.263993 19.801704\n",
      "activ tensor(1.6391, grad_fn=<CopyBackwards>) tensor(0.0828, grad_fn=<DivBackward0>) tensor(0.2790, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.6511, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.6391, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.6511, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5668728 0.0 0.021517819 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.6513, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -5.187296 -0.10660206 5.4646816 19.914536\n",
      "activ tensor(1.6513, grad_fn=<CopyBackwards>) tensor(0.0829, grad_fn=<DivBackward0>) tensor(0.2522, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.6593, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.6513, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.6593, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5648018 0.0 0.021665817 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.6595, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -5.6049376 -0.09324455 4.943805 20.064016\n",
      "activ tensor(1.6595, grad_fn=<CopyBackwards>) tensor(0.0827, grad_fn=<DivBackward0>) tensor(0.3211, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.6604, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.6595, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.6604, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5738581 0.0 0.02209924 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.6604, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -6.1645575 -0.07893691 4.9522977 20.18542\n",
      "activ tensor(1.6604, grad_fn=<CopyBackwards>) tensor(0.0823, grad_fn=<DivBackward0>) tensor(0.3338, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.6653, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.6604, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.6653, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5612635 0.0 0.021549534 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.6654, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -5.8279624 -0.100272894 5.314518 20.296194\n",
      "activ tensor(1.6654, grad_fn=<CopyBackwards>) tensor(0.0821, grad_fn=<DivBackward0>) tensor(0.3040, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.6717, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.6654, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.6717, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5744703 0.0 0.021908958 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.6718, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -5.526563 -0.102915004 5.840503 20.415344\n",
      "activ tensor(1.6718, grad_fn=<CopyBackwards>) tensor(0.0819, grad_fn=<DivBackward0>) tensor(0.3143, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.6772, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.6718, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.6772, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.550832 0.0 0.021042112 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.6772, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -5.7993574 -0.08837948 5.5676675 20.52939\n",
      "activ tensor(1.6772, grad_fn=<CopyBackwards>) tensor(0.0817, grad_fn=<DivBackward0>) tensor(0.2727, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.6809, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.6772, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.6809, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5654239 0.0 0.021475535 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.6809, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -6.26156 -0.07823002 5.4897733 20.642172\n",
      "activ tensor(1.6809, grad_fn=<CopyBackwards>) tensor(0.0814, grad_fn=<DivBackward0>) tensor(0.2803, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.6850, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.6809, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.6850, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5706841 0.0 0.021834956 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.6851, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -6.4137044 -0.0595721 5.264027 20.735708\n",
      "activ tensor(1.6851, grad_fn=<CopyBackwards>) tensor(0.0813, grad_fn=<DivBackward0>) tensor(0.2921, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.6983, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.6851, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.6983, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.575082 0.0 0.022056954 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.6981, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -6.0976334 -0.08131165 5.2596583 20.803995\n",
      "activ tensor(1.6981, grad_fn=<CopyBackwards>) tensor(0.0816, grad_fn=<DivBackward0>) tensor(0.3022, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.7093, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.6981, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.7093, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5541198 0.0 0.021105539 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.7092, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -5.717245 -0.084044114 6.00246 20.850275\n",
      "activ tensor(1.7092, grad_fn=<CopyBackwards>) tensor(0.0820, grad_fn=<DivBackward0>) tensor(0.2623, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.7123, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.7092, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.7123, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56749254 0.0 0.021739814 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.7125, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -5.7266707 -0.09193276 5.5934815 20.891382\n",
      "activ tensor(1.7125, grad_fn=<CopyBackwards>) tensor(0.0820, grad_fn=<DivBackward0>) tensor(0.3332, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.7213, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.7125, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.7213, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5711973 0.0 0.021824384 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.7212, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -5.6730037 -0.08178641 5.4556355 20.964018\n",
      "activ tensor(1.7212, grad_fn=<CopyBackwards>) tensor(0.0821, grad_fn=<DivBackward0>) tensor(0.3283, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.7325, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.7212, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.7325, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5764049 0.0 0.022226093 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.7324, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -5.86276 -0.08689685 5.3385797 21.031702\n",
      "activ tensor(1.7324, grad_fn=<CopyBackwards>) tensor(0.0824, grad_fn=<DivBackward0>) tensor(0.3087, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.7398, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.7324, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.7398, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56532025 0.0 0.021718673 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.7396, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -5.8613443 -0.114752814 4.9979157 21.118097\n",
      "activ tensor(1.7396, grad_fn=<CopyBackwards>) tensor(0.0824, grad_fn=<DivBackward0>) tensor(0.2994, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.7370, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.7396, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.7370, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5541197 0.0 0.021094967 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.7368, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -5.8572288 -0.09963294 5.646029 21.205706\n",
      "activ tensor(1.7368, grad_fn=<CopyBackwards>) tensor(0.0819, grad_fn=<DivBackward0>) tensor(0.3281, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.7447, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.7368, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.7447, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5773187 0.0 0.022278951 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.7448, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -6.3393264 -0.09604655 5.79362 21.320143\n",
      "activ tensor(1.7448, grad_fn=<CopyBackwards>) tensor(0.0818, grad_fn=<DivBackward0>) tensor(0.3469, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.7453, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.7448, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.7453, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5702734 0.0 0.021845529 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.7454, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -5.7568693 -0.093022294 5.441461 21.43179\n",
      "activ tensor(1.7454, grad_fn=<CopyBackwards>) tensor(0.0814, grad_fn=<DivBackward0>) tensor(0.3028, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.7529, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.7454, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.7529, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5634517 0.0 0.021285249 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.7530, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -6.0285287 -0.095685944 5.020736 21.53646\n",
      "activ tensor(1.7530, grad_fn=<CopyBackwards>) tensor(0.0814, grad_fn=<DivBackward0>) tensor(0.3022, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.7622, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.7530, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.7622, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5645944 0.0 0.021380391 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.7622, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -6.0033374 -0.12883674 5.400804 21.628101\n",
      "activ tensor(1.7622, grad_fn=<CopyBackwards>) tensor(0.0815, grad_fn=<DivBackward0>) tensor(0.3195, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.7716, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.7622, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.7716, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56934774 0.0 0.021993523 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.7716, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -5.809193 -0.1153065 5.7076826 21.70148\n",
      "activ tensor(1.7716, grad_fn=<CopyBackwards>) tensor(0.0816, grad_fn=<DivBackward0>) tensor(0.3023, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.7782, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.7716, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.7782, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5643867 0.0 0.021760957 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.7783, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -6.5060444 -0.13657379 5.6474204 21.788275\n",
      "activ tensor(1.7783, grad_fn=<CopyBackwards>) tensor(0.0816, grad_fn=<DivBackward0>) tensor(0.3486, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.7819, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.7783, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.7819, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5788393 0.0 0.022405805 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.7818, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -5.8024883 -0.105842434 6.13279 21.88484\n",
      "activ tensor(1.7818, grad_fn=<CopyBackwards>) tensor(0.0814, grad_fn=<DivBackward0>) tensor(0.3405, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.7879, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.7818, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.7879, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5622026 0.0 0.021591818 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.7880, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -6.210664 -0.08160783 5.5075417 21.97534\n",
      "activ tensor(1.7880, grad_fn=<CopyBackwards>) tensor(0.0814, grad_fn=<DivBackward0>) tensor(0.3085, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.8015, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.7880, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.8015, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5670793 0.0 0.021560106 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.8017, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -6.6919174 -0.08994304 5.4408174 22.0413\n",
      "activ tensor(1.8017, grad_fn=<CopyBackwards>) tensor(0.0817, grad_fn=<DivBackward0>) tensor(0.2989, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.8186, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.8017, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.8186, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56480193 0.0 0.02169753 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.8186, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -6.3830233 -0.09797165 6.180268 22.086662\n",
      "activ tensor(1.8186, grad_fn=<CopyBackwards>) tensor(0.0823, grad_fn=<DivBackward0>) tensor(0.3243, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.8279, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.8186, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.8279, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5687301 0.0 0.021898385 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.8279, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -6.0841346 -0.103845544 6.206776 22.148699\n",
      "activ tensor(1.8279, grad_fn=<CopyBackwards>) tensor(0.0825, grad_fn=<DivBackward0>) tensor(0.3548, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.8336, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.8279, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.8336, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57487804 0.0 0.022014668 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.8336, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -6.429887 -0.077169985 6.3533607 22.199425\n",
      "activ tensor(1.8336, grad_fn=<CopyBackwards>) tensor(0.0826, grad_fn=<DivBackward0>) tensor(0.3815, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.8286, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.8336, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.8286, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55580896 0.0 0.020946968 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.8285, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -6.9192085 -0.098739624 5.9548697 22.283667\n",
      "activ tensor(1.8285, grad_fn=<CopyBackwards>) tensor(0.0821, grad_fn=<DivBackward0>) tensor(0.3385, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.8315, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.8285, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.8315, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5588576 0.0 0.021369822 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.8316, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -7.057951 -0.10501357 5.8348875 22.382734\n",
      "activ tensor(1.8316, grad_fn=<CopyBackwards>) tensor(0.0818, grad_fn=<DivBackward0>) tensor(0.3428, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.8384, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.8316, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.8384, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.58065814 0.0 0.022374092 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.8384, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -6.665558 -0.096800484 6.0945225 22.487291\n",
      "activ tensor(1.8384, grad_fn=<CopyBackwards>) tensor(0.0818, grad_fn=<DivBackward0>) tensor(0.3552, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.8553, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.8384, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.8553, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5557036 0.0 0.021042112 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.8552, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -6.3021207 -0.12359314 6.432799 22.611238\n",
      "activ tensor(1.8552, grad_fn=<CopyBackwards>) tensor(0.0820, grad_fn=<DivBackward0>) tensor(0.3542, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.8597, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.8552, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.8597, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56417924 0.0 0.02167639 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.8597, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -6.325619 -0.079646334 6.417475 22.763844\n",
      "activ tensor(1.8597, grad_fn=<CopyBackwards>) tensor(0.0817, grad_fn=<DivBackward0>) tensor(0.3504, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.8567, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.8597, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.8567, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5610549 0.0 0.021295821 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.8566, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -6.3888206 -0.09551677 5.8619866 22.908546\n",
      "activ tensor(1.8566, grad_fn=<CopyBackwards>) tensor(0.0810, grad_fn=<DivBackward0>) tensor(0.3306, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.8595, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.8566, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.8595, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5673894 0.0 0.0217821 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.8595, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -6.4988546 -0.11385024 6.0111465 23.036364\n",
      "activ tensor(1.8595, grad_fn=<CopyBackwards>) tensor(0.0807, grad_fn=<DivBackward0>) tensor(0.3096, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.8663, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.8595, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.8663, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5526375 0.0 0.021380393 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.8663, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -6.468625 -0.12298689 5.756879 23.138237\n",
      "activ tensor(1.8663, grad_fn=<CopyBackwards>) tensor(0.0807, grad_fn=<DivBackward0>) tensor(0.4248, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.8660, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.8663, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.8660, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5628274 0.0 0.021676388 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.8659, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -6.6706276 -0.11634416 6.097237 23.231213\n",
      "activ tensor(1.8659, grad_fn=<CopyBackwards>) tensor(0.0803, grad_fn=<DivBackward0>) tensor(0.3537, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.8747, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.8659, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.8747, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57099205 0.0 0.02169753 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.8746, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -6.7462916 -0.10999162 6.654427 23.316345\n",
      "activ tensor(1.8746, grad_fn=<CopyBackwards>) tensor(0.0804, grad_fn=<DivBackward0>) tensor(0.4187, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.8738, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.8746, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.8738, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5739601 0.0 0.021898385 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.8737, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -6.365229 -0.12224496 5.7839904 23.39419\n",
      "activ tensor(1.8737, grad_fn=<CopyBackwards>) tensor(0.0801, grad_fn=<DivBackward0>) tensor(0.3450, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.8873, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.8737, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.8873, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5632437 0.0 0.021634104 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.8872, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -6.6731873 -0.081550345 5.7369685 23.45802\n",
      "activ tensor(1.8872, grad_fn=<CopyBackwards>) tensor(0.0804, grad_fn=<DivBackward0>) tensor(0.3461, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.8918, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.8872, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.8918, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5626191 0.0 0.021327537 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.8919, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -6.592901 -0.09157586 6.0272045 23.491219\n",
      "activ tensor(1.8919, grad_fn=<CopyBackwards>) tensor(0.0805, grad_fn=<DivBackward0>) tensor(0.3359, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.8941, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.8919, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.8941, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5711972 0.0 0.022004098 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.8941, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -6.6435647 -0.12717041 6.2450852 23.516527\n",
      "activ tensor(1.8941, grad_fn=<CopyBackwards>) tensor(0.0805, grad_fn=<DivBackward0>) tensor(0.3477, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.8995, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.8941, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.8995, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.574878 0.0 0.022141524 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.8995, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -6.998089 -0.10137391 6.459518 23.558838\n",
      "activ tensor(1.8995, grad_fn=<CopyBackwards>) tensor(0.0806, grad_fn=<DivBackward0>) tensor(0.3058, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.9071, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.8995, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.9071, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56676924 0.0 0.021750387 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.9073, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -6.368805 -0.11984902 6.425838 23.606075\n",
      "activ tensor(1.9073, grad_fn=<CopyBackwards>) tensor(0.0808, grad_fn=<DivBackward0>) tensor(0.3511, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.9196, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.9073, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.9196, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5702733 0.0 0.021877242 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.9196, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -6.8119645 -0.1274725 6.2695317 23.688831\n",
      "activ tensor(1.9196, grad_fn=<CopyBackwards>) tensor(0.0810, grad_fn=<DivBackward0>) tensor(0.3783, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.9303, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.9196, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.9303, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57201713 0.0 0.021729242 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.9305, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -7.2178884 -0.1372059 5.9818773 23.779385\n",
      "activ tensor(1.9305, grad_fn=<CopyBackwards>) tensor(0.0812, grad_fn=<DivBackward0>) tensor(0.3687, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.9311, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.9305, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.9311, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5531672 0.0 0.021200681 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.9312, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -6.9136305 -0.13656844 6.774183 23.903534\n",
      "activ tensor(1.9312, grad_fn=<CopyBackwards>) tensor(0.0808, grad_fn=<DivBackward0>) tensor(0.3887, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.9325, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.9312, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.9325, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57068425 0.0 0.02201467 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.9325, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -6.6785746 -0.12618555 6.983512 24.050703\n",
      "activ tensor(1.9325, grad_fn=<CopyBackwards>) tensor(0.0804, grad_fn=<DivBackward0>) tensor(0.3612, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.9404, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.9325, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.9404, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5733473 0.0 0.022215523 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.9405, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -7.036205 -0.11445038 6.6848464 24.212166\n",
      "activ tensor(1.9405, grad_fn=<CopyBackwards>) tensor(0.0801, grad_fn=<DivBackward0>) tensor(0.3569, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.9504, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.9405, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.9504, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.58216995 0.0 0.022268381 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.9504, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -7.5883913 -0.10521087 6.7673144 24.362465\n",
      "activ tensor(1.9504, grad_fn=<CopyBackwards>) tensor(0.0801, grad_fn=<DivBackward0>) tensor(0.3538, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.9544, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.9504, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.9544, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5425802 0.0 0.020502977 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.9543, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -7.675726 -0.11300301 6.274466 24.486578\n",
      "activ tensor(1.9543, grad_fn=<CopyBackwards>) tensor(0.0798, grad_fn=<DivBackward0>) tensor(0.3805, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.9628, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.9543, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.9628, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5549651 0.0 0.021158395 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.9629, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -7.237238 -0.1294331 6.8232036 24.579693\n",
      "activ tensor(1.9629, grad_fn=<CopyBackwards>) tensor(0.0799, grad_fn=<DivBackward0>) tensor(0.3754, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.9657, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.9629, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.9657, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5596955 0.0 0.02128525 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.9656, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -6.894034 -0.10873109 7.1702905 24.659864\n",
      "activ tensor(1.9656, grad_fn=<CopyBackwards>) tensor(0.0797, grad_fn=<DivBackward0>) tensor(0.3520, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.9663, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.9656, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.9663, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5649056 0.0 0.02128525 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.9663, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -7.024892 -0.10601248 6.8099413 24.741537\n",
      "activ tensor(1.9663, grad_fn=<CopyBackwards>) tensor(0.0795, grad_fn=<DivBackward0>) tensor(0.3799, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.9766, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.9663, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.9766, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5607415 0.0 0.021644674 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.9766, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -7.113154 -0.09910218 6.675514 24.804329\n",
      "activ tensor(1.9766, grad_fn=<CopyBackwards>) tensor(0.0797, grad_fn=<DivBackward0>) tensor(0.3810, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.9820, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.9766, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.9820, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5608459 0.0 0.021285253 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.9817, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -7.1253557 -0.09112954 6.427994 24.844217\n",
      "activ tensor(1.9817, grad_fn=<CopyBackwards>) tensor(0.0798, grad_fn=<DivBackward0>) tensor(0.3967, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.9900, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.9817, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.9900, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56386745 0.0 0.021486105 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.9900, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -7.061339 -0.08985532 6.560829 24.88125\n",
      "activ tensor(1.9900, grad_fn=<CopyBackwards>) tensor(0.0800, grad_fn=<DivBackward0>) tensor(0.3540, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.9922, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.9900, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.9922, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56914216 0.0 0.021887813 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.9921, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -7.35747 -0.10951595 6.7443 24.91414\n",
      "activ tensor(1.9921, grad_fn=<CopyBackwards>) tensor(0.0800, grad_fn=<DivBackward0>) tensor(0.3347, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.9930, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.9921, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.9930, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5868811 0.0 0.022617232 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.9930, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -7.357455 -0.12996057 7.1234236 24.979351\n",
      "activ tensor(1.9930, grad_fn=<CopyBackwards>) tensor(0.0798, grad_fn=<DivBackward0>) tensor(0.3500, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.9936, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.9930, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.9936, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5801534 0.0 0.022215523 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.9936, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -7.073559 -0.111173674 6.5825267 25.057976\n",
      "activ tensor(1.9936, grad_fn=<CopyBackwards>) tensor(0.0796, grad_fn=<DivBackward0>) tensor(0.4003, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.9941, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.9936, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.9941, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.59501094 0.0 0.023156367 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.9940, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -7.2151413 -0.12289654 6.0724382 25.157948\n",
      "activ tensor(1.9940, grad_fn=<CopyBackwards>) tensor(0.0793, grad_fn=<DivBackward0>) tensor(0.4338, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.0035, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.9940, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.0035, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5580182 0.0 0.021327537 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.0036, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -6.99731 -0.1634436 6.873016 25.273535\n",
      "activ tensor(2.0036, grad_fn=<CopyBackwards>) tensor(0.0793, grad_fn=<DivBackward0>) tensor(0.3502, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.0057, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.0036, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.0057, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56168115 0.0 0.021327537 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.0056, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -7.4359 -0.14309138 6.8027453 25.413906\n",
      "activ tensor(2.0056, grad_fn=<CopyBackwards>) tensor(0.0789, grad_fn=<DivBackward0>) tensor(0.3750, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.0163, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.0056, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.0163, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5669762 0.0 0.021750387 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.0164, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -7.6413035 -0.14082915 7.0142694 25.573687\n",
      "activ tensor(2.0164, grad_fn=<CopyBackwards>) tensor(0.0788, grad_fn=<DivBackward0>) tensor(0.3465, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.0191, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.0164, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.0191, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5756928 0.0 0.022152096 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.0193, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -6.8980007 -0.13004656 7.1184096 25.74015\n",
      "activ tensor(2.0193, grad_fn=<CopyBackwards>) tensor(0.0784, grad_fn=<DivBackward0>) tensor(0.3317, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.0237, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.0193, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.0237, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5535908 0.0 0.021221822 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.0237, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -7.400942 -0.1421918 6.608172 25.884157\n",
      "activ tensor(2.0237, grad_fn=<CopyBackwards>) tensor(0.0782, grad_fn=<DivBackward0>) tensor(0.3886, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.0251, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.0237, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.0251, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5566517 0.0 0.02128525 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.0252, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -7.737169 -0.13401684 6.8544993 25.997578\n",
      "activ tensor(2.0252, grad_fn=<CopyBackwards>) tensor(0.0779, grad_fn=<DivBackward0>) tensor(0.3584, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.0364, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.0252, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.0364, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5706841 0.0 0.022014668 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.0364, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -7.4458103 -0.14177744 7.247578 26.076723\n",
      "activ tensor(2.0364, grad_fn=<CopyBackwards>) tensor(0.0781, grad_fn=<DivBackward0>) tensor(0.4408, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.0397, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.0364, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.0397, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5771161 0.0 0.021993529 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.0397, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -7.2688828 -0.14593723 7.6259704 26.150728\n",
      "activ tensor(2.0397, grad_fn=<CopyBackwards>) tensor(0.0780, grad_fn=<DivBackward0>) tensor(0.3807, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.0350, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.0397, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.0350, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57068413 0.0 0.021623531 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.0350, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -7.6688023 -0.10559158 7.342612 26.21369\n",
      "activ tensor(2.0350, grad_fn=<CopyBackwards>) tensor(0.0776, grad_fn=<DivBackward0>) tensor(0.4321, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.0353, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.0350, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.0353, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5663557 0.0 0.021718675 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.0354, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -8.216622 -0.124482185 7.141186 26.262278\n",
      "activ tensor(2.0354, grad_fn=<CopyBackwards>) tensor(0.0775, grad_fn=<DivBackward0>) tensor(0.4131, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.0403, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.0354, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.0403, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5751838 0.0 0.022099238 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.0401, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -8.308649 -0.12117046 7.1472692 26.303162\n",
      "activ tensor(2.0401, grad_fn=<CopyBackwards>) tensor(0.0776, grad_fn=<DivBackward0>) tensor(0.3655, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.0623, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.0401, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.0623, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5639713 0.0 0.021560103 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.0623, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -7.7883797 -0.09076974 7.2654586 26.336828\n",
      "activ tensor(2.0623, grad_fn=<CopyBackwards>) tensor(0.0783, grad_fn=<DivBackward0>) tensor(0.3528, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.0715, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.0623, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.0715, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55115116 0.0 0.020989256 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.0715, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -7.5019827 -0.116443396 7.9591513 26.378424\n",
      "activ tensor(2.0715, grad_fn=<CopyBackwards>) tensor(0.0785, grad_fn=<DivBackward0>) tensor(0.3897, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.0773, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.0715, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.0773, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5727337 0.0 0.021972384 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.0772, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -7.724482 -0.11813784 7.3781447 26.445772\n",
      "activ tensor(2.0772, grad_fn=<CopyBackwards>) tensor(0.0785, grad_fn=<DivBackward0>) tensor(0.3733, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.0890, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.0772, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.0890, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55380255 0.0 0.020851828 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.0891, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -7.8303323 -0.12646076 7.1271977 26.514013\n",
      "activ tensor(2.0891, grad_fn=<CopyBackwards>) tensor(0.0788, grad_fn=<DivBackward0>) tensor(0.3838, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.1010, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.0891, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.1010, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.54805976 0.0 0.020767258 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.1010, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -7.753231 -0.1257353 7.2868233 26.610573\n",
      "activ tensor(2.1010, grad_fn=<CopyBackwards>) tensor(0.0790, grad_fn=<DivBackward0>) tensor(0.4270, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.1071, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.1010, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.1071, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5923465 0.0 0.022828655 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.1072, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -7.642106 -0.15408364 6.997397 26.71967\n",
      "activ tensor(2.1072, grad_fn=<CopyBackwards>) tensor(0.0789, grad_fn=<DivBackward0>) tensor(0.3725, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.1139, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.1072, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.1139, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5588576 0.0 0.021221822 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.1138, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -7.8871255 -0.13970014 7.581612 26.849762\n",
      "activ tensor(2.1138, grad_fn=<CopyBackwards>) tensor(0.0787, grad_fn=<DivBackward0>) tensor(0.3962, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.1284, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.1138, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.1284, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56188965 0.0 0.02160239 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.1284, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -7.9381304 -0.117507935 7.626204 26.997885\n",
      "activ tensor(2.1284, grad_fn=<CopyBackwards>) tensor(0.0788, grad_fn=<DivBackward0>) tensor(0.4180, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.1354, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.1284, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.1354, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.569965 0.0 0.021760955 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.1354, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -7.488718 -0.13645487 7.1142297 27.142647\n",
      "activ tensor(2.1354, grad_fn=<CopyBackwards>) tensor(0.0787, grad_fn=<DivBackward0>) tensor(0.3948, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.1428, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.1354, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.1428, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5864815 0.0 0.022722945 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.1428, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -7.712565 -0.1413542 6.8393683 27.267584\n",
      "activ tensor(2.1428, grad_fn=<CopyBackwards>) tensor(0.0786, grad_fn=<DivBackward0>) tensor(0.4002, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.1422, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.1428, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.1422, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5562305 0.0 0.021179538 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.1422, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -7.788343 -0.13816181 7.2752967 27.38408\n",
      "activ tensor(2.1422, grad_fn=<CopyBackwards>) tensor(0.0782, grad_fn=<DivBackward0>) tensor(0.3635, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.1456, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.1422, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.1456, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5555981 0.0 0.021380393 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.1455, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -8.013119 -0.14155693 7.6993747 27.48268\n",
      "activ tensor(2.1455, grad_fn=<CopyBackwards>) tensor(0.0781, grad_fn=<DivBackward0>) tensor(0.4312, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.1485, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.1455, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.1485, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56945086 0.0 0.02177153 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.1483, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -8.33425 -0.13578373 7.423865 27.574617\n",
      "activ tensor(2.1483, grad_fn=<CopyBackwards>) tensor(0.0779, grad_fn=<DivBackward0>) tensor(0.4221, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.1529, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.1483, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.1529, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5888742 0.0 0.0226278 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.1528, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -7.5599732 -0.14426678 7.707581 27.665575\n",
      "activ tensor(2.1528, grad_fn=<CopyBackwards>) tensor(0.0778, grad_fn=<DivBackward0>) tensor(0.4102, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.1519, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.1528, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.1519, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5605325 0.0 0.021338107 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.1520, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -8.090738 -0.15146771 7.2647343 27.752256\n",
      "activ tensor(2.1520, grad_fn=<CopyBackwards>) tensor(0.0775, grad_fn=<DivBackward0>) tensor(0.4530, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.1616, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.1520, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.1616, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57711595 0.0 0.022352949 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.1615, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -8.238838 -0.1490291 7.3087373 27.828705\n",
      "activ tensor(2.1615, grad_fn=<CopyBackwards>) tensor(0.0777, grad_fn=<DivBackward0>) tensor(0.3737, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.1685, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.1615, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.1685, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5866811 0.0 0.02261723 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.1686, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -7.984289 -0.12941147 8.126708 27.916565\n",
      "activ tensor(2.1686, grad_fn=<CopyBackwards>) tensor(0.0777, grad_fn=<DivBackward0>) tensor(0.3843, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.1733, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.1686, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.1733, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.569348 0.0 0.021908956 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.1734, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -7.93972 -0.13869561 7.9923882 28.010471\n",
      "activ tensor(2.1734, grad_fn=<CopyBackwards>) tensor(0.0776, grad_fn=<DivBackward0>) tensor(0.3899, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.1753, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.1734, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.1753, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55517614 0.0 0.021242967 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.1753, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -8.296398 -0.1467185 8.035969 28.095394\n",
      "activ tensor(2.1753, grad_fn=<CopyBackwards>) tensor(0.0774, grad_fn=<DivBackward0>) tensor(0.4454, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.1742, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.1753, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.1742, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5748779 0.0 0.022130953 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.1745, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -8.84831 -0.15262577 7.7210803 28.165308\n",
      "activ tensor(2.1745, grad_fn=<CopyBackwards>) tensor(0.0772, grad_fn=<DivBackward0>) tensor(0.4306, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.1813, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.1745, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.1813, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57355154 0.0 0.022152096 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.1813, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -8.9043865 -0.14489941 7.6443467 28.244753\n",
      "activ tensor(2.1813, grad_fn=<CopyBackwards>) tensor(0.0772, grad_fn=<DivBackward0>) tensor(0.4437, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.1821, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.1813, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.1821, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55040663 0.0 0.020830687 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.1822, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -8.369757 -0.15005983 8.157279 28.323925\n",
      "activ tensor(2.1822, grad_fn=<CopyBackwards>) tensor(0.0770, grad_fn=<DivBackward0>) tensor(0.4383, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.1874, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.1822, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.1874, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56573474 0.0 0.021739814 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.1873, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -8.089654 -0.14284308 8.347443 28.414324\n",
      "activ tensor(2.1873, grad_fn=<CopyBackwards>) tensor(0.0770, grad_fn=<DivBackward0>) tensor(0.4397, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.1865, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.1873, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.1865, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56831807 0.0 0.02194067 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.1864, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -8.442557 -0.12932667 8.15871 28.507536\n",
      "activ tensor(2.1864, grad_fn=<CopyBackwards>) tensor(0.0767, grad_fn=<DivBackward0>) tensor(0.4096, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.1934, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.1864, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.1934, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.562411 0.0 0.021401538 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.1935, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -8.535865 -0.1229211 7.6033654 28.593315\n",
      "activ tensor(2.1935, grad_fn=<CopyBackwards>) tensor(0.0767, grad_fn=<DivBackward0>) tensor(0.3743, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.1995, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.1935, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.1995, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57487804 0.0 0.022204952 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.1995, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -8.373006 -0.1349632 7.9026175 28.654146\n",
      "activ tensor(2.1995, grad_fn=<CopyBackwards>) tensor(0.0768, grad_fn=<DivBackward0>) tensor(0.4217, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.2019, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.1995, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.2019, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5533791 0.0 0.020862399 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.2021, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -8.226105 -0.13255161 7.838225 28.723898\n",
      "activ tensor(2.2021, grad_fn=<CopyBackwards>) tensor(0.0767, grad_fn=<DivBackward0>) tensor(0.4274, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.2063, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.2021, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.2063, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5588577 0.0 0.021126682 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.2063, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -8.682481 -0.13173601 7.9840918 28.800953\n",
      "activ tensor(2.2063, grad_fn=<CopyBackwards>) tensor(0.0766, grad_fn=<DivBackward0>) tensor(0.4487, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.2076, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.2063, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.2076, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5664591 0.0 0.021750385 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.2075, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -8.280609 -0.14429303 8.446666 28.883839\n",
      "activ tensor(2.2075, grad_fn=<CopyBackwards>) tensor(0.0764, grad_fn=<DivBackward0>) tensor(0.4337, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.2180, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.2075, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.2180, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56811154 0.0 0.021803245 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.2179, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -8.203265 -0.15724869 7.5287585 28.98252\n",
      "activ tensor(2.2179, grad_fn=<CopyBackwards>) tensor(0.0765, grad_fn=<DivBackward0>) tensor(0.4615, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.2223, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.2179, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.2223, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57263154 0.0 0.021877244 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.2222, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -8.497865 -0.14986502 7.4962173 29.089264\n",
      "activ tensor(2.2222, grad_fn=<CopyBackwards>) tensor(0.0764, grad_fn=<DivBackward0>) tensor(0.4537, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.2285, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.2222, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.2285, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5610549 0.0 0.021528393 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.2284, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -8.311208 -0.14482859 8.011718 29.207449\n",
      "activ tensor(2.2284, grad_fn=<CopyBackwards>) tensor(0.0763, grad_fn=<DivBackward0>) tensor(0.3958, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.2286, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.2284, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.2286, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5604281 0.0 0.021390963 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.2286, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -8.837365 -0.1387882 8.145749 29.342278\n",
      "activ tensor(2.2286, grad_fn=<CopyBackwards>) tensor(0.0760, grad_fn=<DivBackward0>) tensor(0.4807, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.2302, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.2286, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.2302, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5615768 0.0 0.02142268 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.2303, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -8.752267 -0.1514914 8.2558775 29.480312\n",
      "activ tensor(2.2303, grad_fn=<CopyBackwards>) tensor(0.0757, grad_fn=<DivBackward0>) tensor(0.4287, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.2388, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.2303, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.2388, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56324357 0.0 0.021623531 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.2390, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -8.447121 -0.15702115 8.012822 29.618462\n",
      "activ tensor(2.2390, grad_fn=<CopyBackwards>) tensor(0.0756, grad_fn=<DivBackward0>) tensor(0.3893, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.2541, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.2390, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.2541, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5587527 0.0 0.021306392 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.2542, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -8.741507 -0.16789953 7.9884467 29.726862\n",
      "activ tensor(2.2542, grad_fn=<CopyBackwards>) tensor(0.0758, grad_fn=<DivBackward0>) tensor(0.4158, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.2560, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.2542, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.2560, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.58287394 0.0 0.022374092 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.2559, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -8.731608 -0.16379416 7.9473386 29.838257\n",
      "activ tensor(2.2559, grad_fn=<CopyBackwards>) tensor(0.0756, grad_fn=<DivBackward0>) tensor(0.4371, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.2666, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.2559, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.2666, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57731897 0.0 0.02203581 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.2667, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -8.516722 -0.16072285 8.640641 29.94734\n",
      "activ tensor(2.2667, grad_fn=<CopyBackwards>) tensor(0.0757, grad_fn=<DivBackward0>) tensor(0.4518, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.2682, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.2667, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.2682, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5681116 0.0 0.02212038 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.2681, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -8.484606 -0.16616774 8.782862 30.048119\n",
      "activ tensor(2.2681, grad_fn=<CopyBackwards>) tensor(0.0755, grad_fn=<DivBackward0>) tensor(0.4886, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.2670, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.2681, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.2670, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5808602 0.0 0.022321235 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.2670, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -8.919455 -0.16344959 8.351918 30.140392\n",
      "activ tensor(2.2670, grad_fn=<CopyBackwards>) tensor(0.0752, grad_fn=<DivBackward0>) tensor(0.4177, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.2727, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.2670, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.2727, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56542385 0.0 0.021560103 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.2729, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -9.46433 -0.13817108 8.506422 30.217978\n",
      "activ tensor(2.2729, grad_fn=<CopyBackwards>) tensor(0.0752, grad_fn=<DivBackward0>) tensor(0.4061, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.2805, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.2729, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.2805, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5495544 0.0 0.020841258 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.2806, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -9.499119 -0.15225805 8.18733 30.275112\n",
      "activ tensor(2.2806, grad_fn=<CopyBackwards>) tensor(0.0753, grad_fn=<DivBackward0>) tensor(0.4293, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.2812, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.2806, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.2812, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55623055 0.0 0.021179538 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.2812, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -8.934221 -0.15119514 8.775127 30.3244\n",
      "activ tensor(2.2812, grad_fn=<CopyBackwards>) tensor(0.0752, grad_fn=<DivBackward0>) tensor(0.4399, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.2841, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.2812, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.2841, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5578082 0.0 0.02134868 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.2843, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -8.710157 -0.14290777 9.1463995 30.370796\n",
      "activ tensor(2.2843, grad_fn=<CopyBackwards>) tensor(0.0752, grad_fn=<DivBackward0>) tensor(0.4878, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.3004, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.2843, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.3004, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.564075 0.0 0.021665819 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.3005, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -9.131128 -0.15065648 8.488632 30.428812\n",
      "activ tensor(2.3005, grad_fn=<CopyBackwards>) tensor(0.0756, grad_fn=<DivBackward0>) tensor(0.4815, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.2980, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.3005, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.2980, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5758963 0.0 0.021982955 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.2979, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -9.259737 -0.16916713 8.428769 30.486622\n",
      "activ tensor(2.2979, grad_fn=<CopyBackwards>) tensor(0.0754, grad_fn=<DivBackward0>) tensor(0.4433, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.3013, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.2979, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.3013, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56220245 0.0 0.021369819 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.3012, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -8.971448 -0.1687333 8.38293 30.571758\n",
      "activ tensor(2.3012, grad_fn=<CopyBackwards>) tensor(0.0753, grad_fn=<DivBackward0>) tensor(0.4788, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.3051, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.3012, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.3051, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5803555 0.0 0.022553802 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.3049, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -8.812492 -0.13565448 8.579794 30.672298\n",
      "activ tensor(2.3049, grad_fn=<CopyBackwards>) tensor(0.0751, grad_fn=<DivBackward0>) tensor(0.4129, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.3146, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.3049, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.3146, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5511512 0.0 0.020968113 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.3145, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -9.245079 -0.13708355 8.690896 30.769838\n",
      "activ tensor(2.3145, grad_fn=<CopyBackwards>) tensor(0.0752, grad_fn=<DivBackward0>) tensor(0.4373, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.3201, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.3145, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.3201, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5696567 0.0 0.021655245 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.3202, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -8.891017 -0.1561898 8.835862 30.882416\n",
      "activ tensor(2.3202, grad_fn=<CopyBackwards>) tensor(0.0751, grad_fn=<DivBackward0>) tensor(0.4220, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.3231, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.3202, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.3231, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5549651 0.0 0.021306396 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.3233, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -9.060587 -0.14350748 8.356882 30.994877\n",
      "activ tensor(2.3233, grad_fn=<CopyBackwards>) tensor(0.0750, grad_fn=<DivBackward0>) tensor(0.4291, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.3349, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.3233, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.3349, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5778262 0.0 0.022395235 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.3348, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -9.055347 -0.17408803 7.8975406 31.087667\n",
      "activ tensor(2.3348, grad_fn=<CopyBackwards>) tensor(0.0751, grad_fn=<DivBackward0>) tensor(0.4302, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.3401, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.3348, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.3401, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5601141 0.0 0.021538964 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.3400, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -8.714081 -0.16808696 8.793096 31.200495\n",
      "activ tensor(2.3400, grad_fn=<CopyBackwards>) tensor(0.0750, grad_fn=<DivBackward0>) tensor(0.4534, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.3492, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.3400, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.3492, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55612516 0.0 0.021137252 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.3494, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -9.533975 -0.15556817 8.782209 31.317263\n",
      "activ tensor(2.3494, grad_fn=<CopyBackwards>) tensor(0.0750, grad_fn=<DivBackward0>) tensor(0.4772, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.3643, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.3494, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.3643, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55019367 0.0 0.02104211 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.3641, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -9.374513 -0.15593365 8.7025175 31.42313\n",
      "activ tensor(2.3641, grad_fn=<CopyBackwards>) tensor(0.0752, grad_fn=<DivBackward0>) tensor(0.4645, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.3643, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.3641, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.3643, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5749799 0.0 0.021993527 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.3643, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -8.963398 -0.16287656 8.7748995 31.529947\n",
      "activ tensor(2.3643, grad_fn=<CopyBackwards>) tensor(0.0750, grad_fn=<DivBackward0>) tensor(0.4392, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.3643, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.3643, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.3643, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56996495 0.0 0.021866672 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.3644, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -9.151672 -0.15116006 8.325561 31.649914\n",
      "activ tensor(2.3644, grad_fn=<CopyBackwards>) tensor(0.0747, grad_fn=<DivBackward0>) tensor(0.4932, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.3651, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.3644, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.3651, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5659418 0.0 0.02161296 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.3651, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -9.237819 -0.15703872 8.80807 31.745735\n",
      "activ tensor(2.3651, grad_fn=<CopyBackwards>) tensor(0.0745, grad_fn=<DivBackward0>) tensor(0.4089, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.3616, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.3651, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.3616, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.565113 0.0 0.021634104 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.3617, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -9.041784 -0.18231846 9.17034 31.847282\n",
      "activ tensor(2.3617, grad_fn=<CopyBackwards>) tensor(0.0742, grad_fn=<DivBackward0>) tensor(0.4442, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.3734, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.3617, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.3734, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.54880756 0.0 0.020883542 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.3736, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -9.109499 -0.16942796 9.315361 31.959644\n",
      "activ tensor(2.3736, grad_fn=<CopyBackwards>) tensor(0.0743, grad_fn=<DivBackward0>) tensor(0.4250, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.3800, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.3736, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.3800, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5768114 0.0 0.022141526 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.3799, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -9.5508375 -0.17140019 9.065026 32.059364\n",
      "activ tensor(2.3799, grad_fn=<CopyBackwards>) tensor(0.0742, grad_fn=<DivBackward0>) tensor(0.4940, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.3946, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.3799, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.3946, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5734496 0.0 0.021993527 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.3948, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -10.056806 -0.20503736 8.8709545 32.165\n",
      "activ tensor(2.3948, grad_fn=<CopyBackwards>) tensor(0.0745, grad_fn=<DivBackward0>) tensor(0.4747, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.3982, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.3948, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.3982, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55896235 0.0 0.021137252 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.3982, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -10.086661 -0.18572223 9.062272 32.286686\n",
      "activ tensor(2.3982, grad_fn=<CopyBackwards>) tensor(0.0743, grad_fn=<DivBackward0>) tensor(0.4756, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.4026, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.3982, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.4026, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57599807 0.0 0.022152096 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.4025, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -9.507566 -0.16708061 9.262853 32.396267\n",
      "activ tensor(2.4025, grad_fn=<CopyBackwards>) tensor(0.0742, grad_fn=<DivBackward0>) tensor(0.4429, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.4027, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.4025, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.4027, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.578738 0.0 0.022331806 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.4027, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -9.32312 -0.16303515 9.816478 32.491074\n",
      "activ tensor(2.4027, grad_fn=<CopyBackwards>) tensor(0.0740, grad_fn=<DivBackward0>) tensor(0.4369, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.4080, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.4027, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.4080, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.58065814 0.0 0.022331808 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.4081, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -9.839277 -0.16293502 9.146707 32.57567\n",
      "activ tensor(2.4081, grad_fn=<CopyBackwards>) tensor(0.0739, grad_fn=<DivBackward0>) tensor(0.4406, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.4104, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.4081, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.4104, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5565462 0.0 0.021221824 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.4104, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -9.948702 -0.15376121 8.815665 32.643745\n",
      "activ tensor(2.4104, grad_fn=<CopyBackwards>) tensor(0.0738, grad_fn=<DivBackward0>) tensor(0.4708, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.4167, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.4104, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.4167, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5643869 0.0 0.021285253 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.4168, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -9.591675 -0.1670306 9.2966385 32.67515\n",
      "activ tensor(2.4168, grad_fn=<CopyBackwards>) tensor(0.0740, grad_fn=<DivBackward0>) tensor(0.4337, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.4210, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.4168, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.4210, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55263734 0.0 0.020915257 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.4209, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -9.376722 -0.16567329 9.03456 32.718994\n",
      "activ tensor(2.4209, grad_fn=<CopyBackwards>) tensor(0.0740, grad_fn=<DivBackward0>) tensor(0.4203, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.4236, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.4209, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.4236, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56552744 0.0 0.021792673 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.4237, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -9.761628 -0.15999147 9.453992 32.770298\n",
      "activ tensor(2.4237, grad_fn=<CopyBackwards>) tensor(0.0740, grad_fn=<DivBackward0>) tensor(0.4262, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.4233, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.4237, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.4233, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55959105 0.0 0.021274678 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.4232, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -9.361883 -0.16122882 9.396566 32.814396\n",
      "activ tensor(2.4232, grad_fn=<CopyBackwards>) tensor(0.0738, grad_fn=<DivBackward0>) tensor(0.4978, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.4314, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.4232, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.4314, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57711583 0.0 0.02217324 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.4314, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -9.662095 -0.15517756 8.829799 32.876686\n",
      "activ tensor(2.4314, grad_fn=<CopyBackwards>) tensor(0.0740, grad_fn=<DivBackward0>) tensor(0.4552, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.4345, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.4314, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.4345, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.54257995 0.0 0.02060869 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.4344, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -9.549287 -0.12956733 8.744329 32.956844\n",
      "activ tensor(2.4344, grad_fn=<CopyBackwards>) tensor(0.0739, grad_fn=<DivBackward0>) tensor(0.4113, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.4312, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.4344, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.4312, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.54731077 0.0 0.020883545 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.4312, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -9.527659 -0.16636772 9.201287 33.036663\n",
      "activ tensor(2.4312, grad_fn=<CopyBackwards>) tensor(0.0736, grad_fn=<DivBackward0>) tensor(0.3976, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.4366, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.4312, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.4366, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55517614 0.0 0.021232393 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.4367, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -10.068257 -0.18486004 9.610265 33.146244\n",
      "activ tensor(2.4367, grad_fn=<CopyBackwards>) tensor(0.0735, grad_fn=<DivBackward0>) tensor(0.4414, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.4361, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.4367, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.4361, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.59076136 0.0 0.022870941 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.4361, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -9.970722 -0.175147 9.183326 33.275528\n",
      "activ tensor(2.4361, grad_fn=<CopyBackwards>) tensor(0.0732, grad_fn=<DivBackward0>) tensor(0.5246, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.4406, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.4361, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.4406, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5752857 0.0 0.022226093 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.4405, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -9.628457 -0.19163127 9.275027 33.42061\n",
      "activ tensor(2.4405, grad_fn=<CopyBackwards>) tensor(0.0730, grad_fn=<DivBackward0>) tensor(0.4909, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.4412, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.4405, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.4412, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56718266 0.0 0.021771532 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.4413, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -9.739813 -0.18850076 9.088447 33.57398\n",
      "activ tensor(2.4413, grad_fn=<CopyBackwards>) tensor(0.0727, grad_fn=<DivBackward0>) tensor(0.5093, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.4485, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.4413, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.4485, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5550705 0.0 0.021158397 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.4485, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -9.713545 -0.19258292 9.223078 33.742638\n",
      "activ tensor(2.4485, grad_fn=<CopyBackwards>) tensor(0.0726, grad_fn=<DivBackward0>) tensor(0.4559, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.4515, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.4485, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.4515, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.58025455 0.0 0.022310665 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.4514, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -9.592796 -0.19235581 10.041875 33.90905\n",
      "activ tensor(2.4514, grad_fn=<CopyBackwards>) tensor(0.0723, grad_fn=<DivBackward0>) tensor(0.3906, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.4516, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.4514, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.4516, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5550706 0.0 0.021190109 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.4516, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -9.727 -0.18336032 9.695519 34.060307\n",
      "activ tensor(2.4516, grad_fn=<CopyBackwards>) tensor(0.0720, grad_fn=<DivBackward0>) tensor(0.4680, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.4608, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.4516, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.4608, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57721734 0.0 0.022067524 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.4608, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -10.1872635 -0.19277263 9.677342 34.199726\n",
      "activ tensor(2.4608, grad_fn=<CopyBackwards>) tensor(0.0720, grad_fn=<DivBackward0>) tensor(0.4605, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.4666, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.4608, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.4666, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5763031 0.0 0.02218381 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.4665, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -10.638096 -0.17216042 9.553292 34.30946\n",
      "activ tensor(2.4665, grad_fn=<CopyBackwards>) tensor(0.0719, grad_fn=<DivBackward0>) tensor(0.4426, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.4703, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.4665, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.4703, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5666659 0.0 0.021655243 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.4705, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -10.654809 -0.19428232 9.517684 34.384193\n",
      "activ tensor(2.4705, grad_fn=<CopyBackwards>) tensor(0.0719, grad_fn=<DivBackward0>) tensor(0.4114, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.4765, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.4705, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.4765, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56449044 0.0 0.021581246 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.4764, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -10.094549 -0.17940807 10.151893 34.45246\n",
      "activ tensor(2.4764, grad_fn=<CopyBackwards>) tensor(0.0719, grad_fn=<DivBackward0>) tensor(0.4765, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.4805, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.4764, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.4805, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5682148 0.0 0.0218561 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.4806, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -9.941152 -0.19046938 10.208391 34.505135\n",
      "activ tensor(2.4806, grad_fn=<CopyBackwards>) tensor(0.0719, grad_fn=<DivBackward0>) tensor(0.4834, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.4872, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.4806, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.4872, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56738937 0.0 0.021623533 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.4871, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -10.534885 -0.1782616 9.849458 34.54765\n",
      "activ tensor(2.4871, grad_fn=<CopyBackwards>) tensor(0.0720, grad_fn=<DivBackward0>) tensor(0.5171, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.4960, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.4871, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.4960, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5485939 0.0 0.020894112 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.4959, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -10.646155 -0.13909027 9.4041605 34.580467\n",
      "activ tensor(2.4959, grad_fn=<CopyBackwards>) tensor(0.0722, grad_fn=<DivBackward0>) tensor(0.4581, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.5051, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.4959, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.5051, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.58787847 0.0 0.022955513 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.5052, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -10.182522 -0.16148049 9.834771 34.61693\n",
      "activ tensor(2.5052, grad_fn=<CopyBackwards>) tensor(0.0724, grad_fn=<DivBackward0>) tensor(0.4055, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.5122, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.5052, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.5122, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5638674 0.0 0.021729246 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.5123, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -9.969862 -0.16386704 9.941426 34.647034\n",
      "activ tensor(2.5123, grad_fn=<CopyBackwards>) tensor(0.0725, grad_fn=<DivBackward0>) tensor(0.4104, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.5151, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.5123, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.5151, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57660794 0.0 0.022183808 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.5153, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -10.48833 -0.17577074 9.816176 34.693657\n",
      "activ tensor(2.5153, grad_fn=<CopyBackwards>) tensor(0.0725, grad_fn=<DivBackward0>) tensor(0.4715, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.5179, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.5153, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.5179, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56021875 0.0 0.021454392 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.5179, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -9.995342 -0.18807507 10.17282 34.754993\n",
      "activ tensor(2.5179, grad_fn=<CopyBackwards>) tensor(0.0724, grad_fn=<DivBackward0>) tensor(0.4630, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.5204, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.5179, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.5204, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.600012 0.0 0.023388933 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.5203, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -10.070515 -0.17803444 9.326824 34.833996\n",
      "activ tensor(2.5203, grad_fn=<CopyBackwards>) tensor(0.0724, grad_fn=<DivBackward0>) tensor(0.5269, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.5259, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.5203, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.5259, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5752856 0.0 0.021961812 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.5261, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -10.146276 -0.19173308 9.35509 34.9348\n",
      "activ tensor(2.5261, grad_fn=<CopyBackwards>) tensor(0.0723, grad_fn=<DivBackward0>) tensor(0.4698, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.5352, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.5261, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.5352, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.54902095 0.0 0.020735545 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.5353, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -10.054849 -0.20066233 10.008909 35.06029\n",
      "activ tensor(2.5353, grad_fn=<CopyBackwards>) tensor(0.0723, grad_fn=<DivBackward0>) tensor(0.4551, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.5463, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.5353, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.5463, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5716074 0.0 0.022289522 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.5462, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -10.866386 -0.21249568 10.000443 35.20361\n",
      "activ tensor(2.5462, grad_fn=<CopyBackwards>) tensor(0.0723, grad_fn=<DivBackward0>) tensor(0.4323, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.5500, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.5462, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.5500, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55200094 0.0 0.020957544 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.5499, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -10.329331 -0.16873123 9.989084 35.350815\n",
      "activ tensor(2.5499, grad_fn=<CopyBackwards>) tensor(0.0721, grad_fn=<DivBackward0>) tensor(0.4411, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.5518, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.5499, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.5518, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5610547 0.0 0.021665815 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.5519, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -10.4690075 -0.18748638 9.656243 35.502464\n",
      "activ tensor(2.5519, grad_fn=<CopyBackwards>) tensor(0.0719, grad_fn=<DivBackward0>) tensor(0.5110, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.5542, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.5519, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.5542, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5663557 0.0 0.021454394 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.5542, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -10.559594 -0.20280899 9.740838 35.647602\n",
      "activ tensor(2.5542, grad_fn=<CopyBackwards>) tensor(0.0717, grad_fn=<DivBackward0>) tensor(0.5149, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.5524, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.5542, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.5524, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5577031 0.0 0.021190109 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.5525, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -10.31737 -0.18439314 9.967238 35.76887\n",
      "activ tensor(2.5525, grad_fn=<CopyBackwards>) tensor(0.0714, grad_fn=<DivBackward0>) tensor(0.4666, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.5525, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.5525, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.5525, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56209826 0.0 0.02145439 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.5523, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -10.404061 -0.1953861 10.464762 35.895065\n",
      "activ tensor(2.5523, grad_fn=<CopyBackwards>) tensor(0.0711, grad_fn=<DivBackward0>) tensor(0.4894, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.5581, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.5523, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.5581, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5613683 0.0 0.021570675 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.5581, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -10.378168 -0.1990116 10.496926 36.011505\n",
      "activ tensor(2.5581, grad_fn=<CopyBackwards>) tensor(0.0710, grad_fn=<DivBackward0>) tensor(0.5083, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.5621, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.5581, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.5621, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5642831 0.0 0.021591818 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.5620, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -10.801322 -0.17817952 10.00064 36.116047\n",
      "activ tensor(2.5620, grad_fn=<CopyBackwards>) tensor(0.0709, grad_fn=<DivBackward0>) tensor(0.5218, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.5684, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.5620, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.5684, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55146986 0.0 0.020915257 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.5684, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -11.216932 -0.16367161 10.308976 36.211292\n",
      "activ tensor(2.5684, grad_fn=<CopyBackwards>) tensor(0.0709, grad_fn=<DivBackward0>) tensor(0.4822, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.5729, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.5684, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.5729, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5778261 0.0 0.02219438 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.5729, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -11.21089 -0.18436733 10.152079 36.300343\n",
      "activ tensor(2.5729, grad_fn=<CopyBackwards>) tensor(0.0709, grad_fn=<DivBackward0>) tensor(0.5128, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.5816, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.5729, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.5816, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5623066 0.0 0.021486104 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.5817, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -10.68245 -0.20162831 10.672245 36.3696\n",
      "activ tensor(2.5817, grad_fn=<CopyBackwards>) tensor(0.0710, grad_fn=<DivBackward0>) tensor(0.4581, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.5752, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.5817, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.5752, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5408496 0.0 0.020502977 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.5755, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -10.575156 -0.18568249 11.024924 36.445175\n",
      "activ tensor(2.5755, grad_fn=<CopyBackwards>) tensor(0.0707, grad_fn=<DivBackward0>) tensor(0.4748, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.5904, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.5755, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.5904, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5666661 0.0 0.021718673 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.5906, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -11.217286 -0.20162222 10.17798 36.52052\n",
      "activ tensor(2.5906, grad_fn=<CopyBackwards>) tensor(0.0709, grad_fn=<DivBackward0>) tensor(0.5376, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.6000, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.5906, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.6000, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5613682 0.0 0.021295823 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.6000, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -11.332335 -0.20422235 10.201102 36.59092\n",
      "activ tensor(2.6000, grad_fn=<CopyBackwards>) tensor(0.0711, grad_fn=<DivBackward0>) tensor(0.5162, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.6019, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.6000, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.6019, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5743683 0.0 0.022078097 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.6020, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -10.780903 -0.20349744 10.411851 36.666748\n",
      "activ tensor(2.6020, grad_fn=<CopyBackwards>) tensor(0.0710, grad_fn=<DivBackward0>) tensor(0.5400, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.6047, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.6020, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.6047, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5648018 0.0 0.021443821 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.6046, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -10.545194 -0.21181428 10.580042 36.748188\n",
      "activ tensor(2.6046, grad_fn=<CopyBackwards>) tensor(0.0709, grad_fn=<DivBackward0>) tensor(0.5456, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.6122, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.6046, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.6122, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5756929 0.0 0.022226095 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.6122, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -10.945389 -0.21182297 10.590026 36.83068\n",
      "activ tensor(2.6122, grad_fn=<CopyBackwards>) tensor(0.0709, grad_fn=<DivBackward0>) tensor(0.4602, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.6185, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.6122, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.6185, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5548593 0.0 0.021306394 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.6186, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -10.500427 -0.1767009 10.497516 36.919315\n",
      "activ tensor(2.6186, grad_fn=<CopyBackwards>) tensor(0.0709, grad_fn=<DivBackward0>) tensor(0.4845, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.6223, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.6186, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.6223, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5593816 0.0 0.021422679 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.6224, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -10.764191 -0.18500464 10.178685 37.006226\n",
      "activ tensor(2.6224, grad_fn=<CopyBackwards>) tensor(0.0709, grad_fn=<DivBackward0>) tensor(0.4952, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.6162, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.6224, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.6162, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5729384 0.0 0.021824388 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.6163, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -10.813186 -0.19515626 9.830157 37.078854\n",
      "activ tensor(2.6163, grad_fn=<CopyBackwards>) tensor(0.0706, grad_fn=<DivBackward0>) tensor(0.5256, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.6209, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.6163, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.6209, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5738579 0.0 0.022173237 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.6209, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -10.758916 -0.19420132 10.718581 37.139458\n",
      "activ tensor(2.6209, grad_fn=<CopyBackwards>) tensor(0.0706, grad_fn=<DivBackward0>) tensor(0.5041, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.6295, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.6209, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.6295, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.53748083 0.0 0.020576976 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.6293, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -11.4319315 -0.18704073 10.689654 37.217205\n",
      "activ tensor(2.6293, grad_fn=<CopyBackwards>) tensor(0.0706, grad_fn=<DivBackward0>) tensor(0.5180, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.6382, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.6293, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.6382, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5738577 0.0 0.022141526 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.6382, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -10.957948 -0.1919868 10.369044 37.301056\n",
      "activ tensor(2.6382, grad_fn=<CopyBackwards>) tensor(0.0707, grad_fn=<DivBackward0>) tensor(0.4941, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.6340, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.6382, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.6340, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5580182 0.0 0.021147825 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.6338, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -10.891912 -0.1982056 10.455106 37.39809\n",
      "activ tensor(2.6338, grad_fn=<CopyBackwards>) tensor(0.0704, grad_fn=<DivBackward0>) tensor(0.5359, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.6387, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.6338, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.6387, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5619939 0.0 0.021412108 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.6387, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -11.072314 -0.17562824 10.130006 37.506367\n",
      "activ tensor(2.6387, grad_fn=<CopyBackwards>) tensor(0.0704, grad_fn=<DivBackward0>) tensor(0.4488, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.6494, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.6387, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.6494, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5699651 0.0 0.021877244 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.6493, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -10.71828 -0.18281364 10.75767 37.6221\n",
      "activ tensor(2.6493, grad_fn=<CopyBackwards>) tensor(0.0704, grad_fn=<DivBackward0>) tensor(0.5005, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.6565, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.6493, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.6565, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56386745 0.0 0.021538962 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.6564, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -11.096862 -0.21033078 11.075273 37.74149\n",
      "activ tensor(2.6564, grad_fn=<CopyBackwards>) tensor(0.0704, grad_fn=<DivBackward0>) tensor(0.5004, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.6588, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.6564, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.6588, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55780816 0.0 0.021242965 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.6590, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -11.0093775 -0.2019856 10.913457 37.886223\n",
      "activ tensor(2.6590, grad_fn=<CopyBackwards>) tensor(0.0702, grad_fn=<DivBackward0>) tensor(0.4961, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.6690, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.6590, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.6690, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5615769 0.0 0.021591816 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.6694, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -11.44138 -0.2119954 10.778256 38.02868\n",
      "activ tensor(2.6694, grad_fn=<CopyBackwards>) tensor(0.0702, grad_fn=<DivBackward0>) tensor(0.5518, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.6599, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.6694, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.6599, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.569348 0.0 0.021982955 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.6598, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -11.756401 -0.22044608 10.677457 38.168903\n",
      "activ tensor(2.6598, grad_fn=<CopyBackwards>) tensor(0.0697, grad_fn=<DivBackward0>) tensor(0.4977, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.6591, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.6598, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.6591, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5800525 0.0 0.022289522 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.6592, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -11.776372 -0.23508513 11.010886 38.31173\n",
      "activ tensor(2.6592, grad_fn=<CopyBackwards>) tensor(0.0694, grad_fn=<DivBackward0>) tensor(0.4783, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.6593, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.6592, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.6593, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5522133 0.0 0.021147825 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.6593, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -11.268998 -0.2197798 11.198741 38.44701\n",
      "activ tensor(2.6593, grad_fn=<CopyBackwards>) tensor(0.0692, grad_fn=<DivBackward0>) tensor(0.4977, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.6628, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.6593, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.6628, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5708894 0.0 0.022067526 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.6628, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -11.215169 -0.21964245 11.58707 38.576862\n",
      "activ tensor(2.6628, grad_fn=<CopyBackwards>) tensor(0.0690, grad_fn=<DivBackward0>) tensor(0.4237, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.6648, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.6628, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.6648, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5600098 0.0 0.021475535 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.6649, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -11.898727 -0.20314737 10.907338 38.68625\n",
      "activ tensor(2.6649, grad_fn=<CopyBackwards>) tensor(0.0689, grad_fn=<DivBackward0>) tensor(0.5662, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.6639, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.6649, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.6639, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56053257 0.0 0.02126411 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.6640, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -11.998144 -0.19354226 10.576149 38.769257\n",
      "activ tensor(2.6640, grad_fn=<CopyBackwards>) tensor(0.0687, grad_fn=<DivBackward0>) tensor(0.4733, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.6683, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.6640, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.6683, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56697613 0.0 0.021982955 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.6684, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -11.381681 -0.20678008 11.319968 38.825207\n",
      "activ tensor(2.6684, grad_fn=<CopyBackwards>) tensor(0.0687, grad_fn=<DivBackward0>) tensor(0.4972, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.6777, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.6684, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.6777, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.568936 0.0 0.022130953 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.6777, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -11.264487 -0.20297983 11.086734 38.854805\n",
      "activ tensor(2.6777, grad_fn=<CopyBackwards>) tensor(0.0689, grad_fn=<DivBackward0>) tensor(0.4639, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.6838, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.6777, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.6838, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55644095 0.0 0.021242965 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.6837, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -11.438215 -0.1914801 11.242347 38.89365\n",
      "activ tensor(2.6837, grad_fn=<CopyBackwards>) tensor(0.0690, grad_fn=<DivBackward0>) tensor(0.5073, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.6877, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.6837, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.6877, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5578083 0.0 0.021105539 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.6878, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -11.0722685 -0.19582711 11.147439 38.925014\n",
      "activ tensor(2.6878, grad_fn=<CopyBackwards>) tensor(0.0691, grad_fn=<DivBackward0>) tensor(0.5401, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.6848, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.6878, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.6848, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57109475 0.0 0.021813814 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.6846, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -11.57169 -0.20520128 10.595019 38.961926\n",
      "activ tensor(2.6846, grad_fn=<CopyBackwards>) tensor(0.0689, grad_fn=<DivBackward0>) tensor(0.5315, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.6828, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.6846, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.6828, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57211965 0.0 0.021834958 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.6827, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -11.462464 -0.19376571 10.743598 39.00789\n",
      "activ tensor(2.6827, grad_fn=<CopyBackwards>) tensor(0.0688, grad_fn=<DivBackward0>) tensor(0.5366, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.6891, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.6827, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.6891, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5651129 0.0 0.021496676 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.6891, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -11.619174 -0.18837512 11.13631 39.075924\n",
      "activ tensor(2.6891, grad_fn=<CopyBackwards>) tensor(0.0688, grad_fn=<DivBackward0>) tensor(0.4952, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.6998, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.6891, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.6998, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5630357 0.0 0.021739816 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.6999, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -11.947915 -0.21678717 11.436803 39.162437\n",
      "activ tensor(2.6999, grad_fn=<CopyBackwards>) tensor(0.0689, grad_fn=<DivBackward0>) tensor(0.4485, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.7010, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.6999, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.7010, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5601142 0.0 0.021285253 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.7011, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -11.45226 -0.20479459 10.921266 39.26739\n",
      "activ tensor(2.7011, grad_fn=<CopyBackwards>) tensor(0.0688, grad_fn=<DivBackward0>) tensor(0.5299, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.7159, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.7011, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.7159, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5633477 0.0 0.021538962 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.7161, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -11.6646 -0.21724194 10.904184 39.383743\n",
      "activ tensor(2.7161, grad_fn=<CopyBackwards>) tensor(0.0690, grad_fn=<DivBackward0>) tensor(0.5422, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.7243, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.7161, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.7243, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55601966 0.0 0.021327537 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.7242, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -11.48921 -0.2273109 10.959969 39.5054\n",
      "activ tensor(2.7242, grad_fn=<CopyBackwards>) tensor(0.0690, grad_fn=<DivBackward0>) tensor(0.4834, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.7281, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.7242, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.7281, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5696563 0.0 0.021845529 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.7282, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -11.415359 -0.24070908 11.169617 39.6418\n",
      "activ tensor(2.7282, grad_fn=<CopyBackwards>) tensor(0.0688, grad_fn=<DivBackward0>) tensor(0.5470, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.7302, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.7282, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.7302, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.58767915 0.0 0.022828655 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.7302, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -11.624925 -0.22683193 11.882661 39.796204\n",
      "activ tensor(2.7302, grad_fn=<CopyBackwards>) tensor(0.0686, grad_fn=<DivBackward0>) tensor(0.5923, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.7403, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.7302, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.7403, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5475248 0.0 0.02070383 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.7401, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -11.654664 -0.20908928 11.35913 39.950302\n",
      "activ tensor(2.7401, grad_fn=<CopyBackwards>) tensor(0.0686, grad_fn=<DivBackward0>) tensor(0.5024, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.7400, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.7401, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.7400, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5529554 0.0 0.020989256 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.7399, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -12.058015 -0.20834371 11.29441 40.092243\n",
      "activ tensor(2.7399, grad_fn=<CopyBackwards>) tensor(0.0683, grad_fn=<DivBackward0>) tensor(0.5128, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.7451, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.7399, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.7451, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57802904 0.0 0.02227895 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.7450, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -12.309919 -0.20267642 11.470798 40.216293\n",
      "activ tensor(2.7450, grad_fn=<CopyBackwards>) tensor(0.0683, grad_fn=<DivBackward0>) tensor(0.4931, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.7524, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.7450, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.7524, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55759805 0.0 0.021158395 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.7524, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -12.306044 -0.21763015 11.422397 40.328293\n",
      "activ tensor(2.7524, grad_fn=<CopyBackwards>) tensor(0.0682, grad_fn=<DivBackward0>) tensor(0.5209, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.7573, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.7524, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.7573, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5670794 0.0 0.02161296 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.7575, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -11.8869095 -0.2239306 12.0645275 40.424072\n",
      "activ tensor(2.7575, grad_fn=<CopyBackwards>) tensor(0.0682, grad_fn=<DivBackward0>) tensor(0.5122, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.7634, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.7575, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.7634, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.551895 0.0 0.021221824 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.7633, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -11.846359 -0.21390252 11.992795 40.527893\n",
      "activ tensor(2.7633, grad_fn=<CopyBackwards>) tensor(0.0682, grad_fn=<DivBackward0>) tensor(0.5835, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.7758, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.7633, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.7758, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5620982 0.0 0.021475535 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.7759, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -12.575798 -0.2212249 11.545062 40.627823\n",
      "activ tensor(2.7759, grad_fn=<CopyBackwards>) tensor(0.0683, grad_fn=<DivBackward0>) tensor(0.5630, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.7837, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.7759, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.7837, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5710948 0.0 0.022035811 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.7835, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -12.654742 -0.2239741 11.265847 40.714413\n",
      "activ tensor(2.7835, grad_fn=<CopyBackwards>) tensor(0.0684, grad_fn=<DivBackward0>) tensor(0.5307, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.7855, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.7835, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.7855, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5721198 0.0 0.021845529 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.7857, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -11.973155 -0.22824854 11.80531 40.796204\n",
      "activ tensor(2.7857, grad_fn=<CopyBackwards>) tensor(0.0683, grad_fn=<DivBackward0>) tensor(0.4892, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.7898, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.7857, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.7898, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5605326 0.0 0.021454392 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.7898, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -11.760568 -0.19519992 11.997845 40.88391\n",
      "activ tensor(2.7898, grad_fn=<CopyBackwards>) tensor(0.0682, grad_fn=<DivBackward0>) tensor(0.5824, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.7873, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.7898, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.7873, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.569965 0.0 0.02167639 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.7873, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -12.09804 -0.2181971 11.617465 40.956757\n",
      "activ tensor(2.7873, grad_fn=<CopyBackwards>) tensor(0.0681, grad_fn=<DivBackward0>) tensor(0.4779, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.7937, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.7873, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.7937, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55854297 0.0 0.021464963 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.7939, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -11.899058 -0.22290541 11.846263 41.031136\n",
      "activ tensor(2.7939, grad_fn=<CopyBackwards>) tensor(0.0681, grad_fn=<DivBackward0>) tensor(0.5846, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.7996, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.7939, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.7996, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57457227 0.0 0.021972386 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.7997, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -12.18149 -0.2126413 11.210498 41.094715\n",
      "activ tensor(2.7997, grad_fn=<CopyBackwards>) tensor(0.0681, grad_fn=<DivBackward0>) tensor(0.6518, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.7963, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.7997, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.7963, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56438667 0.0 0.021464963 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.7962, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -11.872477 -0.23110661 11.287208 41.14238\n",
      "activ tensor(2.7962, grad_fn=<CopyBackwards>) tensor(0.0680, grad_fn=<DivBackward0>) tensor(0.5871, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.8095, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.7962, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.8095, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5603236 0.0 0.021327538 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.8096, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -12.195737 -0.23238151 12.004799 41.196636\n",
      "activ tensor(2.8096, grad_fn=<CopyBackwards>) tensor(0.0682, grad_fn=<DivBackward0>) tensor(0.5666, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.8047, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.8096, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.8047, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56697595 0.0 0.021538962 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.8047, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -12.675022 -0.22370547 11.778728 41.265232\n",
      "activ tensor(2.8047, grad_fn=<CopyBackwards>) tensor(0.0680, grad_fn=<DivBackward0>) tensor(0.5990, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.8040, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.8047, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.8040, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.564179 0.0 0.021549534 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.8039, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -12.1108465 -0.20520556 11.696856 41.34196\n",
      "activ tensor(2.8039, grad_fn=<CopyBackwards>) tensor(0.0678, grad_fn=<DivBackward0>) tensor(0.5096, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.8125, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.8039, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.8125, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56480193 0.0 0.021570675 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.8124, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -12.413752 -0.20934959 11.376805 41.420902\n",
      "activ tensor(2.8124, grad_fn=<CopyBackwards>) tensor(0.0679, grad_fn=<DivBackward0>) tensor(0.4902, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.8119, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.8124, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.8119, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5627233 0.0 0.021496676 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.8119, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -12.064479 -0.23055643 11.552813 41.507355\n",
      "activ tensor(2.8119, grad_fn=<CopyBackwards>) tensor(0.0677, grad_fn=<DivBackward0>) tensor(0.5813, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.8231, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.8119, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.8231, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56480205 0.0 0.021644674 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.8231, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -12.112873 -0.21460788 11.975641 41.60111\n",
      "activ tensor(2.8231, grad_fn=<CopyBackwards>) tensor(0.0679, grad_fn=<DivBackward0>) tensor(0.6170, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.8308, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.8231, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.8308, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.58186764 0.0 0.022300094 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.8309, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -12.417666 -0.2350033 12.253613 41.70185\n",
      "activ tensor(2.8309, grad_fn=<CopyBackwards>) tensor(0.0679, grad_fn=<DivBackward0>) tensor(0.5345, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.8379, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.8309, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.8379, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57099193 0.0 0.021961814 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.8378, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -12.306151 -0.23105738 12.132249 41.842148\n",
      "activ tensor(2.8378, grad_fn=<CopyBackwards>) tensor(0.0678, grad_fn=<DivBackward0>) tensor(0.5402, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.8457, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.8378, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.8457, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5779276 0.0 0.022257805 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.8459, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -12.674297 -0.23595035 11.684497 41.989414\n",
      "activ tensor(2.8459, grad_fn=<CopyBackwards>) tensor(0.0678, grad_fn=<DivBackward0>) tensor(0.5832, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.8464, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.8459, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.8464, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5656311 0.0 0.021750389 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.8463, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -12.838219 -0.25046462 12.15751 42.142044\n",
      "activ tensor(2.8463, grad_fn=<CopyBackwards>) tensor(0.0675, grad_fn=<DivBackward0>) tensor(0.5907, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.8493, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.8463, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.8493, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.54548776 0.0 0.020481834 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.8494, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -12.852049 -0.23384577 12.159583 42.29983\n",
      "activ tensor(2.8494, grad_fn=<CopyBackwards>) tensor(0.0674, grad_fn=<DivBackward0>) tensor(0.5065, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.8563, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.8494, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.8563, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5679053 0.0 0.0218561 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.8563, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -12.484757 -0.21799378 12.477839 42.45504\n",
      "activ tensor(2.8563, grad_fn=<CopyBackwards>) tensor(0.0673, grad_fn=<DivBackward0>) tensor(0.5404, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.8669, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.8563, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.8669, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57027334 0.0 0.021813814 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.8669, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -12.510647 -0.22162627 12.820471 42.599552\n",
      "activ tensor(2.8669, grad_fn=<CopyBackwards>) tensor(0.0673, grad_fn=<DivBackward0>) tensor(0.5466, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.8678, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.8669, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.8678, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57314265 0.0 0.021940669 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.8679, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -13.220686 -0.2275489 11.895537 42.72872\n",
      "activ tensor(2.8679, grad_fn=<CopyBackwards>) tensor(0.0671, grad_fn=<DivBackward0>) tensor(0.5549, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.8756, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.8679, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.8756, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5781302 0.0 0.022363521 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.8757, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -13.309984 -0.24552944 12.032824 42.827076\n",
      "activ tensor(2.8757, grad_fn=<CopyBackwards>) tensor(0.0671, grad_fn=<DivBackward0>) tensor(0.5633, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.8771, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.8757, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.8771, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.569348 0.0 0.0217821 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.8769, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -12.56275 -0.23637879 12.46133 42.90354\n",
      "activ tensor(2.8769, grad_fn=<CopyBackwards>) tensor(0.0671, grad_fn=<DivBackward0>) tensor(0.6366, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.8805, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.8769, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.8805, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5603235 0.0 0.021369822 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.8806, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -12.480718 -0.23603725 12.543933 42.969593\n",
      "activ tensor(2.8806, grad_fn=<CopyBackwards>) tensor(0.0670, grad_fn=<DivBackward0>) tensor(0.5950, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.8879, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.8806, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.8879, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.562515 0.0 0.021390965 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.8878, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -12.450915 -0.22314528 12.421822 43.02854\n",
      "activ tensor(2.8878, grad_fn=<CopyBackwards>) tensor(0.0671, grad_fn=<DivBackward0>) tensor(0.6126, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.8889, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.8878, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.8889, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56376344 0.0 0.021570677 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.8889, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -12.493971 -0.20829174 12.169277 43.077614\n",
      "activ tensor(2.8889, grad_fn=<CopyBackwards>) tensor(0.0671, grad_fn=<DivBackward0>) tensor(0.5352, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.8904, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.8889, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.8904, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5674926 0.0 0.021750389 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.8905, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -12.627289 -0.20945764 12.038415 43.113674\n",
      "activ tensor(2.8905, grad_fn=<CopyBackwards>) tensor(0.0670, grad_fn=<DivBackward0>) tensor(0.5958, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.8948, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.8905, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.8948, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5535908 0.0 0.02095754 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.8949, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -12.439171 -0.23128682 11.868924 43.143993\n",
      "activ tensor(2.8949, grad_fn=<CopyBackwards>) tensor(0.0671, grad_fn=<DivBackward0>) tensor(0.5501, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.8967, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.8949, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.8967, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5657346 0.0 0.021634102 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.8968, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -12.904778 -0.24517372 12.608023 43.190414\n",
      "activ tensor(2.8968, grad_fn=<CopyBackwards>) tensor(0.0671, grad_fn=<DivBackward0>) tensor(0.5974, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.9095, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.8968, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.9095, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.53562456 0.0 0.020132983 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.9097, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -13.133813 -0.22843844 12.528604 43.250587\n",
      "activ tensor(2.9097, grad_fn=<CopyBackwards>) tensor(0.0673, grad_fn=<DivBackward0>) tensor(0.5578, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.9123, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.9097, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.9123, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5685239 0.0 0.021676388 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.9121, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -12.472315 -0.23181319 12.024166 43.33439\n",
      "activ tensor(2.9121, grad_fn=<CopyBackwards>) tensor(0.0672, grad_fn=<DivBackward0>) tensor(0.5708, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.9222, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.9121, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.9222, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.571915 0.0 0.021982955 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.9224, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -12.801651 -0.26712114 12.210673 43.429874\n",
      "activ tensor(2.9224, grad_fn=<CopyBackwards>) tensor(0.0673, grad_fn=<DivBackward0>) tensor(0.5765, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.9253, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.9224, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.9253, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57171 0.0 0.022046383 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.9251, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -12.8181505 -0.2489863 12.001845 43.543232\n",
      "activ tensor(2.9251, grad_fn=<CopyBackwards>) tensor(0.0672, grad_fn=<DivBackward0>) tensor(0.6015, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.9335, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.9251, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.9335, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56345165 0.0 0.021348678 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.9334, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -12.78975 -0.25238028 12.68358 43.672478\n",
      "activ tensor(2.9334, grad_fn=<CopyBackwards>) tensor(0.0672, grad_fn=<DivBackward0>) tensor(0.5667, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.9388, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.9334, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.9388, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57222193 0.0 0.021951241 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.9388, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -12.980725 -0.24046528 12.923033 43.820778\n",
      "activ tensor(2.9388, grad_fn=<CopyBackwards>) tensor(0.0671, grad_fn=<DivBackward0>) tensor(0.5701, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.9445, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.9388, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.9445, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.542904 0.0 0.020333838 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.9446, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -12.949123 -0.22575244 12.481118 43.965115\n",
      "activ tensor(2.9446, grad_fn=<CopyBackwards>) tensor(0.0670, grad_fn=<DivBackward0>) tensor(0.5398, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.9418, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.9446, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.9418, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55093855 0.0 0.020904684 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.9419, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -13.293297 -0.2343637 12.489955 44.093452\n",
      "activ tensor(2.9419, grad_fn=<CopyBackwards>) tensor(0.0667, grad_fn=<DivBackward0>) tensor(0.6664, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.9579, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.9419, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.9579, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5769128 0.0 0.022310665 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.9580, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -13.35472 -0.25031912 12.576873 44.206882\n",
      "activ tensor(2.9580, grad_fn=<CopyBackwards>) tensor(0.0669, grad_fn=<DivBackward0>) tensor(0.6885, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.9605, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.9580, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.9605, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57985044 0.0 0.022522088 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.9604, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -13.385967 -0.24840885 12.946814 44.3185\n",
      "activ tensor(2.9604, grad_fn=<CopyBackwards>) tensor(0.0668, grad_fn=<DivBackward0>) tensor(0.5173, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.9660, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.9604, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.9660, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56084603 0.0 0.021338107 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.9660, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -13.107143 -0.22962442 13.070895 44.433018\n",
      "activ tensor(2.9660, grad_fn=<CopyBackwards>) tensor(0.0668, grad_fn=<DivBackward0>) tensor(0.6463, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.9730, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.9660, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.9730, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56220233 0.0 0.021464964 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.9730, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -13.155058 -0.23955092 13.264491 44.54253\n",
      "activ tensor(2.9730, grad_fn=<CopyBackwards>) tensor(0.0667, grad_fn=<DivBackward0>) tensor(0.6362, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.9835, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.9730, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.9835, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5538023 0.0 0.021084398 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.9836, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -13.882039 -0.229402 12.704702 44.642555\n",
      "activ tensor(2.9836, grad_fn=<CopyBackwards>) tensor(0.0668, grad_fn=<DivBackward0>) tensor(0.5383, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.9877, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.9836, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.9877, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56334764 0.0 0.021655245 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.9876, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -13.925525 -0.20475799 12.410663 44.73217\n",
      "activ tensor(2.9876, grad_fn=<CopyBackwards>) tensor(0.0668, grad_fn=<DivBackward0>) tensor(0.6588, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.9939, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.9876, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.9939, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56334764 0.0 0.021380393 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.9940, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -13.172289 -0.2608645 13.340468 44.817356\n",
      "activ tensor(2.9940, grad_fn=<CopyBackwards>) tensor(0.0668, grad_fn=<DivBackward0>) tensor(0.5744, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.9995, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.9940, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.9995, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5857817 0.0 0.022448093 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.9996, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -13.159675 -0.25167552 13.09307 44.918465\n",
      "activ tensor(2.9996, grad_fn=<CopyBackwards>) tensor(0.0668, grad_fn=<DivBackward0>) tensor(0.6114, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.0089, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.9996, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.0089, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56188977 0.0 0.02153896 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.0090, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -13.147393 -0.23695797 12.980475 45.012135\n",
      "activ tensor(3.0090, grad_fn=<CopyBackwards>) tensor(0.0668, grad_fn=<DivBackward0>) tensor(0.6211, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.0142, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.0090, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.0142, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56573457 0.0 0.02161296 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.0140, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -13.152987 -0.2635293 12.892241 45.105854\n",
      "activ tensor(3.0140, grad_fn=<CopyBackwards>) tensor(0.0668, grad_fn=<DivBackward0>) tensor(0.6686, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.0151, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.0140, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.0151, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57487804 0.0 0.021866672 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.0151, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -13.180162 -0.2649699 12.44343 45.19898\n",
      "activ tensor(3.0151, grad_fn=<CopyBackwards>) tensor(0.0667, grad_fn=<DivBackward0>) tensor(0.6377, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.0216, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.0151, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.0216, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5582282 0.0 0.021158395 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.0215, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -13.234348 -0.2499257 12.787259 45.289726\n",
      "activ tensor(3.0215, grad_fn=<CopyBackwards>) tensor(0.0667, grad_fn=<DivBackward0>) tensor(0.6135, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.0321, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.0215, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.0321, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5678022 0.0 0.021623531 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.0320, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -13.693109 -0.24313374 13.075916 45.3789\n",
      "activ tensor(3.0320, grad_fn=<CopyBackwards>) tensor(0.0668, grad_fn=<DivBackward0>) tensor(0.5312, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.0320, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.0320, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.0320, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5631396 0.0 0.021401534 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.0320, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -13.626752 -0.24645713 13.159151 45.468204\n",
      "activ tensor(3.0320, grad_fn=<CopyBackwards>) tensor(0.0667, grad_fn=<DivBackward0>) tensor(0.6733, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.0336, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.0320, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.0336, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5661488 0.0 0.021750387 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.0337, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -13.163134 -0.23825753 12.677247 45.540787\n",
      "activ tensor(3.0337, grad_fn=<CopyBackwards>) tensor(0.0666, grad_fn=<DivBackward0>) tensor(0.6477, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.0460, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.0337, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.0460, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5754893 0.0 0.022257807 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.0461, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -13.620223 -0.23835427 12.613872 45.60091\n",
      "activ tensor(3.0461, grad_fn=<CopyBackwards>) tensor(0.0668, grad_fn=<DivBackward0>) tensor(0.6186, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.0479, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.0461, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.0479, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57385796 0.0 0.022099238 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.0480, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -13.447958 -0.26943982 12.886666 45.65712\n",
      "activ tensor(3.0480, grad_fn=<CopyBackwards>) tensor(0.0668, grad_fn=<DivBackward0>) tensor(0.6260, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.0488, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.0480, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.0488, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56282747 0.0 0.021591818 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.0487, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -13.665113 -0.23090233 13.092026 45.721844\n",
      "activ tensor(3.0487, grad_fn=<CopyBackwards>) tensor(0.0667, grad_fn=<DivBackward0>) tensor(0.6227, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.0579, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.0487, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.0579, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55833316 0.0 0.021158395 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.0579, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -13.4934225 -0.2452704 13.656922 45.79401\n",
      "activ tensor(3.0579, grad_fn=<CopyBackwards>) tensor(0.0668, grad_fn=<DivBackward0>) tensor(0.6015, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.0624, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.0579, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.0624, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5738578 0.0 0.022004098 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.0623, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -13.587979 -0.2716125 12.999902 45.879604\n",
      "activ tensor(3.0623, grad_fn=<CopyBackwards>) tensor(0.0667, grad_fn=<DivBackward0>) tensor(0.7110, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.0663, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.0623, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.0663, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5531674 0.0 0.020978685 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.0664, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -13.900658 -0.24154855 12.944062 45.981533\n",
      "activ tensor(3.0664, grad_fn=<CopyBackwards>) tensor(0.0667, grad_fn=<DivBackward0>) tensor(0.5401, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.0703, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.0664, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.0703, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.554014 0.0 0.020830683 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.0702, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -13.872211 -0.23718195 13.435055 46.094326\n",
      "activ tensor(3.0702, grad_fn=<CopyBackwards>) tensor(0.0666, grad_fn=<DivBackward0>) tensor(0.5691, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.0801, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.0702, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.0801, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57609975 0.0 0.022162667 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.0802, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -13.910003 -0.2538257 13.353574 46.22076\n",
      "activ tensor(3.0802, grad_fn=<CopyBackwards>) tensor(0.0666, grad_fn=<DivBackward0>) tensor(0.6159, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.0843, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.0802, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.0843, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.581868 0.0 0.022405803 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.0846, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -13.735806 -0.2618215 13.860942 46.361515\n",
      "activ tensor(3.0846, grad_fn=<CopyBackwards>) tensor(0.0665, grad_fn=<DivBackward0>) tensor(0.6691, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.0916, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.0846, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.0916, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57630324 0.0 0.022268377 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.0917, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -13.813921 -0.2584201 13.732076 46.499012\n",
      "activ tensor(3.0917, grad_fn=<CopyBackwards>) tensor(0.0665, grad_fn=<DivBackward0>) tensor(0.5912, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.0942, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.0917, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.0942, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55507046 0.0 0.021063253 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.0939, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -14.511002 -0.2676555 13.259782 46.646282\n",
      "activ tensor(3.0939, grad_fn=<CopyBackwards>) tensor(0.0663, grad_fn=<DivBackward0>) tensor(0.6027, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.0906, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.0939, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.0906, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5715048 0.0 0.02202524 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.0907, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -14.553629 -0.28324565 13.211976 46.788692\n",
      "activ tensor(3.0907, grad_fn=<CopyBackwards>) tensor(0.0661, grad_fn=<DivBackward0>) tensor(0.5977, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.0956, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.0907, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.0956, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56728595 0.0 0.021718673 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.0955, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -13.756157 -0.24910665 13.766707 46.92761\n",
      "activ tensor(3.0955, grad_fn=<CopyBackwards>) tensor(0.0660, grad_fn=<DivBackward0>) tensor(0.6327, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.1053, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.0955, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.1053, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57853544 0.0 0.02227895 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.1053, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -13.613722 -0.2532707 13.984704 47.0533\n",
      "activ tensor(3.1053, grad_fn=<CopyBackwards>) tensor(0.0660, grad_fn=<DivBackward0>) tensor(0.6157, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.1125, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.1053, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.1125, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5634516 0.0 0.021475533 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.1125, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -13.831441 -0.24993014 13.384566 47.16243\n",
      "activ tensor(3.1125, grad_fn=<CopyBackwards>) tensor(0.0660, grad_fn=<DivBackward0>) tensor(0.7155, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.1182, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.1125, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.1182, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5492344 0.0 0.020904686 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.1183, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -13.881762 -0.25373182 13.528603 47.253235\n",
      "activ tensor(3.1183, grad_fn=<CopyBackwards>) tensor(0.0660, grad_fn=<DivBackward0>) tensor(0.5562, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.1262, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.1183, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.1262, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56955373 0.0 0.021750387 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.1262, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -14.029731 -0.24830317 13.159015 47.32421\n",
      "activ tensor(3.1262, grad_fn=<CopyBackwards>) tensor(0.0661, grad_fn=<DivBackward0>) tensor(0.5880, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.1390, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.1262, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.1390, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.58086 0.0 0.022321235 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.1390, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -13.751729 -0.24620274 13.286337 47.377125\n",
      "activ tensor(3.1390, grad_fn=<CopyBackwards>) tensor(0.0663, grad_fn=<DivBackward0>) tensor(0.6722, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.1412, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.1390, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.1412, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55485946 0.0 0.020968113 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.1412, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -14.185455 -0.25554794 13.950888 47.42439\n",
      "activ tensor(3.1412, grad_fn=<CopyBackwards>) tensor(0.0662, grad_fn=<DivBackward0>) tensor(0.7364, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.1448, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.1412, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.1448, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.58327574 0.0 0.022426948 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.1448, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -14.290057 -0.25268134 13.50793 47.471455\n",
      "activ tensor(3.1448, grad_fn=<CopyBackwards>) tensor(0.0662, grad_fn=<DivBackward0>) tensor(0.6366, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.1411, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.1448, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.1411, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5660452 0.0 0.021486104 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.1411, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -14.0938015 -0.2613411 13.386171 47.524426\n",
      "activ tensor(3.1411, grad_fn=<CopyBackwards>) tensor(0.0661, grad_fn=<DivBackward0>) tensor(0.6823, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.1507, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.1411, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.1507, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5501937 0.0 0.020851828 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.1507, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -14.239411 -0.25947416 13.21646 47.583645\n",
      "activ tensor(3.1507, grad_fn=<CopyBackwards>) tensor(0.0662, grad_fn=<DivBackward0>) tensor(0.5640, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.1549, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.1507, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.1549, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5785354 0.0 0.022247236 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.1548, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -13.74833 -0.24191403 13.400495 47.65373\n",
      "activ tensor(3.1548, grad_fn=<CopyBackwards>) tensor(0.0662, grad_fn=<DivBackward0>) tensor(0.6503, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.1671, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.1548, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.1671, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5573878 0.0 0.021454392 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.1671, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -14.255028 -0.26253057 13.94649 47.731228\n",
      "activ tensor(3.1671, grad_fn=<CopyBackwards>) tensor(0.0664, grad_fn=<DivBackward0>) tensor(0.6181, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.1718, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.1671, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.1718, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.564179 0.0 0.02135925 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.1719, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -14.22091 -0.2812319 13.986165 47.826122\n",
      "activ tensor(3.1719, grad_fn=<CopyBackwards>) tensor(0.0663, grad_fn=<DivBackward0>) tensor(0.6549, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.1930, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.1719, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.1930, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.54741794 0.0 0.020862399 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.1932, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -14.24185 -0.28519252 13.747688 47.927094\n",
      "activ tensor(3.1932, grad_fn=<CopyBackwards>) tensor(0.0666, grad_fn=<DivBackward0>) tensor(0.6000, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.2000, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.1932, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.2000, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57436836 0.0 0.022078095 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.1999, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -14.492405 -0.2705081 13.413587 48.04157\n",
      "activ tensor(3.1999, grad_fn=<CopyBackwards>) tensor(0.0666, grad_fn=<DivBackward0>) tensor(0.6182, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.2027, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.1999, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.2027, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5543313 0.0 0.02103154 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.2028, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -14.383056 -0.28337264 14.056853 48.16777\n",
      "activ tensor(3.2028, grad_fn=<CopyBackwards>) tensor(0.0665, grad_fn=<DivBackward0>) tensor(0.6561, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.2015, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.2028, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.2015, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56707937 0.0 0.021623533 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.2014, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -14.43958 -0.28610694 14.153062 48.29843\n",
      "activ tensor(3.2014, grad_fn=<CopyBackwards>) tensor(0.0663, grad_fn=<DivBackward0>) tensor(0.7600, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.2118, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.2014, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.2118, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55927664 0.0 0.021496678 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.2119, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -14.36195 -0.26912436 14.2112665 48.43536\n",
      "activ tensor(3.2119, grad_fn=<CopyBackwards>) tensor(0.0663, grad_fn=<DivBackward0>) tensor(0.5901, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.2105, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.2119, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.2105, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5563358 0.0 0.021242969 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.2105, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -14.480129 -0.2530201 14.527841 48.577663\n",
      "activ tensor(3.2105, grad_fn=<CopyBackwards>) tensor(0.0661, grad_fn=<DivBackward0>) tensor(0.6617, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.2124, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.2105, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.2124, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5575982 0.0 0.021401534 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.2124, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -15.13098 -0.27149498 13.688275 48.7039\n",
      "activ tensor(3.2124, grad_fn=<CopyBackwards>) tensor(0.0660, grad_fn=<DivBackward0>) tensor(0.5827, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.2194, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.2124, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.2194, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5629315 0.0 0.021549534 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.2195, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -15.154137 -0.24891542 13.911057 48.8218\n",
      "activ tensor(3.2195, grad_fn=<CopyBackwards>) tensor(0.0659, grad_fn=<DivBackward0>) tensor(0.6833, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.2249, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.2195, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.2249, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5775218 0.0 0.02227895 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.2249, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -14.365897 -0.24946511 14.510711 48.929977\n",
      "activ tensor(3.2249, grad_fn=<CopyBackwards>) tensor(0.0659, grad_fn=<DivBackward0>) tensor(0.6781, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.2290, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.2249, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.2290, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5786366 0.0 0.022374092 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.2288, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -14.310562 -0.27604163 14.424972 49.032608\n",
      "activ tensor(3.2288, grad_fn=<CopyBackwards>) tensor(0.0659, grad_fn=<DivBackward0>) tensor(0.6646, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.2397, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.2288, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.2397, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5668727 0.0 0.02169753 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.2397, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -14.542473 -0.26822796 14.217908 49.128174\n",
      "activ tensor(3.2397, grad_fn=<CopyBackwards>) tensor(0.0659, grad_fn=<DivBackward0>) tensor(0.6093, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.2430, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.2397, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.2430, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57447034 0.0 0.02209924 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.2429, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -14.582628 -0.26458833 13.875949 49.223637\n",
      "activ tensor(3.2429, grad_fn=<CopyBackwards>) tensor(0.0659, grad_fn=<DivBackward0>) tensor(0.6971, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.2512, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.2429, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.2512, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5753875 0.0 0.021961814 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.2511, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -14.652239 -0.2557258 13.954009 49.313885\n",
      "activ tensor(3.2511, grad_fn=<CopyBackwards>) tensor(0.0659, grad_fn=<DivBackward0>) tensor(0.6224, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.2544, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.2511, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.2544, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5604279 0.0 0.02145439 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.2543, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -14.264369 -0.2555122 13.954537 49.392048\n",
      "activ tensor(3.2543, grad_fn=<CopyBackwards>) tensor(0.0659, grad_fn=<DivBackward0>) tensor(0.6927, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.2601, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.2543, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.2601, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.53748095 0.0 0.020228123 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.2602, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -14.89492 -0.26209167 14.465352 49.47125\n",
      "activ tensor(3.2602, grad_fn=<CopyBackwards>) tensor(0.0659, grad_fn=<DivBackward0>) tensor(0.6616, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.2742, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.2602, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.2742, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56707954 0.0 0.021824386 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.2743, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -14.648336 -0.2701129 14.2847805 49.54916\n",
      "activ tensor(3.2743, grad_fn=<CopyBackwards>) tensor(0.0661, grad_fn=<DivBackward0>) tensor(0.6494, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.2819, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.2743, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.2819, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56074154 0.0 0.021295821 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.2820, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -14.661987 -0.29613167 13.7196 49.61021\n",
      "activ tensor(3.2820, grad_fn=<CopyBackwards>) tensor(0.0662, grad_fn=<DivBackward0>) tensor(0.6830, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.2855, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.2820, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.2855, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5527434 0.0 0.021137254 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.2854, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -14.638662 -0.2680209 14.038973 49.683865\n",
      "activ tensor(3.2854, grad_fn=<CopyBackwards>) tensor(0.0661, grad_fn=<DivBackward0>) tensor(0.7395, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.2879, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.2854, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.2879, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55927676 0.0 0.021274678 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.2881, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -14.353409 -0.28921196 13.945404 49.764046\n",
      "activ tensor(3.2881, grad_fn=<CopyBackwards>) tensor(0.0661, grad_fn=<DivBackward0>) tensor(0.6544, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.2964, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.2881, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.2964, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57150495 0.0 0.022004098 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.2964, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -14.973831 -0.29486647 14.534446 49.846012\n",
      "activ tensor(3.2964, grad_fn=<CopyBackwards>) tensor(0.0661, grad_fn=<DivBackward0>) tensor(0.7102, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.3072, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.2964, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.3072, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.58468044 0.0 0.022553802 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.3073, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -14.684169 -0.28056014 14.72425 49.936314\n",
      "activ tensor(3.3073, grad_fn=<CopyBackwards>) tensor(0.0662, grad_fn=<DivBackward0>) tensor(0.6590, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.3177, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.3073, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.3177, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55612504 0.0 0.02134868 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.3176, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -14.8623905 -0.26731393 14.049296 50.035667\n",
      "activ tensor(3.3176, grad_fn=<CopyBackwards>) tensor(0.0663, grad_fn=<DivBackward0>) tensor(0.6133, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.3275, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.3176, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.3275, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5767096 0.0 0.02219438 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.3274, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -15.100754 -0.28015387 14.242829 50.128883\n",
      "activ tensor(3.3274, grad_fn=<CopyBackwards>) tensor(0.0664, grad_fn=<DivBackward0>) tensor(0.6662, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.3352, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.3274, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.3352, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5626189 0.0 0.021412108 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.3352, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -14.874443 -0.28983754 14.527541 50.22805\n",
      "activ tensor(3.3352, grad_fn=<CopyBackwards>) tensor(0.0664, grad_fn=<DivBackward0>) tensor(0.7157, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.3392, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.3352, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.3392, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5842794 0.0 0.022638373 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.3393, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -14.974623 -0.26417297 14.858946 50.33405\n",
      "activ tensor(3.3393, grad_fn=<CopyBackwards>) tensor(0.0663, grad_fn=<DivBackward0>) tensor(0.7092, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.3399, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.3393, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.3399, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5774203 0.0 0.022310663 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.3399, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -14.996663 -0.28806162 14.855253 50.439827\n",
      "activ tensor(3.3399, grad_fn=<CopyBackwards>) tensor(0.0662, grad_fn=<DivBackward0>) tensor(0.6520, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.3431, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.3399, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.3431, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57191485 0.0 0.021982957 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.3431, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -15.139292 -0.29746452 14.898072 50.55008\n",
      "activ tensor(3.3431, grad_fn=<CopyBackwards>) tensor(0.0661, grad_fn=<DivBackward0>) tensor(0.6779, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.3493, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.3431, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.3493, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57823145 0.0 0.02219438 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.3492, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -15.746129 -0.2732705 14.536012 50.674686\n",
      "activ tensor(3.3492, grad_fn=<CopyBackwards>) tensor(0.0661, grad_fn=<DivBackward0>) tensor(0.7775, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.3617, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.3492, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.3617, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5631396 0.0 0.021549534 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.3617, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -15.7391205 -0.27466804 14.340245 50.797768\n",
      "activ tensor(3.3617, grad_fn=<CopyBackwards>) tensor(0.0662, grad_fn=<DivBackward0>) tensor(0.6249, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.3654, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.3617, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.3654, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5661488 0.0 0.021803245 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.3655, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -14.972622 -0.26739147 15.30393 50.918766\n",
      "activ tensor(3.3655, grad_fn=<CopyBackwards>) tensor(0.0661, grad_fn=<DivBackward0>) tensor(0.6026, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.3680, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.3655, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.3680, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.59451807 0.0 0.02293437 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.3680, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -14.886676 -0.2725996 15.041015 51.040077\n",
      "activ tensor(3.3680, grad_fn=<CopyBackwards>) tensor(0.0660, grad_fn=<DivBackward0>) tensor(0.6268, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.3758, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.3680, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.3758, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5715049 0.0 0.021887813 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.3759, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -15.22704 -0.26716822 14.676051 51.153214\n",
      "activ tensor(3.3759, grad_fn=<CopyBackwards>) tensor(0.0660, grad_fn=<DivBackward0>) tensor(0.6997, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.3863, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.3759, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.3863, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56303537 0.0 0.021486105 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.3865, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -15.3032675 -0.29741463 14.683476 51.25185\n",
      "activ tensor(3.3865, grad_fn=<CopyBackwards>) tensor(0.0661, grad_fn=<DivBackward0>) tensor(0.7077, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.3826, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.3865, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.3826, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5720175 0.0 0.022109812 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.3824, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -15.043881 -0.27906474 14.355342 51.356075\n",
      "activ tensor(3.3824, grad_fn=<CopyBackwards>) tensor(0.0659, grad_fn=<DivBackward0>) tensor(0.6898, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.3905, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.3824, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.3905, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5734496 0.0 0.021940669 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.3906, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -14.845597 -0.27947843 14.851165 51.45319\n",
      "activ tensor(3.3906, grad_fn=<CopyBackwards>) tensor(0.0659, grad_fn=<DivBackward0>) tensor(0.6679, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.3901, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.3906, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.3901, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.54731095 0.0 0.020841258 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.3901, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -15.560756 -0.2978198 14.981798 51.539173\n",
      "activ tensor(3.3901, grad_fn=<CopyBackwards>) tensor(0.0658, grad_fn=<DivBackward0>) tensor(0.6400, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.4007, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.3901, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.4007, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57109463 0.0 0.022025239 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.4007, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -15.159033 -0.27295476 14.818837 51.626556\n",
      "activ tensor(3.4007, grad_fn=<CopyBackwards>) tensor(0.0659, grad_fn=<DivBackward0>) tensor(0.7177, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.4035, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.4007, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.4035, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57088953 0.0 0.022141524 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.4034, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -15.12453 -0.25721523 14.452973 51.71285\n",
      "activ tensor(3.4034, grad_fn=<CopyBackwards>) tensor(0.0658, grad_fn=<DivBackward0>) tensor(0.7443, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.4069, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.4034, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.4069, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57559115 0.0 0.02202524 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.4069, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -15.45761 -0.27782324 14.441796 51.78154\n",
      "activ tensor(3.4069, grad_fn=<CopyBackwards>) tensor(0.0658, grad_fn=<DivBackward0>) tensor(0.5915, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.4109, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.4069, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.4109, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57914275 0.0 0.022226093 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.4110, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -15.15948 -0.292335 14.827189 51.85145\n",
      "activ tensor(3.4110, grad_fn=<CopyBackwards>) tensor(0.0658, grad_fn=<DivBackward0>) tensor(0.7071, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.4192, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.4110, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.4192, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5798506 0.0 0.022268377 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.4192, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -15.773366 -0.284043 14.978663 51.91907\n",
      "activ tensor(3.4192, grad_fn=<CopyBackwards>) tensor(0.0659, grad_fn=<DivBackward0>) tensor(0.7081, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.4343, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.4192, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.4343, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5772174 0.0 0.022384664 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.4343, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -15.182696 -0.30263138 15.349429 51.99231\n",
      "activ tensor(3.4343, grad_fn=<CopyBackwards>) tensor(0.0661, grad_fn=<DivBackward0>) tensor(0.7249, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.4396, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.4343, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.4396, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5877786 0.0 0.022786373 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.4397, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -15.498725 -0.3155846 14.6769285 52.08054\n",
      "activ tensor(3.4397, grad_fn=<CopyBackwards>) tensor(0.0660, grad_fn=<DivBackward0>) tensor(0.7197, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.4474, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.4397, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.4474, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57823163 0.0 0.02218381 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.4473, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -15.673706 -0.29605097 14.638232 52.181667\n",
      "activ tensor(3.4473, grad_fn=<CopyBackwards>) tensor(0.0661, grad_fn=<DivBackward0>) tensor(0.7020, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.4581, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.4473, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.4581, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56769896 0.0 0.021570675 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.4582, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -15.391396 -0.30007732 15.431134 52.28485\n",
      "activ tensor(3.4582, grad_fn=<CopyBackwards>) tensor(0.0661, grad_fn=<DivBackward0>) tensor(0.6051, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.4578, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.4582, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.4578, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55284953 0.0 0.021073828 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.4579, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -15.489628 -0.29262587 15.262876 52.38794\n",
      "activ tensor(3.4579, grad_fn=<CopyBackwards>) tensor(0.0660, grad_fn=<DivBackward0>) tensor(0.6390, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.4616, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.4579, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.4616, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.53617114 0.0 0.02010127 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.4617, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -15.647827 -0.30341232 15.566676 52.499805\n",
      "activ tensor(3.4617, grad_fn=<CopyBackwards>) tensor(0.0659, grad_fn=<DivBackward0>) tensor(0.6841, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.4607, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.4617, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.4607, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5623066 0.0 0.021507246 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.4609, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -15.795886 -0.28687665 15.4319315 52.617966\n",
      "activ tensor(3.4609, grad_fn=<CopyBackwards>) tensor(0.0658, grad_fn=<DivBackward0>) tensor(0.7956, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.4677, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.4609, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.4677, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5727338 0.0 0.02186667 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.4679, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -16.343163 -0.29896426 15.036459 52.72587\n",
      "activ tensor(3.4679, grad_fn=<CopyBackwards>) tensor(0.0658, grad_fn=<DivBackward0>) tensor(0.6825, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.4787, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.4679, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.4787, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5638675 0.0 0.021338109 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.4785, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -16.32051 -0.29364586 15.208745 52.834133\n",
      "activ tensor(3.4785, grad_fn=<CopyBackwards>) tensor(0.0658, grad_fn=<DivBackward0>) tensor(0.6380, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.4896, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.4785, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.4896, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57416415 0.0 0.021951241 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.4897, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -15.57727 -0.29063687 15.71013 52.939484\n",
      "activ tensor(3.4897, grad_fn=<CopyBackwards>) tensor(0.0659, grad_fn=<DivBackward0>) tensor(0.7309, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.4915, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.4897, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.4915, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55040646 0.0 0.020883542 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.4914, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -15.457476 -0.29700997 15.856993 53.045486\n",
      "activ tensor(3.4914, grad_fn=<CopyBackwards>) tensor(0.0658, grad_fn=<DivBackward0>) tensor(0.7456, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.4968, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.4914, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.4968, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.58085996 0.0 0.022553802 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.4968, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -15.918939 -0.2682007 15.159783 53.15318\n",
      "activ tensor(3.4968, grad_fn=<CopyBackwards>) tensor(0.0658, grad_fn=<DivBackward0>) tensor(0.7171, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.5072, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.4968, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.5072, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56614894 0.0 0.021496676 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.5071, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -15.993848 -0.26419455 15.242295 53.266018\n",
      "activ tensor(3.5071, grad_fn=<CopyBackwards>) tensor(0.0658, grad_fn=<DivBackward0>) tensor(0.6789, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.5151, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.5071, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.5151, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5825722 0.0 0.022712374 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.5151, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -15.672794 -0.27782822 15.17541 53.3646\n",
      "activ tensor(3.5151, grad_fn=<CopyBackwards>) tensor(0.0659, grad_fn=<DivBackward0>) tensor(0.6788, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.5138, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.5151, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.5138, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56532025 0.0 0.021708101 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.5139, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -15.686381 -0.29810572 15.295638 53.45598\n",
      "activ tensor(3.5139, grad_fn=<CopyBackwards>) tensor(0.0657, grad_fn=<DivBackward0>) tensor(0.6986, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.5261, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.5139, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.5261, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5623066 0.0 0.021581246 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.5261, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -15.995591 -0.30548 15.847104 53.552063\n",
      "activ tensor(3.5261, grad_fn=<CopyBackwards>) tensor(0.0658, grad_fn=<DivBackward0>) tensor(0.7659, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.5252, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.5261, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.5252, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.550726 0.0 0.021031542 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.5250, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -15.741453 -0.28873545 15.194532 53.655655\n",
      "activ tensor(3.5250, grad_fn=<CopyBackwards>) tensor(0.0657, grad_fn=<DivBackward0>) tensor(0.7083, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.5371, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.5250, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.5371, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5885756 0.0 0.02260666 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.5371, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -15.790716 -0.30995333 15.104741 53.757607\n",
      "activ tensor(3.5371, grad_fn=<CopyBackwards>) tensor(0.0658, grad_fn=<DivBackward0>) tensor(0.7244, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.5407, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.5371, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.5407, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5753875 0.0 0.021951241 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.5407, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -15.965413 -0.30880213 15.155761 53.860455\n",
      "activ tensor(3.5407, grad_fn=<CopyBackwards>) tensor(0.0657, grad_fn=<DivBackward0>) tensor(0.8340, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.5404, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.5407, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.5404, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5702733 0.0 0.021856101 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.5404, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -15.819785 -0.30811393 15.2890215 53.959686\n",
      "activ tensor(3.5404, grad_fn=<CopyBackwards>) tensor(0.0656, grad_fn=<DivBackward0>) tensor(0.6260, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.5486, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.5404, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.5486, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5687301 0.0 0.021972384 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.5485, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -16.275612 -0.29247415 15.826879 54.049496\n",
      "activ tensor(3.5485, grad_fn=<CopyBackwards>) tensor(0.0657, grad_fn=<DivBackward0>) tensor(0.8565, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.5594, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.5485, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.5594, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5585431 0.0 0.021295823 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.5594, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -15.857717 -0.30355787 15.691969 54.138515\n",
      "activ tensor(3.5594, grad_fn=<CopyBackwards>) tensor(0.0657, grad_fn=<DivBackward0>) tensor(0.7403, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.5669, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.5594, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.5669, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55707246 0.0 0.02125354 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.5670, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -16.120079 -0.29558995 15.36799 54.22384\n",
      "activ tensor(3.5670, grad_fn=<CopyBackwards>) tensor(0.0658, grad_fn=<DivBackward0>) tensor(0.6797, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.5679, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.5670, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.5679, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57006776 0.0 0.021750387 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.5680, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -16.255575 -0.30003634 15.225246 54.28995\n",
      "activ tensor(3.5680, grad_fn=<CopyBackwards>) tensor(0.0657, grad_fn=<DivBackward0>) tensor(0.7328, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.5793, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.5680, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.5793, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5716073 0.0 0.022141526 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.5791, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -15.884016 -0.33110732 15.961268 54.360558\n",
      "activ tensor(3.5791, grad_fn=<CopyBackwards>) tensor(0.0658, grad_fn=<DivBackward0>) tensor(0.6766, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.5873, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.5791, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.5873, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57304054 0.0 0.021655247 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.5873, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -16.035864 -0.3162753 16.113377 54.438324\n",
      "activ tensor(3.5873, grad_fn=<CopyBackwards>) tensor(0.0659, grad_fn=<DivBackward0>) tensor(0.7613, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.5927, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.5873, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.5927, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5514703 0.0 0.020968113 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.5926, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -16.27584 -0.2976251 15.871248 54.52161\n",
      "activ tensor(3.5926, grad_fn=<CopyBackwards>) tensor(0.0659, grad_fn=<DivBackward0>) tensor(0.7728, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.5977, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.5926, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.5977, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5484871 0.0 0.02070383 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.5978, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -16.469505 -0.29203144 16.19557 54.612022\n",
      "activ tensor(3.5978, grad_fn=<CopyBackwards>) tensor(0.0659, grad_fn=<DivBackward0>) tensor(0.7288, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.6078, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.5978, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.6078, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57620144 0.0 0.022162667 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.6079, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -16.91902 -0.28616402 15.549036 54.71151\n",
      "activ tensor(3.6079, grad_fn=<CopyBackwards>) tensor(0.0659, grad_fn=<DivBackward0>) tensor(0.7025, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.6139, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.6079, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.6139, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5797495 0.0 0.022183808 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.6139, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -16.88998 -0.3107016 15.845059 54.810406\n",
      "activ tensor(3.6139, grad_fn=<CopyBackwards>) tensor(0.0659, grad_fn=<DivBackward0>) tensor(0.7514, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.6200, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.6139, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.6200, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5631396 0.0 0.021591818 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.6202, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -16.194038 -0.32101783 16.503206 54.924107\n",
      "activ tensor(3.6202, grad_fn=<CopyBackwards>) tensor(0.0659, grad_fn=<DivBackward0>) tensor(0.7947, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.6318, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.6202, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.6318, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5792439 0.0 0.022342378 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.6321, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -16.10939 -0.31724715 16.23176 55.048122\n",
      "activ tensor(3.6321, grad_fn=<CopyBackwards>) tensor(0.0660, grad_fn=<DivBackward0>) tensor(0.7866, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.6335, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.6321, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.6335, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5790416 0.0 0.022099238 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.6334, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -16.611307 -0.29281378 15.980201 55.17675\n",
      "activ tensor(3.6334, grad_fn=<CopyBackwards>) tensor(0.0659, grad_fn=<DivBackward0>) tensor(0.7126, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.6387, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.6334, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.6387, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.54816663 0.0 0.020746116 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.6388, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -16.676357 -0.3130514 15.672915 55.304153\n",
      "activ tensor(3.6388, grad_fn=<CopyBackwards>) tensor(0.0658, grad_fn=<DivBackward0>) tensor(0.7228, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.6449, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.6388, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.6449, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5652166 0.0 0.021422677 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.6449, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -16.38716 -0.32602885 15.893776 55.43306\n",
      "activ tensor(3.6449, grad_fn=<CopyBackwards>) tensor(0.0658, grad_fn=<DivBackward0>) tensor(0.8207, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.6482, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.6449, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.6482, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5773189 0.0 0.02218381 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.6481, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -16.330778 -0.30831778 16.055756 55.559124\n",
      "activ tensor(3.6481, grad_fn=<CopyBackwards>) tensor(0.0657, grad_fn=<DivBackward0>) tensor(0.7590, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.6494, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.6481, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.6494, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5640751 0.0 0.021644674 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.6493, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -16.668776 -0.29278672 16.266474 55.67559\n",
      "activ tensor(3.6493, grad_fn=<CopyBackwards>) tensor(0.0655, grad_fn=<DivBackward0>) tensor(0.7040, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.6578, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.6493, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.6578, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56417894 0.0 0.021412106 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.6578, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -16.09235 -0.29446912 16.001396 55.782528\n",
      "activ tensor(3.6578, grad_fn=<CopyBackwards>) tensor(0.0656, grad_fn=<DivBackward0>) tensor(0.7518, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.6582, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.6578, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.6582, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56511307 0.0 0.02161296 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.6581, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -16.589455 -0.2876465 15.464721 55.870964\n",
      "activ tensor(3.6581, grad_fn=<CopyBackwards>) tensor(0.0655, grad_fn=<DivBackward0>) tensor(0.7318, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.6660, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.6581, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.6660, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55959105 0.0 0.021433247 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.6660, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -16.408607 -0.30151725 15.956318 55.93629\n",
      "activ tensor(3.6660, grad_fn=<CopyBackwards>) tensor(0.0655, grad_fn=<DivBackward0>) tensor(0.6843, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.6698, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.6660, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.6698, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.54214805 0.0 0.020545263 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.6697, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -16.572855 -0.3116176 15.915274 56.002556\n",
      "activ tensor(3.6697, grad_fn=<CopyBackwards>) tensor(0.0655, grad_fn=<DivBackward0>) tensor(0.8036, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.6721, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.6697, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.6721, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56011415 0.0 0.02134868 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.6721, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -16.993515 -0.31007048 16.312914 56.069042\n",
      "activ tensor(3.6721, grad_fn=<CopyBackwards>) tensor(0.0655, grad_fn=<DivBackward0>) tensor(0.7567, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.6774, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.6721, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.6774, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57304066 0.0 0.021982955 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.6774, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -16.308441 -0.30411845 16.464334 56.13818\n",
      "activ tensor(3.6774, grad_fn=<CopyBackwards>) tensor(0.0655, grad_fn=<DivBackward0>) tensor(0.7079, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.6792, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.6774, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.6792, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5688332 0.0 0.021782102 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.6793, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -16.725636 -0.29661927 15.685414 56.210255\n",
      "activ tensor(3.6793, grad_fn=<CopyBackwards>) tensor(0.0655, grad_fn=<DivBackward0>) tensor(0.7868, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.6836, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.6793, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.6836, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5640752 0.0 0.021623531 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.6835, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -16.829 -0.30354515 16.031906 56.290546\n",
      "activ tensor(3.6835, grad_fn=<CopyBackwards>) tensor(0.0654, grad_fn=<DivBackward0>) tensor(0.8109, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.6992, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.6835, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.6992, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56738925 0.0 0.021538962 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.6991, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -16.389412 -0.33295155 16.513042 56.372044\n",
      "activ tensor(3.6991, grad_fn=<CopyBackwards>) tensor(0.0656, grad_fn=<DivBackward0>) tensor(0.7671, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.7107, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.6991, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.7107, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5622026 0.0 0.021316964 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.7110, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -16.566769 -0.3376546 16.701601 56.471397\n",
      "activ tensor(3.7110, grad_fn=<CopyBackwards>) tensor(0.0657, grad_fn=<DivBackward0>) tensor(0.7451, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.7142, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.7110, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.7142, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56136805 0.0 0.021380391 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.7145, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -16.931679 -0.34350407 16.583546 56.582073\n",
      "activ tensor(3.7145, grad_fn=<CopyBackwards>) tensor(0.0656, grad_fn=<DivBackward0>) tensor(0.7501, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.7151, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.7145, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.7151, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56500924 0.0 0.021919528 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.7151, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -17.111666 -0.33792222 16.509548 56.700706\n",
      "activ tensor(3.7151, grad_fn=<CopyBackwards>) tensor(0.0655, grad_fn=<DivBackward0>) tensor(0.7609, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.7200, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.7151, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.7200, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55072576 0.0 0.020883543 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.7201, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -17.507483 -0.34555164 16.423761 56.825256\n",
      "activ tensor(3.7201, grad_fn=<CopyBackwards>) tensor(0.0655, grad_fn=<DivBackward0>) tensor(0.7739, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.7237, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.7201, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.7237, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57508177 0.0 0.022004098 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.7237, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -17.433699 -0.33767897 16.32725 56.946316\n",
      "activ tensor(3.7237, grad_fn=<CopyBackwards>) tensor(0.0654, grad_fn=<DivBackward0>) tensor(0.8522, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.7300, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.7237, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.7300, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5633476 0.0 0.021581246 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.7300, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -16.821665 -0.33067828 17.203058 57.06814\n",
      "activ tensor(3.7300, grad_fn=<CopyBackwards>) tensor(0.0654, grad_fn=<DivBackward0>) tensor(0.6724, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.7336, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.7300, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.7336, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5559142 0.0 0.02094697 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.7336, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -16.774073 -0.30810964 16.895979 57.18781\n",
      "activ tensor(3.7336, grad_fn=<CopyBackwards>) tensor(0.0653, grad_fn=<DivBackward0>) tensor(0.7178, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.7383, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.7336, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.7383, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5535908 0.0 0.02103154 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.7383, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -17.289917 -0.29619336 16.381664 57.295853\n",
      "activ tensor(3.7383, grad_fn=<CopyBackwards>) tensor(0.0652, grad_fn=<DivBackward0>) tensor(0.7793, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.7473, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.7383, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.7473, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5614724 0.0 0.02136982 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.7472, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -17.354063 -0.31076998 16.526518 57.391117\n",
      "activ tensor(3.7472, grad_fn=<CopyBackwards>) tensor(0.0653, grad_fn=<DivBackward0>) tensor(0.8242, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.7461, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.7472, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.7461, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5503 0.0 0.020703832 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.7462, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -16.89595 -0.29989535 16.335386 57.468502\n",
      "activ tensor(3.7462, grad_fn=<CopyBackwards>) tensor(0.0652, grad_fn=<DivBackward0>) tensor(0.7837, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.7524, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.7462, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.7524, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5686272 0.0 0.02176096 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.7522, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -16.908787 -0.29284942 16.869473 57.544968\n",
      "activ tensor(3.7522, grad_fn=<CopyBackwards>) tensor(0.0652, grad_fn=<DivBackward0>) tensor(0.7262, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.7655, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.7522, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.7655, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55749315 0.0 0.021433247 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.7655, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -17.220428 -0.31380686 16.862885 57.623035\n",
      "activ tensor(3.7655, grad_fn=<CopyBackwards>) tensor(0.0653, grad_fn=<DivBackward0>) tensor(0.7046, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.7683, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.7655, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.7683, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55093837 0.0 0.020862399 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.7682, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -16.921759 -0.302726 16.436666 57.71248\n",
      "activ tensor(3.7682, grad_fn=<CopyBackwards>) tensor(0.0653, grad_fn=<DivBackward0>) tensor(0.8590, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.7752, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.7682, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.7752, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56965667 0.0 0.021655245 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.7753, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -17.266916 -0.29977858 16.284344 57.8036\n",
      "activ tensor(3.7753, grad_fn=<CopyBackwards>) tensor(0.0653, grad_fn=<DivBackward0>) tensor(0.8212, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.7788, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.7753, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.7788, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.58277357 0.0 0.02252209 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.7786, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -17.200726 -0.3058573 16.363972 57.89688\n",
      "activ tensor(3.7786, grad_fn=<CopyBackwards>) tensor(0.0653, grad_fn=<DivBackward0>) tensor(0.7149, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.7851, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.7786, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.7851, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5779274 0.0 0.022352949 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.7851, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -17.184364 -0.33533654 16.773321 57.99546\n",
      "activ tensor(3.7851, grad_fn=<CopyBackwards>) tensor(0.0653, grad_fn=<DivBackward0>) tensor(0.8392, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.7922, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.7851, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.7922, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5756928 0.0 0.02217324 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.7925, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -17.670895 -0.3208031 16.793234 58.105846\n",
      "activ tensor(3.7925, grad_fn=<CopyBackwards>) tensor(0.0653, grad_fn=<DivBackward0>) tensor(0.7263, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.8012, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.7925, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.8012, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57640487 0.0 0.02210981 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.8013, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -16.949575 -0.33250353 16.998976 58.217903\n",
      "activ tensor(3.8013, grad_fn=<CopyBackwards>) tensor(0.0653, grad_fn=<DivBackward0>) tensor(0.8668, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.8061, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.8013, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.8061, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5747762 0.0 0.021803243 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.8059, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -17.328415 -0.35821387 16.405027 58.33088\n",
      "activ tensor(3.8059, grad_fn=<CopyBackwards>) tensor(0.0652, grad_fn=<DivBackward0>) tensor(0.9113, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.8129, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.8059, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.8129, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5684208 0.0 0.02194067 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.8129, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -17.385849 -0.3425945 16.414797 58.440678\n",
      "activ tensor(3.8129, grad_fn=<CopyBackwards>) tensor(0.0652, grad_fn=<DivBackward0>) tensor(0.7590, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.8147, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.8129, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.8147, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.58247155 0.0 0.022352949 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.8147, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -17.041756 -0.35940716 17.398497 58.54721\n",
      "activ tensor(3.8147, grad_fn=<CopyBackwards>) tensor(0.0652, grad_fn=<DivBackward0>) tensor(0.9165, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.8211, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.8147, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.8211, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5448431 0.0 0.020587547 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.8211, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -17.317177 -0.34799317 17.143034 58.653404\n",
      "activ tensor(3.8211, grad_fn=<CopyBackwards>) tensor(0.0651, grad_fn=<DivBackward0>) tensor(0.7132, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.8257, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.8211, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.8257, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55210716 0.0 0.021073828 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.8257, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -17.573816 -0.30943668 17.188267 58.754353\n",
      "activ tensor(3.8257, grad_fn=<CopyBackwards>) tensor(0.0651, grad_fn=<DivBackward0>) tensor(0.7706, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.8265, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.8257, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.8265, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56126386 0.0 0.021390963 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.8264, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -17.771261 -0.32915205 17.143513 58.844074\n",
      "activ tensor(3.8264, grad_fn=<CopyBackwards>) tensor(0.0650, grad_fn=<DivBackward0>) tensor(0.7995, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.8324, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.8264, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.8324, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5603233 0.0 0.021338105 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.8323, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -18.05782 -0.33730552 16.864506 58.917778\n",
      "activ tensor(3.8323, grad_fn=<CopyBackwards>) tensor(0.0650, grad_fn=<DivBackward0>) tensor(0.7410, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.8357, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.8323, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.8357, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56542385 0.0 0.02144382 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.8360, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -17.996325 -0.34282613 17.24444 58.983532\n",
      "activ tensor(3.8360, grad_fn=<CopyBackwards>) tensor(0.0650, grad_fn=<DivBackward0>) tensor(0.7764, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.8400, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.8360, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.8400, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5779274 0.0 0.022236666 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.8400, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -17.432743 -0.3127399 17.59485 59.064976\n",
      "activ tensor(3.8400, grad_fn=<CopyBackwards>) tensor(0.0650, grad_fn=<DivBackward0>) tensor(0.7286, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.8418, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.8400, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.8418, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55125755 0.0 0.021232396 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.8418, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -17.459953 -0.32978755 17.630129 59.149193\n",
      "activ tensor(3.8418, grad_fn=<CopyBackwards>) tensor(0.0650, grad_fn=<DivBackward0>) tensor(0.8216, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.8338, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.8418, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.8338, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55623055 0.0 0.021401536 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.8338, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -17.95949 -0.33218497 16.94809 59.24052\n",
      "activ tensor(3.8338, grad_fn=<CopyBackwards>) tensor(0.0647, grad_fn=<DivBackward0>) tensor(0.7998, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.8441, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.8338, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.8441, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.54430515 0.0 0.02059812 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.8441, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -18.01269 -0.3059308 17.037855 59.33661\n",
      "activ tensor(3.8441, grad_fn=<CopyBackwards>) tensor(0.0648, grad_fn=<DivBackward0>) tensor(0.8387, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.8499, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.8441, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.8499, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5717098 0.0 0.021982955 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.8499, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -17.51399 -0.33250284 17.212175 59.4422\n",
      "activ tensor(3.8499, grad_fn=<CopyBackwards>) tensor(0.0648, grad_fn=<DivBackward0>) tensor(0.7424, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.8558, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.8499, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.8558, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5843796 0.0 0.022701802 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.8558, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -17.727474 -0.3307556 17.2941 59.553673\n",
      "activ tensor(3.8558, grad_fn=<CopyBackwards>) tensor(0.0647, grad_fn=<DivBackward0>) tensor(0.8227, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.8684, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.8558, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.8684, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5859817 0.0 0.022659516 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.8686, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -17.600613 -0.31471422 17.668276 59.677563\n",
      "activ tensor(3.8686, grad_fn=<CopyBackwards>) tensor(0.0648, grad_fn=<DivBackward0>) tensor(0.9376, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.8754, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.8686, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.8754, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5790418 0.0 0.02218381 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.8754, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -17.510347 -0.33141428 16.895273 59.79571\n",
      "activ tensor(3.8754, grad_fn=<CopyBackwards>) tensor(0.0648, grad_fn=<DivBackward0>) tensor(0.8786, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.8810, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.8754, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.8810, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56893617 0.0 0.021919528 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.8809, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -17.644995 -0.3452299 16.857775 59.90131\n",
      "activ tensor(3.8809, grad_fn=<CopyBackwards>) tensor(0.0648, grad_fn=<DivBackward0>) tensor(0.7253, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.8886, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.8809, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.8886, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5527435 0.0 0.02112668 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.8887, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -17.595606 -0.35640687 17.190857 60.012108\n",
      "activ tensor(3.8887, grad_fn=<CopyBackwards>) tensor(0.0648, grad_fn=<DivBackward0>) tensor(0.7614, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.8906, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.8887, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.8906, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5815656 0.0 0.022342378 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.8905, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -18.049747 -0.32917383 17.179289 60.123917\n",
      "activ tensor(3.8905, grad_fn=<CopyBackwards>) tensor(0.0647, grad_fn=<DivBackward0>) tensor(0.8503, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.8988, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.8905, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.8988, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5756928 0.0 0.021813814 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.8986, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -18.11668 -0.33469814 17.621908 60.231247\n",
      "activ tensor(3.8986, grad_fn=<CopyBackwards>) tensor(0.0647, grad_fn=<DivBackward0>) tensor(0.8029, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.9101, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.8986, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.9101, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.54258 0.0 0.020397265 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.9100, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -17.588318 -0.31627977 17.372686 60.331017\n",
      "activ tensor(3.9100, grad_fn=<CopyBackwards>) tensor(0.0648, grad_fn=<DivBackward0>) tensor(0.8441, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.9183, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.9100, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.9183, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5590673 0.0 0.021560105 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.9183, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -17.926575 -0.32133266 17.050985 60.415577\n",
      "activ tensor(3.9183, grad_fn=<CopyBackwards>) tensor(0.0649, grad_fn=<DivBackward0>) tensor(0.8184, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.9248, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.9183, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.9248, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57160723 0.0 0.0218561 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.9249, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -17.938406 -0.34924528 17.102674 60.493637\n",
      "activ tensor(3.9249, grad_fn=<CopyBackwards>) tensor(0.0649, grad_fn=<DivBackward0>) tensor(0.8692, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.9365, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.9249, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.9365, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55411965 0.0 0.021094969 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.9366, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -17.55699 -0.33711815 17.85895 60.56938\n",
      "activ tensor(3.9366, grad_fn=<CopyBackwards>) tensor(0.0650, grad_fn=<DivBackward0>) tensor(0.7668, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.9394, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.9366, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.9394, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5527435 0.0 0.021094969 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.9393, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -17.795181 -0.33349752 17.990438 60.659367\n",
      "activ tensor(3.9393, grad_fn=<CopyBackwards>) tensor(0.0649, grad_fn=<DivBackward0>) tensor(0.8971, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.9451, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.9393, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.9451, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5772174 0.0 0.021961812 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.9450, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -18.211828 -0.35241467 17.509283 60.747616\n",
      "activ tensor(3.9450, grad_fn=<CopyBackwards>) tensor(0.0649, grad_fn=<DivBackward0>) tensor(0.8157, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.9450, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.9450, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.9450, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57853544 0.0 0.022352949 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.9449, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -18.417568 -0.36993015 17.841824 60.839073\n",
      "activ tensor(3.9449, grad_fn=<CopyBackwards>) tensor(0.0648, grad_fn=<DivBackward0>) tensor(0.8314, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.9489, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.9449, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.9489, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5577032 0.0 0.020989254 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.9490, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -18.615513 -0.35814604 17.490969 60.93988\n",
      "activ tensor(3.9490, grad_fn=<CopyBackwards>) tensor(0.0648, grad_fn=<DivBackward0>) tensor(0.8461, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.9507, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.9490, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.9507, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5588576 0.0 0.02117954 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.9507, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -18.527628 -0.3266803 17.791363 61.04502\n",
      "activ tensor(3.9507, grad_fn=<CopyBackwards>) tensor(0.0647, grad_fn=<DivBackward0>) tensor(0.8705, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.9590, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.9507, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.9590, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5789406 0.0 0.02234238 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.9590, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -18.07695 -0.35520226 18.430676 61.15119\n",
      "activ tensor(3.9590, grad_fn=<CopyBackwards>) tensor(0.0647, grad_fn=<DivBackward0>) tensor(0.8953, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.9579, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.9590, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.9579, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5617852 0.0 0.021507248 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.9580, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -18.127338 -0.35104463 17.953743 61.259483\n",
      "activ tensor(3.9580, grad_fn=<CopyBackwards>) tensor(0.0646, grad_fn=<DivBackward0>) tensor(0.8145, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.9610, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.9580, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.9610, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57416415 0.0 0.02212038 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.9611, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -18.64058 -0.35438976 17.752321 61.355194\n",
      "activ tensor(3.9611, grad_fn=<CopyBackwards>) tensor(0.0646, grad_fn=<DivBackward0>) tensor(0.8863, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.9645, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.9611, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.9645, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5662523 0.0 0.021549532 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.9645, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -18.647966 -0.36800396 17.560053 61.44973\n",
      "activ tensor(3.9645, grad_fn=<CopyBackwards>) tensor(0.0645, grad_fn=<DivBackward0>) tensor(0.7260, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.9731, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.9645, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.9731, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57222205 0.0 0.02186667 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.9732, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -18.131706 -0.347102 17.856302 61.550343\n",
      "activ tensor(3.9732, grad_fn=<CopyBackwards>) tensor(0.0646, grad_fn=<DivBackward0>) tensor(0.8114, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.9787, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.9732, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.9787, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57934505 0.0 0.022310665 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.9787, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -18.265587 -0.33216146 18.103342 61.65003\n",
      "activ tensor(3.9787, grad_fn=<CopyBackwards>) tensor(0.0645, grad_fn=<DivBackward0>) tensor(0.9662, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.9878, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.9787, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.9878, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57497984 0.0 0.021993525 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.9877, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -18.249063 -0.32901686 18.037804 61.74759\n",
      "activ tensor(3.9877, grad_fn=<CopyBackwards>) tensor(0.0646, grad_fn=<DivBackward0>) tensor(0.8376, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.9930, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.9877, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.9930, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.564698 0.0 0.021750389 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.9929, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -18.028156 -0.3256349 17.695559 61.843098\n",
      "activ tensor(3.9929, grad_fn=<CopyBackwards>) tensor(0.0646, grad_fn=<DivBackward0>) tensor(0.7190, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.9918, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.9929, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.9918, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55665165 0.0 0.02105268 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.9919, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -18.209179 -0.33981973 17.297714 61.935463\n",
      "activ tensor(3.9919, grad_fn=<CopyBackwards>) tensor(0.0645, grad_fn=<DivBackward0>) tensor(0.8317, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.9965, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.9919, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.9965, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5606371 0.0 0.02143325 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.9965, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -18.190401 -0.32380393 17.92171 62.028496\n",
      "activ tensor(3.9965, grad_fn=<CopyBackwards>) tensor(0.0644, grad_fn=<DivBackward0>) tensor(0.9062, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.0074, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.9965, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.0074, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.58377784 0.0 0.02243752 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.0074, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -18.71461 -0.3356508 17.899277 62.121677\n",
      "activ tensor(4.0074, grad_fn=<CopyBackwards>) tensor(0.0645, grad_fn=<DivBackward0>) tensor(0.9086, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.0144, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.0074, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.0144, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5868811 0.0 0.022701802 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.0142, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -18.801666 -0.34180844 18.00325 62.224545\n",
      "activ tensor(4.0142, grad_fn=<CopyBackwards>) tensor(0.0645, grad_fn=<DivBackward0>) tensor(0.7888, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.0226, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.0142, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.0226, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56759554 0.0 0.021655245 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.0226, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -18.06593 -0.35550052 18.179636 62.32614\n",
      "activ tensor(4.0226, grad_fn=<CopyBackwards>) tensor(0.0645, grad_fn=<DivBackward0>) tensor(0.7581, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.0158, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.0226, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.0158, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5770143 0.0 0.02228952 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.0156, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -18.492392 -0.36653802 17.405918 62.435257\n",
      "activ tensor(4.0156, grad_fn=<CopyBackwards>) tensor(0.0643, grad_fn=<DivBackward0>) tensor(0.8867, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.0116, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.0156, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.0116, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5516826 0.0 0.0208624 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.0115, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -18.496708 -0.34041736 17.87838 62.544537\n",
      "activ tensor(4.0115, grad_fn=<CopyBackwards>) tensor(0.0641, grad_fn=<DivBackward0>) tensor(0.8790, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.0100, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.0115, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.0100, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56199384 0.0 0.02143325 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.0100, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -18.179752 -0.34178886 18.47386 62.65373\n",
      "activ tensor(4.0100, grad_fn=<CopyBackwards>) tensor(0.0640, grad_fn=<DivBackward0>) tensor(0.9473, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.0173, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.0100, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.0173, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.562411 0.0 0.02128525 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.0173, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -18.489508 -0.3504893 18.478632 62.758686\n",
      "activ tensor(4.0173, grad_fn=<CopyBackwards>) tensor(0.0640, grad_fn=<DivBackward0>) tensor(0.9434, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.0202, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.0173, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.0202, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5852814 0.0 0.022617232 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.0204, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -18.855482 -0.36275446 18.262518 62.857895\n",
      "activ tensor(4.0204, grad_fn=<CopyBackwards>) tensor(0.0640, grad_fn=<DivBackward0>) tensor(0.7444, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.0313, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.0204, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.0313, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57518363 0.0 0.022289522 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.0315, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -19.04912 -0.38617006 18.16145 62.952328\n",
      "activ tensor(4.0315, grad_fn=<CopyBackwards>) tensor(0.0640, grad_fn=<DivBackward0>) tensor(0.8313, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.0349, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.0315, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.0349, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56386757 0.0 0.021581246 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.0347, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -19.164867 -0.35527623 18.337568 63.042545\n",
      "activ tensor(4.0347, grad_fn=<CopyBackwards>) tensor(0.0640, grad_fn=<DivBackward0>) tensor(0.8645, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.0396, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.0347, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.0396, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5457028 0.0 0.020587545 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.0397, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -19.064196 -0.38303205 18.356558 63.13235\n",
      "activ tensor(4.0397, grad_fn=<CopyBackwards>) tensor(0.0640, grad_fn=<DivBackward0>) tensor(0.8119, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.0452, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.0397, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.0452, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.54795283 0.0 0.020672116 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.0453, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -18.706093 -0.37428355 19.006144 63.226524\n",
      "activ tensor(4.0453, grad_fn=<CopyBackwards>) tensor(0.0640, grad_fn=<DivBackward0>) tensor(0.9099, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.0542, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.0453, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.0542, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55948627 0.0 0.021507248 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.0543, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -18.82716 -0.35310206 18.681414 63.322037\n",
      "activ tensor(4.0543, grad_fn=<CopyBackwards>) tensor(0.0640, grad_fn=<DivBackward0>) tensor(0.8551, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.0576, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.0543, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.0576, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55728275 0.0 0.02125354 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.0576, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -19.285395 -0.34959787 18.110191 63.415966\n",
      "activ tensor(4.0576, grad_fn=<CopyBackwards>) tensor(0.0640, grad_fn=<DivBackward0>) tensor(0.8026, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.0675, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.0576, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.0675, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56199414 0.0 0.021549532 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.0675, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -19.293095 -0.3482754 18.44977 63.50515\n",
      "activ tensor(4.0675, grad_fn=<CopyBackwards>) tensor(0.0640, grad_fn=<DivBackward0>) tensor(0.9627, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.0808, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.0675, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.0808, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5891726 0.0 0.022670086 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.0808, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -18.734287 -0.36445412 18.33872 63.59818\n",
      "activ tensor(4.0808, grad_fn=<CopyBackwards>) tensor(0.0642, grad_fn=<DivBackward0>) tensor(0.8709, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.0841, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.0808, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.0841, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5794463 0.0 0.022226093 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.0842, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -18.851482 -0.34736785 18.822412 63.68661\n",
      "activ tensor(4.0842, grad_fn=<CopyBackwards>) tensor(0.0641, grad_fn=<DivBackward0>) tensor(0.8083, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.0946, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.0842, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.0946, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55390817 0.0 0.021137254 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.0945, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -18.678452 -0.36755326 18.699879 63.78549\n",
      "activ tensor(4.0945, grad_fn=<CopyBackwards>) tensor(0.0642, grad_fn=<DivBackward0>) tensor(0.8365, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.0963, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.0945, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.0963, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56136805 0.0 0.021570675 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.0964, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -18.739712 -0.36198264 18.080881 63.891624\n",
      "activ tensor(4.0964, grad_fn=<CopyBackwards>) tensor(0.0641, grad_fn=<DivBackward0>) tensor(0.8360, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.1031, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.0964, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.1031, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55348504 0.0 0.02084126 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.1029, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -19.114298 -0.33445334 18.159945 63.993637\n",
      "activ tensor(4.1029, grad_fn=<CopyBackwards>) tensor(0.0641, grad_fn=<DivBackward0>) tensor(1.0118, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.1056, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.1029, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.1056, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57150483 0.0 0.02201467 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.1056, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -18.856197 -0.36424887 18.373798 64.09537\n",
      "activ tensor(4.1056, grad_fn=<CopyBackwards>) tensor(0.0641, grad_fn=<DivBackward0>) tensor(0.8705, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.1080, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.1056, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.1080, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5627232 0.0 0.021718673 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.1079, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -19.304718 -0.33101276 18.67896 64.2025\n",
      "activ tensor(4.1079, grad_fn=<CopyBackwards>) tensor(0.0640, grad_fn=<DivBackward0>) tensor(0.7528, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.1111, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.1079, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.1111, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5764047 0.0 0.02209924 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.1112, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -19.369373 -0.35142365 18.556131 64.30308\n",
      "activ tensor(4.1112, grad_fn=<CopyBackwards>) tensor(0.0639, grad_fn=<DivBackward0>) tensor(0.8548, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.1173, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.1112, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.1173, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5711973 0.0 0.02177153 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.1175, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -18.920645 -0.35363844 18.617807 64.40341\n",
      "activ tensor(4.1175, grad_fn=<CopyBackwards>) tensor(0.0639, grad_fn=<DivBackward0>) tensor(0.8222, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.1246, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.1175, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.1246, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5831755 0.0 0.022553803 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.1246, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -19.094688 -0.35489893 18.223804 64.49822\n",
      "activ tensor(4.1246, grad_fn=<CopyBackwards>) tensor(0.0639, grad_fn=<DivBackward0>) tensor(0.8900, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.1253, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.1246, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.1253, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56303555 0.0 0.02151782 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.1251, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -19.022913 -0.36241168 18.25697 64.5897\n",
      "activ tensor(4.1251, grad_fn=<CopyBackwards>) tensor(0.0639, grad_fn=<DivBackward0>) tensor(0.7761, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.1265, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.1251, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.1265, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56095064 0.0 0.021390965 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.1264, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -18.798088 -0.36459112 19.317873 64.683876\n",
      "activ tensor(4.1264, grad_fn=<CopyBackwards>) tensor(0.0638, grad_fn=<DivBackward0>) tensor(0.8185, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.1371, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.1264, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.1371, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5708897 0.0 0.021887813 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.1371, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -19.148912 -0.3884899 18.95748 64.78373\n",
      "activ tensor(4.1371, grad_fn=<CopyBackwards>) tensor(0.0639, grad_fn=<DivBackward0>) tensor(0.9474, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.1420, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.1371, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.1420, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57974946 0.0 0.02251152 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.1420, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -19.494093 -0.34678912 18.784605 64.88714\n",
      "activ tensor(4.1420, grad_fn=<CopyBackwards>) tensor(0.0638, grad_fn=<DivBackward0>) tensor(0.9488, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.1518, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.1420, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.1518, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55570364 0.0 0.021158395 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.1518, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -19.678463 -0.35567585 18.878477 64.98809\n",
      "activ tensor(4.1518, grad_fn=<CopyBackwards>) tensor(0.0639, grad_fn=<DivBackward0>) tensor(0.9018, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.1543, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.1518, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.1543, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5777248 0.0 0.021898385 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.1543, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -19.698818 -0.39251262 18.755993 65.089775\n",
      "activ tensor(4.1543, grad_fn=<CopyBackwards>) tensor(0.0638, grad_fn=<DivBackward0>) tensor(0.8131, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.1602, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.1543, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.1602, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57538736 0.0 0.022130951 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.1603, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -19.60281 -0.37582165 19.258558 65.19516\n",
      "activ tensor(4.1603, grad_fn=<CopyBackwards>) tensor(0.0638, grad_fn=<DivBackward0>) tensor(0.8721, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.1602, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.1603, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.1602, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.54613197 0.0 0.020640403 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.1604, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -19.34264 -0.39804026 19.430256 65.295975\n",
      "activ tensor(4.1604, grad_fn=<CopyBackwards>) tensor(0.0637, grad_fn=<DivBackward0>) tensor(0.8133, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.1598, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.1604, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.1598, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57201725 0.0 0.022014668 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.1598, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -19.517998 -0.38962606 19.3035 65.40563\n",
      "activ tensor(4.1598, grad_fn=<CopyBackwards>) tensor(0.0636, grad_fn=<DivBackward0>) tensor(0.9953, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.1691, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.1598, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.1691, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57355165 0.0 0.021782102 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.1691, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -19.943718 -0.38887662 18.788029 65.51145\n",
      "activ tensor(4.1691, grad_fn=<CopyBackwards>) tensor(0.0636, grad_fn=<DivBackward0>) tensor(0.9314, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.1768, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.1691, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.1768, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5716073 0.0 0.021908958 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.1767, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -19.896587 -0.3788922 18.909391 65.616135\n",
      "activ tensor(4.1767, grad_fn=<CopyBackwards>) tensor(0.0637, grad_fn=<DivBackward0>) tensor(0.8712, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.1906, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.1767, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.1906, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5756928 0.0 0.022130953 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.1908, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -19.365664 -0.37395883 19.253845 65.71821\n",
      "activ tensor(4.1908, grad_fn=<CopyBackwards>) tensor(0.0638, grad_fn=<DivBackward0>) tensor(0.8358, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.1985, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.1908, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.1985, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5541197 0.0 0.021010397 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.1985, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -19.592276 -0.3585815 19.2291 65.8147\n",
      "activ tensor(4.1985, grad_fn=<CopyBackwards>) tensor(0.0638, grad_fn=<DivBackward0>) tensor(0.8140, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.1941, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.1985, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.1941, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55390817 0.0 0.021327537 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.1942, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -19.057276 -0.34981892 19.442019 65.9024\n",
      "activ tensor(4.1942, grad_fn=<CopyBackwards>) tensor(0.0636, grad_fn=<DivBackward0>) tensor(0.9251, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.1949, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.1942, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.1949, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5551761 0.0 0.02128525 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.1950, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -19.443228 -0.3546223 18.63088 65.98195\n",
      "activ tensor(4.1950, grad_fn=<CopyBackwards>) tensor(0.0636, grad_fn=<DivBackward0>) tensor(0.8533, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.2018, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.1950, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.2018, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5694508 0.0 0.021676386 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.2018, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -19.661901 -0.3593545 18.680021 66.05609\n",
      "activ tensor(4.2018, grad_fn=<CopyBackwards>) tensor(0.0636, grad_fn=<DivBackward0>) tensor(0.8665, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.2084, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.2018, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.2084, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.564179 0.0 0.021634102 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.2082, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -19.19048 -0.3540101 19.261276 66.13214\n",
      "activ tensor(4.2082, grad_fn=<CopyBackwards>) tensor(0.0636, grad_fn=<DivBackward0>) tensor(0.7900, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.2133, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.2082, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.2133, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5451656 0.0 0.020640403 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.2134, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -20.131136 -0.36383152 19.070402 66.21108\n",
      "activ tensor(4.2134, grad_fn=<CopyBackwards>) tensor(0.0636, grad_fn=<DivBackward0>) tensor(0.8230, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.2113, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.2134, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.2113, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5580182 0.0 0.021327537 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.2113, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -19.765625 -0.37233537 19.32059 66.29894\n",
      "activ tensor(4.2113, grad_fn=<CopyBackwards>) tensor(0.0635, grad_fn=<DivBackward0>) tensor(0.9652, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.2168, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.2113, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.2168, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5608461 0.0 0.02126411 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.2166, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -19.551098 -0.35570848 19.075197 66.39368\n",
      "activ tensor(4.2166, grad_fn=<CopyBackwards>) tensor(0.0635, grad_fn=<DivBackward0>) tensor(0.8710, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.2170, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.2166, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.2170, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5623067 0.0 0.02168696 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.2169, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -19.62987 -0.35276976 18.802185 66.49595\n",
      "activ tensor(4.2169, grad_fn=<CopyBackwards>) tensor(0.0634, grad_fn=<DivBackward0>) tensor(0.8362, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.2115, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.2169, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.2115, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5504064 0.0 0.020894114 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.2115, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -19.57038 -0.37733924 19.051237 66.60539\n",
      "activ tensor(4.2115, grad_fn=<CopyBackwards>) tensor(0.0632, grad_fn=<DivBackward0>) tensor(0.8869, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.2220, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.2115, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.2220, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5634517 0.0 0.02159182 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.2221, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -19.67015 -0.3949213 19.703306 66.7229\n",
      "activ tensor(4.2221, grad_fn=<CopyBackwards>) tensor(0.0633, grad_fn=<DivBackward0>) tensor(0.8574, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.2253, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.2221, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.2253, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57263154 0.0 0.02218381 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.2255, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -19.578974 -0.39417434 19.787415 66.840805\n",
      "activ tensor(4.2255, grad_fn=<CopyBackwards>) tensor(0.0632, grad_fn=<DivBackward0>) tensor(0.9651, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.2429, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.2255, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.2429, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5751836 0.0 0.02218381 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.2428, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -20.110683 -0.3845414 19.145464 66.96428\n",
      "activ tensor(4.2428, grad_fn=<CopyBackwards>) tensor(0.0634, grad_fn=<DivBackward0>) tensor(0.8899, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.2449, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.2428, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.2449, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55875266 0.0 0.021316964 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.2449, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -20.30336 -0.40674922 19.522934 67.08832\n",
      "activ tensor(4.2449, grad_fn=<CopyBackwards>) tensor(0.0633, grad_fn=<DivBackward0>) tensor(0.7775, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.2470, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.2449, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.2470, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5639713 0.0 0.021390963 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.2470, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -20.230728 -0.37896475 19.471718 67.20865\n",
      "activ tensor(4.2470, grad_fn=<CopyBackwards>) tensor(0.0632, grad_fn=<DivBackward0>) tensor(0.8553, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.2525, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.2470, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.2525, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56042784 0.0 0.021285253 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.2527, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -20.132275 -0.38881615 19.736992 67.31947\n",
      "activ tensor(4.2527, grad_fn=<CopyBackwards>) tensor(0.0632, grad_fn=<DivBackward0>) tensor(0.8982, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.2611, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.2527, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.2611, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5835771 0.0 0.022374094 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.2611, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -19.99068 -0.39018556 20.261621 67.4272\n",
      "activ tensor(4.2611, grad_fn=<CopyBackwards>) tensor(0.0632, grad_fn=<DivBackward0>) tensor(0.9157, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.2715, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.2611, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.2715, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.54934096 0.0 0.02076726 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.2714, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -20.208792 -0.38173535 19.636726 67.523254\n",
      "activ tensor(4.2714, grad_fn=<CopyBackwards>) tensor(0.0633, grad_fn=<DivBackward0>) tensor(0.9479, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.2734, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.2714, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.2734, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5493411 0.0 0.02094697 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.2735, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -20.586239 -0.37449303 19.53628 67.60923\n",
      "activ tensor(4.2735, grad_fn=<CopyBackwards>) tensor(0.0632, grad_fn=<DivBackward0>) tensor(0.8324, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.2768, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.2735, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.2768, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5544368 0.0 0.020925827 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.2769, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -20.50315 -0.36798015 19.553709 67.68441\n",
      "activ tensor(4.2769, grad_fn=<CopyBackwards>) tensor(0.0632, grad_fn=<DivBackward0>) tensor(0.9022, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.2774, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.2769, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.2774, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55749315 0.0 0.021412106 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.2773, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -19.974497 -0.39365494 19.79799 67.75478\n",
      "activ tensor(4.2773, grad_fn=<CopyBackwards>) tensor(0.0631, grad_fn=<DivBackward0>) tensor(0.9472, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.2791, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.2773, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.2791, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5754891 0.0 0.022130955 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.2792, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -20.043526 -0.3847869 20.078001 67.8275\n",
      "activ tensor(4.2792, grad_fn=<CopyBackwards>) tensor(0.0631, grad_fn=<DivBackward0>) tensor(0.9911, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.2801, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.2792, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.2801, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5562304 0.0 0.021084396 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.2800, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -19.72123 -0.39320147 19.77776 67.90791\n",
      "activ tensor(4.2800, grad_fn=<CopyBackwards>) tensor(0.0630, grad_fn=<DivBackward0>) tensor(0.9522, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.2856, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.2800, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.2856, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5495544 0.0 0.020820115 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.2856, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -20.158539 -0.37878627 19.426119 67.99845\n",
      "activ tensor(4.2856, grad_fn=<CopyBackwards>) tensor(0.0630, grad_fn=<DivBackward0>) tensor(0.8637, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.2886, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.2856, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.2886, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56053245 0.0 0.021560103 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.2887, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -19.984549 -0.3426229 19.204275 68.08931\n",
      "activ tensor(4.2887, grad_fn=<CopyBackwards>) tensor(0.0630, grad_fn=<DivBackward0>) tensor(0.7843, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.2940, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.2887, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.2940, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55969554 0.0 0.021390965 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.2941, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -19.79605 -0.38968366 19.919456 68.17904\n",
      "activ tensor(4.2941, grad_fn=<CopyBackwards>) tensor(0.0630, grad_fn=<DivBackward0>) tensor(0.9407, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.3016, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.2941, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.3016, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56459445 0.0 0.021634104 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.3017, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -20.672916 -0.3931017 19.84804 68.28073\n",
      "activ tensor(4.3017, grad_fn=<CopyBackwards>) tensor(0.0630, grad_fn=<DivBackward0>) tensor(1.0123, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.3080, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.3017, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.3080, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5583333 0.0 0.02121125 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.3082, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -20.432907 -0.39851564 19.648613 68.38229\n",
      "activ tensor(4.3082, grad_fn=<CopyBackwards>) tensor(0.0630, grad_fn=<DivBackward0>) tensor(0.8887, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.3085, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.3082, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.3085, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.54602456 0.0 0.020629833 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.3084, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -20.017239 -0.382052 19.876314 68.49533\n",
      "activ tensor(4.3084, grad_fn=<CopyBackwards>) tensor(0.0629, grad_fn=<DivBackward0>) tensor(0.8786, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.3091, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.3084, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.3091, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57406217 0.0 0.022035813 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.3092, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -20.22322 -0.38546994 19.246012 68.60981\n",
      "activ tensor(4.3092, grad_fn=<CopyBackwards>) tensor(0.0628, grad_fn=<DivBackward0>) tensor(0.9237, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.3187, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.3092, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.3187, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5463465 0.0 0.020693261 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.3186, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -20.154455 -0.3921085 19.75223 68.71622\n",
      "activ tensor(4.3186, grad_fn=<CopyBackwards>) tensor(0.0628, grad_fn=<DivBackward0>) tensor(0.9111, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.3243, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.3186, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.3243, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5479529 0.0 0.020968113 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.3242, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -20.34348 -0.38096845 20.39406 68.823524\n",
      "activ tensor(4.3242, grad_fn=<CopyBackwards>) tensor(0.0628, grad_fn=<DivBackward0>) tensor(0.8962, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.3311, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.3242, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.3311, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.58548146 0.0 0.022342378 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.3310, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -20.252205 -0.38425332 20.171007 68.93408\n",
      "activ tensor(4.3310, grad_fn=<CopyBackwards>) tensor(0.0628, grad_fn=<DivBackward0>) tensor(0.8007, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.3422, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.3310, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.3422, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.54816663 0.0 0.020862399 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.3422, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -20.747444 -0.37450957 19.944557 69.03907\n",
      "activ tensor(4.3422, grad_fn=<CopyBackwards>) tensor(0.0629, grad_fn=<DivBackward0>) tensor(0.8703, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.3470, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.3422, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.3470, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5535907 0.0 0.020841258 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.3471, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -20.895088 -0.37406334 19.87057 69.13922\n",
      "activ tensor(4.3471, grad_fn=<CopyBackwards>) tensor(0.0629, grad_fn=<DivBackward0>) tensor(0.8480, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.3579, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.3471, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.3579, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5721197 0.0 0.02186667 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.3580, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -20.771955 -0.3881006 20.273882 69.230194\n",
      "activ tensor(4.3580, grad_fn=<CopyBackwards>) tensor(0.0629, grad_fn=<DivBackward0>) tensor(1.0661, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.3583, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.3580, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.3583, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55854297 0.0 0.021338107 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.3582, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -20.660028 -0.39867017 20.366266 69.31957\n",
      "activ tensor(4.3582, grad_fn=<CopyBackwards>) tensor(0.0629, grad_fn=<DivBackward0>) tensor(0.8642, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.3675, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.3582, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.3675, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5664592 0.0 0.021602388 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.3674, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -20.63602 -0.4105258 20.737074 69.4145\n",
      "activ tensor(4.3674, grad_fn=<CopyBackwards>) tensor(0.0629, grad_fn=<DivBackward0>) tensor(0.8441, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.3778, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.3674, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.3778, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56532025 0.0 0.021538958 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.3778, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -20.908966 -0.40301484 20.399805 69.52139\n",
      "activ tensor(4.3778, grad_fn=<CopyBackwards>) tensor(0.0630, grad_fn=<DivBackward0>) tensor(0.9513, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.3813, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.3778, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.3813, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5602187 0.0 0.021486104 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.3814, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -21.212442 -0.39969203 19.905045 69.62715\n",
      "activ tensor(4.3814, grad_fn=<CopyBackwards>) tensor(0.0629, grad_fn=<DivBackward0>) tensor(0.9723, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.3836, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.3814, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.3836, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56272316 0.0 0.021729246 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.3836, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -21.096962 -0.39711428 20.424273 69.72985\n",
      "activ tensor(4.3836, grad_fn=<CopyBackwards>) tensor(0.0629, grad_fn=<DivBackward0>) tensor(1.0771, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.3920, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.3836, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.3920, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.54527307 0.0 0.02053469 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.3921, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -20.59575 -0.41544884 20.353691 69.83425\n",
      "activ tensor(4.3921, grad_fn=<CopyBackwards>) tensor(0.0629, grad_fn=<DivBackward0>) tensor(0.9136, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.3931, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.3921, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.3931, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55369663 0.0 0.021168964 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.3932, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -20.61511 -0.40570635 20.668268 69.93634\n",
      "activ tensor(4.3932, grad_fn=<CopyBackwards>) tensor(0.0628, grad_fn=<DivBackward0>) tensor(0.8351, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.3979, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.3932, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.3979, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5541197 0.0 0.021327537 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.3982, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -20.398045 -0.40994602 20.521605 70.03398\n",
      "activ tensor(4.3982, grad_fn=<CopyBackwards>) tensor(0.0628, grad_fn=<DivBackward0>) tensor(0.8445, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.4019, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.3982, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.4019, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5630355 0.0 0.021390965 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.4019, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -20.836727 -0.3938592 19.778818 70.134\n",
      "activ tensor(4.4019, grad_fn=<CopyBackwards>) tensor(0.0628, grad_fn=<DivBackward0>) tensor(0.8628, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.4056, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.4019, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.4056, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57171 0.0 0.021908954 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.4056, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -20.64191 -0.40761086 20.095825 70.22529\n",
      "activ tensor(4.4056, grad_fn=<CopyBackwards>) tensor(0.0627, grad_fn=<DivBackward0>) tensor(0.9953, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.4074, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.4056, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.4074, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56324357 0.0 0.021676388 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.4072, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -20.623905 -0.4149276 20.411451 70.31344\n",
      "activ tensor(4.4072, grad_fn=<CopyBackwards>) tensor(0.0627, grad_fn=<DivBackward0>) tensor(0.9237, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.4088, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.4072, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.4088, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.559591 0.0 0.021380393 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.4087, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -21.25527 -0.37380868 20.543402 70.40225\n",
      "activ tensor(4.4087, grad_fn=<CopyBackwards>) tensor(0.0626, grad_fn=<DivBackward0>) tensor(0.8920, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.4035, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.4087, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.4035, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55485934 0.0 0.021094969 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.4035, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -20.885637 -0.3633547 20.266922 70.49145\n",
      "activ tensor(4.4035, grad_fn=<CopyBackwards>) tensor(0.0625, grad_fn=<DivBackward0>) tensor(0.8011, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.4041, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.4035, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.4041, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57150495 0.0 0.021782102 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.4041, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -20.807198 -0.3747134 20.26264 70.57192\n",
      "activ tensor(4.4041, grad_fn=<CopyBackwards>) tensor(0.0624, grad_fn=<DivBackward0>) tensor(1.0239, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.4121, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.4041, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.4121, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5695537 0.0 0.022067524 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.4121, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -20.974365 -0.38699055 20.11555 70.651726\n",
      "activ tensor(4.4121, grad_fn=<CopyBackwards>) tensor(0.0624, grad_fn=<DivBackward0>) tensor(0.9965, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.4109, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.4121, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.4109, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57731885 0.0 0.022257807 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.4109, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -20.647709 -0.3901763 20.170712 70.7329\n",
      "activ tensor(4.4109, grad_fn=<CopyBackwards>) tensor(0.0624, grad_fn=<DivBackward0>) tensor(0.8671, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.4102, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.4109, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.4102, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56718284 0.0 0.02169753 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.4100, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -20.939804 -0.39773187 21.143225 70.82032\n",
      "activ tensor(4.4100, grad_fn=<CopyBackwards>) tensor(0.0623, grad_fn=<DivBackward0>) tensor(0.8970, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.4122, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.4100, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.4122, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5637637 0.0 0.021369822 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.4123, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -20.815874 -0.41065574 20.722027 70.91974\n",
      "activ tensor(4.4123, grad_fn=<CopyBackwards>) tensor(0.0622, grad_fn=<DivBackward0>) tensor(0.8499, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.4202, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.4123, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.4202, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.54602456 0.0 0.020957544 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.4202, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -21.351427 -0.38428432 20.381193 71.02988\n",
      "activ tensor(4.4202, grad_fn=<CopyBackwards>) tensor(0.0622, grad_fn=<DivBackward0>) tensor(1.0144, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.4250, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.4202, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.4250, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55485946 0.0 0.021105539 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.4249, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -21.503048 -0.4163219 20.678354 71.142685\n",
      "activ tensor(4.4249, grad_fn=<CopyBackwards>) tensor(0.0622, grad_fn=<DivBackward0>) tensor(1.0245, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.4231, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.4249, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.4231, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5619939 0.0 0.021718673 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.4230, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -21.285227 -0.41126114 20.671059 71.25993\n",
      "activ tensor(4.4230, grad_fn=<CopyBackwards>) tensor(0.0621, grad_fn=<DivBackward0>) tensor(0.8187, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.4217, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.4230, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.4217, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55210716 0.0 0.021126684 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.4217, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -21.209833 -0.4023523 21.228058 71.38148\n",
      "activ tensor(4.4217, grad_fn=<CopyBackwards>) tensor(0.0619, grad_fn=<DivBackward0>) tensor(0.9341, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.4240, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.4217, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.4240, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5776231 0.0 0.022268377 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.4242, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -21.273504 -0.41857716 21.200678 71.50489\n",
      "activ tensor(4.4242, grad_fn=<CopyBackwards>) tensor(0.0619, grad_fn=<DivBackward0>) tensor(0.8736, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.4305, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.4242, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.4305, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.58608174 0.0 0.02259609 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.4308, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -21.607372 -0.42223096 20.933195 71.61919\n",
      "activ tensor(4.4308, grad_fn=<CopyBackwards>) tensor(0.0619, grad_fn=<DivBackward0>) tensor(0.9900, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.4342, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.4308, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.4342, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5762016 0.0 0.022120379 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.4340, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -21.836098 -0.42452222 20.66952 71.73305\n",
      "activ tensor(4.4340, grad_fn=<CopyBackwards>) tensor(0.0618, grad_fn=<DivBackward0>) tensor(0.9519, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.4411, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.4340, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.4411, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55875283 0.0 0.021327537 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.4411, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -21.670557 -0.42267272 20.868605 71.84159\n",
      "activ tensor(4.4411, grad_fn=<CopyBackwards>) tensor(0.0618, grad_fn=<DivBackward0>) tensor(0.9155, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.4461, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.4411, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.4461, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.575591 0.0 0.022247238 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.4459, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -21.224955 -0.43517363 21.246641 71.9423\n",
      "activ tensor(4.4459, grad_fn=<CopyBackwards>) tensor(0.0618, grad_fn=<DivBackward0>) tensor(0.8486, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.4550, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.4459, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.4550, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56365937 0.0 0.021634104 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.4551, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -21.27518 -0.40741664 21.101517 72.04222\n",
      "activ tensor(4.4551, grad_fn=<CopyBackwards>) tensor(0.0618, grad_fn=<DivBackward0>) tensor(0.9602, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.4697, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.4551, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.4697, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55906713 0.0 0.02135925 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.4697, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -21.082664 -0.36962855 21.169298 72.13834\n",
      "activ tensor(4.4697, grad_fn=<CopyBackwards>) tensor(0.0620, grad_fn=<DivBackward0>) tensor(1.0135, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.4810, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.4697, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.4810, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55221313 0.0 0.021084396 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.4811, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -21.52663 -0.41569436 20.453066 72.21872\n",
      "activ tensor(4.4811, grad_fn=<CopyBackwards>) tensor(0.0620, grad_fn=<DivBackward0>) tensor(1.0125, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.4820, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.4811, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.4820, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5673893 0.0 0.021686958 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.4821, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -21.391722 -0.40088212 20.555502 72.295135\n",
      "activ tensor(4.4821, grad_fn=<CopyBackwards>) tensor(0.0620, grad_fn=<DivBackward0>) tensor(0.8614, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.4867, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.4821, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.4867, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5616812 0.0 0.021137252 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.4869, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -21.184689 -0.39317477 21.336176 72.36746\n",
      "activ tensor(4.4869, grad_fn=<CopyBackwards>) tensor(0.0620, grad_fn=<DivBackward0>) tensor(0.8727, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.4943, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.4869, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.4943, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55496514 0.0 0.02110554 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.4943, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -21.990314 -0.41001484 20.92764 72.44365\n",
      "activ tensor(4.4943, grad_fn=<CopyBackwards>) tensor(0.0620, grad_fn=<DivBackward0>) tensor(0.8833, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.4955, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.4943, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.4955, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5596957 0.0 0.021190109 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.4955, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -21.287971 -0.4132796 20.970888 72.52578\n",
      "activ tensor(4.4955, grad_fn=<CopyBackwards>) tensor(0.0620, grad_fn=<DivBackward0>) tensor(1.0463, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.5034, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.4955, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.5034, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5540141 0.0 0.021200681 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.5034, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -21.48302 -0.41208008 20.808575 72.61755\n",
      "activ tensor(4.5034, grad_fn=<CopyBackwards>) tensor(0.0620, grad_fn=<DivBackward0>) tensor(1.0151, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.5109, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.5034, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.5109, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5524254 0.0 0.0208624 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.5109, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -21.53959 -0.39545307 20.64696 72.71116\n",
      "activ tensor(4.5109, grad_fn=<CopyBackwards>) tensor(0.0620, grad_fn=<DivBackward0>) tensor(0.8653, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.5124, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.5109, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.5124, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56521666 0.0 0.021655245 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.5125, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -21.252487 -0.40726325 21.024149 72.802605\n",
      "activ tensor(4.5125, grad_fn=<CopyBackwards>) tensor(0.0620, grad_fn=<DivBackward0>) tensor(0.8185, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.5176, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.5125, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.5176, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56168115 0.0 0.02135925 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.5177, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -21.77434 -0.41135392 21.497555 72.90408\n",
      "activ tensor(4.5177, grad_fn=<CopyBackwards>) tensor(0.0620, grad_fn=<DivBackward0>) tensor(0.9270, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.5229, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.5177, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.5229, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5610547 0.0 0.021454392 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.5229, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -21.410414 -0.39770705 21.487482 73.010086\n",
      "activ tensor(4.5229, grad_fn=<CopyBackwards>) tensor(0.0619, grad_fn=<DivBackward0>) tensor(0.9829, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.5342, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.5229, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.5342, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.58297443 0.0 0.022458661 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.5342, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -21.956142 -0.43524557 20.83686 73.11054\n",
      "activ tensor(4.5342, grad_fn=<CopyBackwards>) tensor(0.0620, grad_fn=<DivBackward0>) tensor(0.8787, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.5380, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.5342, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.5380, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55612504 0.0 0.021190109 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.5379, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -22.079298 -0.42315286 21.24786 73.21805\n",
      "activ tensor(4.5379, grad_fn=<CopyBackwards>) tensor(0.0620, grad_fn=<DivBackward0>) tensor(0.9032, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.5488, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.5379, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.5488, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.58648145 0.0 0.022712372 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.5488, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -21.824562 -0.42116773 21.475721 73.325966\n",
      "activ tensor(4.5488, grad_fn=<CopyBackwards>) tensor(0.0620, grad_fn=<DivBackward0>) tensor(1.0239, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.5447, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.5488, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.5447, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5526375 0.0 0.02105268 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.5446, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -21.736126 -0.42397398 21.630445 73.43347\n",
      "activ tensor(4.5446, grad_fn=<CopyBackwards>) tensor(0.0619, grad_fn=<DivBackward0>) tensor(0.9848, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.5478, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.5446, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.5478, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55168235 0.0 0.020999826 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.5479, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -21.935848 -0.4110176 22.020163 73.54715\n",
      "activ tensor(4.5479, grad_fn=<CopyBackwards>) tensor(0.0618, grad_fn=<DivBackward0>) tensor(1.1254, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.5539, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.5479, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.5539, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56095046 0.0 0.021708101 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.5537, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -22.285969 -0.39868027 21.299665 73.66725\n",
      "activ tensor(4.5537, grad_fn=<CopyBackwards>) tensor(0.0618, grad_fn=<DivBackward0>) tensor(0.8682, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.5545, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.5537, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.5545, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5719147 0.0 0.021982957 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.5544, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -22.456465 -0.41228113 21.364529 73.77354\n",
      "activ tensor(4.5544, grad_fn=<CopyBackwards>) tensor(0.0617, grad_fn=<DivBackward0>) tensor(0.9329, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.5656, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.5544, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.5656, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5786366 0.0 0.022236666 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.5658, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -22.237095 -0.44420168 21.609327 73.87917\n",
      "activ tensor(4.5658, grad_fn=<CopyBackwards>) tensor(0.0618, grad_fn=<DivBackward0>) tensor(0.9164, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.5706, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.5658, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.5706, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5692451 0.0 0.021792673 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.5709, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -21.849222 -0.42971712 21.715687 73.986374\n",
      "activ tensor(4.5709, grad_fn=<CopyBackwards>) tensor(0.0618, grad_fn=<DivBackward0>) tensor(0.9820, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.5739, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.5709, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.5739, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5768112 0.0 0.021940669 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.5739, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -21.638815 -0.41563922 21.941319 74.09196\n",
      "activ tensor(4.5739, grad_fn=<CopyBackwards>) tensor(0.0617, grad_fn=<DivBackward0>) tensor(1.0892, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.5822, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.5739, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.5822, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57569283 0.0 0.022226095 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.5822, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -21.764366 -0.446789 21.530058 74.19856\n",
      "activ tensor(4.5822, grad_fn=<CopyBackwards>) tensor(0.0618, grad_fn=<DivBackward0>) tensor(0.8701, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.5900, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.5822, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.5900, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57314295 0.0 0.022078097 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.5899, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -22.198511 -0.43129906 21.206451 74.303474\n",
      "activ tensor(4.5899, grad_fn=<CopyBackwards>) tensor(0.0618, grad_fn=<DivBackward0>) tensor(1.0176, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.5921, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.5899, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.5921, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5636596 0.0 0.021496676 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.5921, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -21.940577 -0.42406836 21.19333 74.397964\n",
      "activ tensor(4.5921, grad_fn=<CopyBackwards>) tensor(0.0617, grad_fn=<DivBackward0>) tensor(0.9497, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.5905, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.5921, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.5905, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5665626 0.0 0.021538962 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.5906, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -21.948639 -0.4192085 21.890722 74.48452\n",
      "activ tensor(4.5906, grad_fn=<CopyBackwards>) tensor(0.0616, grad_fn=<DivBackward0>) tensor(0.9521, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.5994, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.5906, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.5994, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5651128 0.0 0.021676388 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.5995, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -22.432539 -0.41078293 21.758436 74.572\n",
      "activ tensor(4.5995, grad_fn=<CopyBackwards>) tensor(0.0617, grad_fn=<DivBackward0>) tensor(0.9892, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.6019, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.5995, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.6019, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56986207 0.0 0.021824386 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.6020, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -21.922142 -0.40303382 21.268957 74.6524\n",
      "activ tensor(4.6020, grad_fn=<CopyBackwards>) tensor(0.0616, grad_fn=<DivBackward0>) tensor(0.8744, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.6044, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.6020, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.6044, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56738913 0.0 0.02162353 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.6045, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -22.14872 -0.41589424 21.601662 74.719604\n",
      "activ tensor(4.6045, grad_fn=<CopyBackwards>) tensor(0.0616, grad_fn=<DivBackward0>) tensor(0.8871, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.6132, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.6045, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.6132, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5650093 0.0 0.021464963 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.6131, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -21.95509 -0.4339959 21.179428 74.792206\n",
      "activ tensor(4.6131, grad_fn=<CopyBackwards>) tensor(0.0617, grad_fn=<DivBackward0>) tensor(0.9389, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.6179, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.6131, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.6179, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55665165 0.0 0.021264106 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.6180, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -22.03197 -0.42787382 21.652168 74.86627\n",
      "activ tensor(4.6180, grad_fn=<CopyBackwards>) tensor(0.0617, grad_fn=<DivBackward0>) tensor(1.1158, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.6283, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.6180, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.6283, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5651129 0.0 0.021708103 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.6281, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -22.32649 -0.410996 22.23228 74.95006\n",
      "activ tensor(4.6281, grad_fn=<CopyBackwards>) tensor(0.0617, grad_fn=<DivBackward0>) tensor(0.9603, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.6355, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.6281, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.6355, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5743683 0.0 0.021993525 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.6355, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -21.986372 -0.41230872 21.815165 75.045296\n",
      "activ tensor(4.6355, grad_fn=<CopyBackwards>) tensor(0.0618, grad_fn=<DivBackward0>) tensor(0.9641, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.6392, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.6355, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.6392, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.565113 0.0 0.02135925 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.6392, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -22.556253 -0.39799127 21.642454 75.1465\n",
      "activ tensor(4.6392, grad_fn=<CopyBackwards>) tensor(0.0617, grad_fn=<DivBackward0>) tensor(0.9539, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.6440, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.6392, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.6440, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5775219 0.0 0.021813815 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.6440, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -22.646366 -0.4356869 21.67712 75.24718\n",
      "activ tensor(4.6440, grad_fn=<CopyBackwards>) tensor(0.0617, grad_fn=<DivBackward0>) tensor(1.0255, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.6424, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.6440, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.6424, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5720173 0.0 0.021961812 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.6426, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -22.352104 -0.44932517 22.1844 75.35954\n",
      "activ tensor(4.6426, grad_fn=<CopyBackwards>) tensor(0.0616, grad_fn=<DivBackward0>) tensor(1.1228, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.6523, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.6426, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.6523, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55948615 0.0 0.021549532 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.6524, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -22.29068 -0.43347028 22.33446 75.47943\n",
      "activ tensor(4.6524, grad_fn=<CopyBackwards>) tensor(0.0616, grad_fn=<DivBackward0>) tensor(0.9948, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.6628, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.6524, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.6628, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57762337 0.0 0.022215523 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.6627, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -22.570122 -0.4364774 22.397236 75.60354\n",
      "activ tensor(4.6627, grad_fn=<CopyBackwards>) tensor(0.0617, grad_fn=<DivBackward0>) tensor(0.9965, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.6640, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.6627, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.6640, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5796484 0.0 0.02253266 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.6639, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -22.9863 -0.43829036 22.103848 75.72967\n",
      "activ tensor(4.6639, grad_fn=<CopyBackwards>) tensor(0.0616, grad_fn=<DivBackward0>) tensor(0.9527, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.6694, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.6639, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.6694, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57873785 0.0 0.022310663 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.6695, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -23.045694 -0.4565812 21.757788 75.85219\n",
      "activ tensor(4.6695, grad_fn=<CopyBackwards>) tensor(0.0616, grad_fn=<DivBackward0>) tensor(0.9846, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.6820, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.6695, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.6820, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5613683 0.0 0.021549534 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.6820, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -22.808306 -0.43997082 22.442242 75.96988\n",
      "activ tensor(4.6820, grad_fn=<CopyBackwards>) tensor(0.0616, grad_fn=<DivBackward0>) tensor(1.0769, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.6886, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.6820, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.6886, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.560428 0.0 0.02135925 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.6884, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -22.474066 -0.4172075 22.331059 76.083595\n",
      "activ tensor(4.6884, grad_fn=<CopyBackwards>) tensor(0.0616, grad_fn=<DivBackward0>) tensor(0.9264, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.7019, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.6884, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.7019, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5580182 0.0 0.02103154 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.7020, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -22.223236 -0.4366325 22.425795 76.19006\n",
      "activ tensor(4.7020, grad_fn=<CopyBackwards>) tensor(0.0617, grad_fn=<DivBackward0>) tensor(0.9262, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.7064, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.7020, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.7064, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.58065814 0.0 0.02253266 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.7064, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -22.453571 -0.42094654 22.324707 76.28568\n",
      "activ tensor(4.7064, grad_fn=<CopyBackwards>) tensor(0.0617, grad_fn=<DivBackward0>) tensor(0.9335, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.7063, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.7064, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.7063, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5681117 0.0 0.021697532 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.7066, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -22.851076 -0.4438454 21.58329 76.367455\n",
      "activ tensor(4.7066, grad_fn=<CopyBackwards>) tensor(0.0616, grad_fn=<DivBackward0>) tensor(1.0570, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.7002, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.7066, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.7002, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5629315 0.0 0.021401532 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.7001, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -22.375942 -0.43234947 22.058266 76.44905\n",
      "activ tensor(4.7001, grad_fn=<CopyBackwards>) tensor(0.0615, grad_fn=<DivBackward0>) tensor(1.0879, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.7031, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.7001, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.7031, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56811154 0.0 0.021739814 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.7031, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -22.707048 -0.4359495 22.45078 76.53036\n",
      "activ tensor(4.7031, grad_fn=<CopyBackwards>) tensor(0.0615, grad_fn=<DivBackward0>) tensor(1.0352, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.7034, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.7031, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.7034, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55717754 0.0 0.021412104 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.7032, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -22.99011 -0.42873847 22.338917 76.613625\n",
      "activ tensor(4.7032, grad_fn=<CopyBackwards>) tensor(0.0614, grad_fn=<DivBackward0>) tensor(1.0559, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.7041, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.7032, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.7041, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56914216 0.0 0.021771532 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.7041, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -22.509317 -0.41283095 21.976505 76.69657\n",
      "activ tensor(4.7041, grad_fn=<CopyBackwards>) tensor(0.0613, grad_fn=<DivBackward0>) tensor(0.9660, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.7078, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.7041, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.7078, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56852394 0.0 0.022014668 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.7079, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -22.785189 -0.43875188 21.949938 76.78399\n",
      "activ tensor(4.7079, grad_fn=<CopyBackwards>) tensor(0.0613, grad_fn=<DivBackward0>) tensor(1.0432, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.7091, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.7079, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.7091, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5604279 0.0 0.021380391 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.7092, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -22.560143 -0.4266123 22.081934 76.86976\n",
      "activ tensor(4.7092, grad_fn=<CopyBackwards>) tensor(0.0613, grad_fn=<DivBackward0>) tensor(1.0939, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.7138, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.7092, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.7138, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5871804 0.0 0.022744088 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.7139, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -22.891617 -0.43697128 22.111143 76.962425\n",
      "activ tensor(4.7139, grad_fn=<CopyBackwards>) tensor(0.0612, grad_fn=<DivBackward0>) tensor(0.9338, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.7159, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.7139, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.7159, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5535907 0.0 0.021147825 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.7160, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -22.918278 -0.45252436 22.885508 77.05819\n",
      "activ tensor(4.7160, grad_fn=<CopyBackwards>) tensor(0.0612, grad_fn=<DivBackward0>) tensor(0.9634, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.7164, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.7160, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.7164, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5675957 0.0 0.021919528 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.7165, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -22.43091 -0.44328892 22.428915 77.15574\n",
      "activ tensor(4.7165, grad_fn=<CopyBackwards>) tensor(0.0611, grad_fn=<DivBackward0>) tensor(0.9655, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.7173, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.7165, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.7173, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56105494 0.0 0.021242965 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.7173, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -23.138197 -0.45717812 22.037767 77.25665\n",
      "activ tensor(4.7173, grad_fn=<CopyBackwards>) tensor(0.0611, grad_fn=<DivBackward0>) tensor(1.0485, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.7274, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.7173, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.7274, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5520009 0.0 0.0209364 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.7273, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -23.20958 -0.44980782 22.531675 77.35958\n",
      "activ tensor(4.7273, grad_fn=<CopyBackwards>) tensor(0.0611, grad_fn=<DivBackward0>) tensor(1.0033, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.7299, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.7273, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.7299, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.573858 0.0 0.0220041 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.7299, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -22.879705 -0.4462257 22.600727 77.46655\n",
      "activ tensor(4.7299, grad_fn=<CopyBackwards>) tensor(0.0611, grad_fn=<DivBackward0>) tensor(0.9667, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.7343, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.7299, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.7343, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5582282 0.0 0.021464963 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.7343, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -22.846329 -0.40990478 23.099863 77.57091\n",
      "activ tensor(4.7343, grad_fn=<CopyBackwards>) tensor(0.0610, grad_fn=<DivBackward0>) tensor(0.9181, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.7369, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.7343, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.7369, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5742662 0.0 0.022257809 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.7370, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -23.219002 -0.44476 22.941307 77.67267\n",
      "activ tensor(4.7370, grad_fn=<CopyBackwards>) tensor(0.0610, grad_fn=<DivBackward0>) tensor(0.9363, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.7417, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.7370, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.7417, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55770314 0.0 0.02095754 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.7418, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -23.654505 -0.44658643 22.546507 77.7733\n",
      "activ tensor(4.7418, grad_fn=<CopyBackwards>) tensor(0.0610, grad_fn=<DivBackward0>) tensor(1.1018, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.7499, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.7418, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.7499, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.58257234 0.0 0.022278951 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.7500, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -23.654423 -0.4192213 22.608072 77.86612\n",
      "activ tensor(4.7500, grad_fn=<CopyBackwards>) tensor(0.0610, grad_fn=<DivBackward0>) tensor(1.0838, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.7565, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.7500, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.7565, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56769896 0.0 0.021634104 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.7564, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -23.346886 -0.44195378 22.86963 77.96789\n",
      "activ tensor(4.7564, grad_fn=<CopyBackwards>) tensor(0.0610, grad_fn=<DivBackward0>) tensor(0.9398, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.7613, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.7564, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.7613, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5626192 0.0 0.021390965 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.7614, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -23.1218 -0.44916913 23.182459 78.071754\n",
      "activ tensor(4.7614, grad_fn=<CopyBackwards>) tensor(0.0610, grad_fn=<DivBackward0>) tensor(0.9917, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.7628, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.7614, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.7628, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5626191 0.0 0.021507248 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.7627, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -22.91613 -0.45250353 22.892633 78.18073\n",
      "activ tensor(4.7627, grad_fn=<CopyBackwards>) tensor(0.0609, grad_fn=<DivBackward0>) tensor(0.9969, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.7694, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.7627, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.7694, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57711595 0.0 0.022078097 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.7694, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -23.14034 -0.44084167 22.89681 78.29313\n",
      "activ tensor(4.7694, grad_fn=<CopyBackwards>) tensor(0.0609, grad_fn=<DivBackward0>) tensor(1.0917, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.7733, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.7694, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.7733, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.54859376 0.0 0.020862399 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.7733, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -23.49768 -0.44622853 22.358812 78.406876\n",
      "activ tensor(4.7733, grad_fn=<CopyBackwards>) tensor(0.0609, grad_fn=<DivBackward0>) tensor(1.0824, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.7769, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.7733, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.7769, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5552815 0.0 0.021359248 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.7769, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -22.987469 -0.45624554 22.494219 78.51905\n",
      "activ tensor(4.7769, grad_fn=<CopyBackwards>) tensor(0.0608, grad_fn=<DivBackward0>) tensor(0.9878, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.7860, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.7769, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.7860, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.54548794 0.0 0.020428978 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.7861, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -23.216293 -0.44957978 23.348286 78.63141\n",
      "activ tensor(4.7861, grad_fn=<CopyBackwards>) tensor(0.0609, grad_fn=<DivBackward0>) tensor(0.9059, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.7889, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.7861, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.7889, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.555809 0.0 0.02127468 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.7892, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -23.636463 -0.48095787 22.763594 78.73979\n",
      "activ tensor(4.7892, grad_fn=<CopyBackwards>) tensor(0.0608, grad_fn=<DivBackward0>) tensor(1.0032, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.7952, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.7892, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.7952, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56293124 0.0 0.021739814 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.7952, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -22.93553 -0.45724803 22.595762 78.84127\n",
      "activ tensor(4.7952, grad_fn=<CopyBackwards>) tensor(0.0608, grad_fn=<DivBackward0>) tensor(1.0500, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.8000, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.7952, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.8000, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5709922 0.0 0.021982955 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.8000, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -23.342163 -0.4514746 22.614729 78.93606\n",
      "activ tensor(4.8000, grad_fn=<CopyBackwards>) tensor(0.0608, grad_fn=<DivBackward0>) tensor(0.9564, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.8005, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.8000, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.8005, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56500936 0.0 0.02152839 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.8004, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -23.333609 -0.4498701 22.553213 79.025444\n",
      "activ tensor(4.8004, grad_fn=<CopyBackwards>) tensor(0.0607, grad_fn=<DivBackward0>) tensor(1.0640, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.8016, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.8004, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.8016, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5734495 0.0 0.02194067 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.8015, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -23.478401 -0.4464351 23.005468 79.10621\n",
      "activ tensor(4.8015, grad_fn=<CopyBackwards>) tensor(0.0607, grad_fn=<DivBackward0>) tensor(0.9872, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.8095, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.8015, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.8095, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5542255 0.0 0.021211253 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.8095, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -23.66665 -0.4298699 23.222216 79.18409\n",
      "activ tensor(4.8095, grad_fn=<CopyBackwards>) tensor(0.0607, grad_fn=<DivBackward0>) tensor(1.0501, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.8079, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.8095, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.8079, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5879781 0.0 0.022405803 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.8079, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -23.136963 -0.42806017 23.130854 79.25791\n",
      "activ tensor(4.8079, grad_fn=<CopyBackwards>) tensor(0.0607, grad_fn=<DivBackward0>) tensor(1.1864, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.8040, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.8079, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.8040, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55104494 0.0 0.021042112 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.8041, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -23.707085 -0.44187292 22.59189 79.32473\n",
      "activ tensor(4.8041, grad_fn=<CopyBackwards>) tensor(0.0606, grad_fn=<DivBackward0>) tensor(1.0064, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.8131, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.8041, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.8131, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5777248 0.0 0.022300093 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.8131, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -23.756245 -0.43066216 23.049341 79.38923\n",
      "activ tensor(4.8131, grad_fn=<CopyBackwards>) tensor(0.0606, grad_fn=<DivBackward0>) tensor(1.0264, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.8249, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.8131, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.8249, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5782316 0.0 0.022511518 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.8248, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -23.420258 -0.4433359 23.444868 79.466774\n",
      "activ tensor(4.8248, grad_fn=<CopyBackwards>) tensor(0.0607, grad_fn=<DivBackward0>) tensor(0.9837, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.8352, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.8248, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.8352, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.596977 0.0 0.023103511 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.8353, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -23.40352 -0.4717255 23.467222 79.55411\n",
      "activ tensor(4.8353, grad_fn=<CopyBackwards>) tensor(0.0608, grad_fn=<DivBackward0>) tensor(0.9991, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.8370, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.8353, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.8370, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5787382 0.0 0.022289522 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.8369, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -23.865051 -0.46664172 23.706991 79.65628\n",
      "activ tensor(4.8369, grad_fn=<CopyBackwards>) tensor(0.0607, grad_fn=<DivBackward0>) tensor(1.1933, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.8445, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.8369, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.8445, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5665625 0.0 0.021676388 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.8446, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -24.322784 -0.45060828 23.002884 79.77196\n",
      "activ tensor(4.8446, grad_fn=<CopyBackwards>) tensor(0.0607, grad_fn=<DivBackward0>) tensor(1.0066, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.8487, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.8446, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.8487, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5617854 0.0 0.021560103 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.8486, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -24.23582 -0.45005864 23.219877 79.8951\n",
      "activ tensor(4.8486, grad_fn=<CopyBackwards>) tensor(0.0607, grad_fn=<DivBackward0>) tensor(0.9153, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.8551, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.8486, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.8551, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5688332 0.0 0.021750387 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.8551, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -23.907242 -0.47797978 23.703571 80.019936\n",
      "activ tensor(4.8551, grad_fn=<CopyBackwards>) tensor(0.0607, grad_fn=<DivBackward0>) tensor(0.9464, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.8598, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.8551, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.8598, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5393308 0.0 0.020481836 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.8600, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -23.744926 -0.46352357 23.574774 80.15541\n",
      "activ tensor(4.8600, grad_fn=<CopyBackwards>) tensor(0.0606, grad_fn=<DivBackward0>) tensor(1.0348, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.8652, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.8600, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.8652, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5669759 0.0 0.021686962 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.8654, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -23.641275 -0.4749031 23.716318 80.28643\n",
      "activ tensor(4.8654, grad_fn=<CopyBackwards>) tensor(0.0606, grad_fn=<DivBackward0>) tensor(1.0744, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.8681, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.8654, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.8681, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5584382 0.0 0.021306392 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.8680, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -23.807428 -0.48018634 23.301088 80.41147\n",
      "activ tensor(4.8680, grad_fn=<CopyBackwards>) tensor(0.0605, grad_fn=<DivBackward0>) tensor(0.9726, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.8712, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.8680, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.8712, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.562515 0.0 0.021422679 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.8711, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -24.137634 -0.46735603 23.073484 80.53376\n",
      "activ tensor(4.8711, grad_fn=<CopyBackwards>) tensor(0.0605, grad_fn=<DivBackward0>) tensor(0.9415, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.8740, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.8711, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.8740, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57293856 0.0 0.02194067 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.8740, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -23.644396 -0.46035403 23.221577 80.64439\n",
      "activ tensor(4.8740, grad_fn=<CopyBackwards>) tensor(0.0604, grad_fn=<DivBackward0>) tensor(1.1138, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.8782, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.8740, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.8782, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5481666 0.0 0.020883542 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.8782, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -23.985533 -0.46090356 23.820507 80.746414\n",
      "activ tensor(4.8782, grad_fn=<CopyBackwards>) tensor(0.0604, grad_fn=<DivBackward0>) tensor(1.1779, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.8820, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.8782, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.8820, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56293136 0.0 0.021327537 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.8819, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -23.984253 -0.43204492 23.600954 80.83981\n",
      "activ tensor(4.8819, grad_fn=<CopyBackwards>) tensor(0.0604, grad_fn=<DivBackward0>) tensor(1.1238, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.8831, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.8819, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.8831, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5779276 0.0 0.02210981 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.8831, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -23.694792 -0.43376625 22.929552 80.920364\n",
      "activ tensor(4.8831, grad_fn=<CopyBackwards>) tensor(0.0603, grad_fn=<DivBackward0>) tensor(0.9360, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.8848, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.8831, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.8848, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5559142 0.0 0.021338109 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.8847, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -23.987375 -0.46024692 23.360516 80.99278\n",
      "activ tensor(4.8847, grad_fn=<CopyBackwards>) tensor(0.0603, grad_fn=<DivBackward0>) tensor(1.0026, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.8880, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.8847, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.8880, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5383522 0.0 0.020238694 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.8882, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -23.90593 -0.44824296 23.197716 81.05782\n",
      "activ tensor(4.8882, grad_fn=<CopyBackwards>) tensor(0.0603, grad_fn=<DivBackward0>) tensor(0.9213, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.8837, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.8882, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.8837, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57518375 0.0 0.021919524 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.8837, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -24.261808 -0.4438019 23.533953 81.1293\n",
      "activ tensor(4.8837, grad_fn=<CopyBackwards>) tensor(0.0602, grad_fn=<DivBackward0>) tensor(1.1955, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.8881, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.8837, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.8881, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.54570276 0.0 0.02079897 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.8881, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -24.124794 -0.46944103 24.003706 81.203384\n",
      "activ tensor(4.8881, grad_fn=<CopyBackwards>) tensor(0.0602, grad_fn=<DivBackward0>) tensor(1.0601, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.8942, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.8881, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.8942, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5720173 0.0 0.022046383 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.8941, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -23.948225 -0.47677386 23.423311 81.28612\n",
      "activ tensor(4.8941, grad_fn=<CopyBackwards>) tensor(0.0602, grad_fn=<DivBackward0>) tensor(0.9520, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.9000, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.8941, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.9000, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5736537 0.0 0.022130951 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.9000, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -24.285534 -0.46027136 23.39911 81.37656\n",
      "activ tensor(4.9000, grad_fn=<CopyBackwards>) tensor(0.0602, grad_fn=<DivBackward0>) tensor(1.0071, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.9081, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.9000, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.9081, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55738795 0.0 0.021401532 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.9080, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -24.282602 -0.46212035 23.563093 81.47468\n",
      "activ tensor(4.9080, grad_fn=<CopyBackwards>) tensor(0.0602, grad_fn=<DivBackward0>) tensor(0.9768, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.9117, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.9080, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.9117, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5819687 0.0 0.022363521 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.9118, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -23.966154 -0.48681602 24.060495 81.57685\n",
      "activ tensor(4.9118, grad_fn=<CopyBackwards>) tensor(0.0602, grad_fn=<DivBackward0>) tensor(1.1234, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.9140, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.9118, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.9140, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5551759 0.0 0.021126682 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.9141, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -23.976822 -0.45424438 24.21147 81.68318\n",
      "activ tensor(4.9141, grad_fn=<CopyBackwards>) tensor(0.0602, grad_fn=<DivBackward0>) tensor(1.0128, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.9170, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.9141, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.9170, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5765066 0.0 0.021951241 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.9172, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -24.498257 -0.48763674 24.038439 81.788345\n",
      "activ tensor(4.9172, grad_fn=<CopyBackwards>) tensor(0.0601, grad_fn=<DivBackward0>) tensor(0.9560, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.9243, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.9172, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.9243, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5526376 0.0 0.02094697 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.9243, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -24.98497 -0.49218717 23.81067 81.8941\n",
      "activ tensor(4.9243, grad_fn=<CopyBackwards>) tensor(0.0601, grad_fn=<DivBackward0>) tensor(0.9583, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.9353, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.9243, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.9353, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56965655 0.0 0.021845527 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.9353, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -24.81294 -0.4758716 23.68321 82.00334\n",
      "activ tensor(4.9353, grad_fn=<CopyBackwards>) tensor(0.0602, grad_fn=<DivBackward0>) tensor(1.0569, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.9346, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.9353, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.9346, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56032336 0.0 0.021338105 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.9346, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -24.452215 -0.47725967 24.445889 82.115295\n",
      "activ tensor(4.9346, grad_fn=<CopyBackwards>) tensor(0.0601, grad_fn=<DivBackward0>) tensor(1.1955, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.9364, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.9346, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.9364, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5517886 0.0 0.020978685 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.9364, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -24.393414 -0.4558987 24.267164 82.2253\n",
      "activ tensor(4.9364, grad_fn=<CopyBackwards>) tensor(0.0600, grad_fn=<DivBackward0>) tensor(1.0374, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.9420, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.9364, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.9420, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5554926 0.0 0.021063255 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.9419, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -24.344866 -0.45311904 24.097414 82.33234\n",
      "activ tensor(4.9419, grad_fn=<CopyBackwards>) tensor(0.0600, grad_fn=<DivBackward0>) tensor(0.9354, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.9484, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.9419, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.9484, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56780213 0.0 0.021760957 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.9484, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -24.497751 -0.44814605 24.146303 82.43726\n",
      "activ tensor(4.9484, grad_fn=<CopyBackwards>) tensor(0.0600, grad_fn=<DivBackward0>) tensor(0.9826, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.9512, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.9484, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.9512, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.58206904 0.0 0.02269123 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.9513, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -24.74274 -0.47269186 23.487726 82.54005\n",
      "activ tensor(4.9513, grad_fn=<CopyBackwards>) tensor(0.0600, grad_fn=<DivBackward0>) tensor(1.0801, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.9451, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.9513, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.9451, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5724268 0.0 0.021845529 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.9452, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -24.23564 -0.47176987 24.04361 82.639694\n",
      "activ tensor(4.9452, grad_fn=<CopyBackwards>) tensor(0.0598, grad_fn=<DivBackward0>) tensor(1.1275, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.9576, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.9452, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.9576, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5648018 0.0 0.02161296 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.9574, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -24.62534 -0.4593982 24.43266 82.74738\n",
      "activ tensor(4.9574, grad_fn=<CopyBackwards>) tensor(0.0599, grad_fn=<DivBackward0>) tensor(1.0530, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.9736, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.9574, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.9736, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5594861 0.0 0.021221824 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.9737, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -24.546604 -0.49705112 24.090353 82.853386\n",
      "activ tensor(4.9737, grad_fn=<CopyBackwards>) tensor(0.0600, grad_fn=<DivBackward0>) tensor(0.9764, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.9765, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.9737, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.9765, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.588476 0.0 0.022839228 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.9763, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -24.443546 -0.4726275 23.701853 82.95926\n",
      "activ tensor(4.9763, grad_fn=<CopyBackwards>) tensor(0.0600, grad_fn=<DivBackward0>) tensor(1.0324, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.9769, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.9763, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.9769, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5720171 0.0 0.021993525 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.9769, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -24.674667 -0.4728874 23.725756 83.064384\n",
      "activ tensor(4.9769, grad_fn=<CopyBackwards>) tensor(0.0599, grad_fn=<DivBackward0>) tensor(1.1467, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.9802, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.9769, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.9802, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5864816 0.0 0.022553803 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.9802, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -24.324629 -0.45431793 24.071016 83.16768\n",
      "activ tensor(4.9802, grad_fn=<CopyBackwards>) tensor(0.0599, grad_fn=<DivBackward0>) tensor(1.1053, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.9979, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.9802, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.9979, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57263136 0.0 0.021834956 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.9980, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -25.037487 -0.47544146 24.066137 83.26117\n",
      "activ tensor(4.9980, grad_fn=<CopyBackwards>) tensor(0.0600, grad_fn=<DivBackward0>) tensor(0.9907, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.9972, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.9980, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.9972, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57385814 0.0 0.022120383 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.9973, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -24.698769 -0.48617858 24.537891 83.35129\n",
      "activ tensor(4.9973, grad_fn=<CopyBackwards>) tensor(0.0600, grad_fn=<DivBackward0>) tensor(1.0601, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.9992, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.9973, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.9992, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57037604 0.0 0.021824386 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.9993, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -24.628944 -0.4691916 24.1215 83.4367\n",
      "activ tensor(4.9993, grad_fn=<CopyBackwards>) tensor(0.0599, grad_fn=<DivBackward0>) tensor(0.9830, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.0000, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.9993, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.0000, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56718266 0.0 0.02167639 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.0001, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -24.826454 -0.4862519 23.762943 83.514465\n",
      "activ tensor(5.0001, grad_fn=<CopyBackwards>) tensor(0.0599, grad_fn=<DivBackward0>) tensor(1.2101, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.0092, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.0001, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.0092, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5410661 0.0 0.020513551 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.0092, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -24.823978 -0.4784965 24.448833 83.59268\n",
      "activ tensor(5.0092, grad_fn=<CopyBackwards>) tensor(0.0599, grad_fn=<DivBackward0>) tensor(1.1795, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.0131, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.0092, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.0131, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5875795 0.0 0.022765227 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.0130, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -24.501976 -0.48937196 24.497028 83.67515\n",
      "activ tensor(5.0130, grad_fn=<CopyBackwards>) tensor(0.0599, grad_fn=<DivBackward0>) tensor(1.0275, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.0211, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.0130, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.0211, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.562098 0.0 0.021464963 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.0211, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -24.566225 -0.4781387 24.876234 83.76016\n",
      "activ tensor(5.0211, grad_fn=<CopyBackwards>) tensor(0.0599, grad_fn=<DivBackward0>) tensor(1.0451, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.0278, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.0211, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.0278, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.58508116 0.0 0.022691227 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.0277, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -25.12903 -0.4662271 24.656935 83.848526\n",
      "activ tensor(5.0277, grad_fn=<CopyBackwards>) tensor(0.0600, grad_fn=<DivBackward0>) tensor(1.0176, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.0287, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.0277, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.0287, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5629315 0.0 0.021528391 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.0287, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -25.626352 -0.4827112 24.20858 83.941696\n",
      "activ tensor(5.0287, grad_fn=<CopyBackwards>) tensor(0.0599, grad_fn=<DivBackward0>) tensor(1.1440, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.0281, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.0287, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.0281, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56157684 0.0 0.021581246 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.0282, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -25.391914 -0.46445346 24.567875 84.03923\n",
      "activ tensor(5.0282, grad_fn=<CopyBackwards>) tensor(0.0598, grad_fn=<DivBackward0>) tensor(1.1746, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.0298, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.0282, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.0298, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5467752 0.0 0.020936402 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.0300, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -24.993958 -0.5009527 24.892687 84.13465\n",
      "activ tensor(5.0300, grad_fn=<CopyBackwards>) tensor(0.0598, grad_fn=<DivBackward0>) tensor(1.0004, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.0372, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.0300, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.0372, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5657345 0.0 0.021697532 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.0371, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -25.038761 -0.4961251 25.027052 84.24518\n",
      "activ tensor(5.0371, grad_fn=<CopyBackwards>) tensor(0.0598, grad_fn=<DivBackward0>) tensor(1.0539, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.0462, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.0371, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.0462, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5584382 0.0 0.021242965 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.0461, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -25.068422 -0.48726115 24.640732 84.36304\n",
      "activ tensor(5.0461, grad_fn=<CopyBackwards>) tensor(0.0598, grad_fn=<DivBackward0>) tensor(1.0731, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.0484, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.0461, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.0484, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5664592 0.0 0.021908954 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.0483, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -25.158625 -0.4855516 24.633688 84.48261\n",
      "activ tensor(5.0483, grad_fn=<CopyBackwards>) tensor(0.0598, grad_fn=<DivBackward0>) tensor(1.2079, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.0558, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.0483, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.0558, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.58738005 0.0 0.022585519 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.0558, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -25.3639 -0.47571146 24.362608 84.602615\n",
      "activ tensor(5.0558, grad_fn=<CopyBackwards>) tensor(0.0598, grad_fn=<DivBackward0>) tensor(1.0141, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.0529, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.0558, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.0529, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5794463 0.0 0.02243752 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.0529, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -24.826761 -0.46819133 24.457258 84.720924\n",
      "activ tensor(5.0529, grad_fn=<CopyBackwards>) tensor(0.0596, grad_fn=<DivBackward0>) tensor(0.9990, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.0598, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.0529, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.0598, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5596958 0.0 0.021316964 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.0599, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -25.10475 -0.47885197 25.281746 84.83282\n",
      "activ tensor(5.0599, grad_fn=<CopyBackwards>) tensor(0.0596, grad_fn=<DivBackward0>) tensor(0.9745, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.0711, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.0599, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.0711, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5816666 0.0 0.022173237 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.0711, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -25.086834 -0.49312228 24.562677 84.94272\n",
      "activ tensor(5.0711, grad_fn=<CopyBackwards>) tensor(0.0597, grad_fn=<DivBackward0>) tensor(1.1428, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.0770, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.0711, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.0770, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5594865 0.0 0.021211252 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.0771, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -25.061745 -0.478563 24.260523 85.04752\n",
      "activ tensor(5.0771, grad_fn=<CopyBackwards>) tensor(0.0597, grad_fn=<DivBackward0>) tensor(1.2521, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.0848, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.0771, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.0848, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56552756 0.0 0.021464964 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.0849, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -25.136478 -0.47795027 24.485704 85.14626\n",
      "activ tensor(5.0849, grad_fn=<CopyBackwards>) tensor(0.0597, grad_fn=<DivBackward0>) tensor(1.0395, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.0912, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.0849, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.0912, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55580896 0.0 0.021242967 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.0912, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -24.89988 -0.5034289 24.512627 85.23922\n",
      "activ tensor(5.0912, grad_fn=<CopyBackwards>) tensor(0.0597, grad_fn=<DivBackward0>) tensor(1.0004, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.0952, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.0912, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.0952, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55485916 0.0 0.021073826 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.0951, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -25.558014 -0.49433234 24.940788 85.33024\n",
      "activ tensor(5.0951, grad_fn=<CopyBackwards>) tensor(0.0597, grad_fn=<DivBackward0>) tensor(1.0721, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.1034, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.0951, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.1034, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5652166 0.0 0.02202524 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.1033, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -25.366894 -0.470472 24.912992 85.42309\n",
      "activ tensor(5.1033, grad_fn=<CopyBackwards>) tensor(0.0597, grad_fn=<DivBackward0>) tensor(1.0687, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.1074, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.1033, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.1074, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.54634666 0.0 0.020809542 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.1073, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -25.063368 -0.46557492 24.733309 85.51291\n",
      "activ tensor(5.1073, grad_fn=<CopyBackwards>) tensor(0.0597, grad_fn=<DivBackward0>) tensor(1.2151, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.1066, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.1073, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.1066, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5734495 0.0 0.021972384 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.1066, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -25.380676 -0.4935813 24.441385 85.59752\n",
      "activ tensor(5.1066, grad_fn=<CopyBackwards>) tensor(0.0597, grad_fn=<DivBackward0>) tensor(1.0113, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.1229, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.1066, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.1229, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5610549 0.0 0.021486105 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.1230, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -25.329815 -0.49173144 24.904247 85.68338\n",
      "activ tensor(5.1230, grad_fn=<CopyBackwards>) tensor(0.0598, grad_fn=<DivBackward0>) tensor(1.0139, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.1304, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.1230, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.1304, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55696744 0.0 0.021433247 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.1306, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -25.079967 -0.5035517 25.364292 85.76335\n",
      "activ tensor(5.1306, grad_fn=<CopyBackwards>) tensor(0.0598, grad_fn=<DivBackward0>) tensor(1.0430, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.1260, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.1306, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.1260, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57833284 0.0 0.022331806 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.1259, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -25.181 -0.5025439 25.219168 85.85049\n",
      "activ tensor(5.1259, grad_fn=<CopyBackwards>) tensor(0.0597, grad_fn=<DivBackward0>) tensor(1.0866, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.1341, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.1259, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.1341, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5718123 0.0 0.022173239 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.1341, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -25.766903 -0.4946919 25.371595 85.943306\n",
      "activ tensor(5.1341, grad_fn=<CopyBackwards>) tensor(0.0597, grad_fn=<DivBackward0>) tensor(1.1441, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.1401, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.1341, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.1401, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55938166 0.0 0.021221824 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.1400, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -26.250711 -0.49270362 24.760998 86.037834\n",
      "activ tensor(5.1400, grad_fn=<CopyBackwards>) tensor(0.0597, grad_fn=<DivBackward0>) tensor(1.1512, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.1434, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.1400, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.1434, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.568421 0.0 0.02168696 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.1434, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -25.960558 -0.5059924 25.112558 86.13084\n",
      "activ tensor(5.1434, grad_fn=<CopyBackwards>) tensor(0.0597, grad_fn=<DivBackward0>) tensor(0.9894, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.1412, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.1434, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.1412, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5532732 0.0 0.02126411 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.1412, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -25.54611 -0.4950335 25.766193 86.228584\n",
      "activ tensor(5.1412, grad_fn=<CopyBackwards>) tensor(0.0596, grad_fn=<DivBackward0>) tensor(1.0051, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.1491, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.1412, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.1491, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5826729 0.0 0.022458663 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.1491, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -25.679703 -0.4813401 25.389826 86.328766\n",
      "activ tensor(5.1491, grad_fn=<CopyBackwards>) tensor(0.0596, grad_fn=<DivBackward0>) tensor(1.2090, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.1516, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.1491, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.1516, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55623055 0.0 0.021242969 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.1518, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -25.793682 -0.49084428 25.408487 86.428276\n",
      "activ tensor(5.1518, grad_fn=<CopyBackwards>) tensor(0.0596, grad_fn=<DivBackward0>) tensor(1.1686, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.1586, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.1518, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.1586, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5615771 0.0 0.02161296 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.1588, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -25.82315 -0.4945963 25.131063 86.53152\n",
      "activ tensor(5.1588, grad_fn=<CopyBackwards>) tensor(0.0596, grad_fn=<DivBackward0>) tensor(1.1213, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.1636, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.1588, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.1636, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5717098 0.0 0.021856101 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.1637, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -25.954744 -0.49325183 25.006042 86.63662\n",
      "activ tensor(5.1637, grad_fn=<CopyBackwards>) tensor(0.0596, grad_fn=<DivBackward0>) tensor(1.0371, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.1727, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.1637, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.1727, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5423642 0.0 0.020746116 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.1727, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -25.453663 -0.5118452 25.275171 86.74145\n",
      "activ tensor(5.1727, grad_fn=<CopyBackwards>) tensor(0.0596, grad_fn=<DivBackward0>) tensor(1.0056, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.1761, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.1727, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.1761, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55104476 0.0 0.020851828 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.1760, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -25.83163 -0.5083523 25.668936 86.85728\n",
      "activ tensor(5.1760, grad_fn=<CopyBackwards>) tensor(0.0596, grad_fn=<DivBackward0>) tensor(1.1622, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.1848, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.1760, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.1848, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5543312 0.0 0.020946968 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.1849, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -25.523026 -0.48035127 25.398642 86.97736\n",
      "activ tensor(5.1849, grad_fn=<CopyBackwards>) tensor(0.0596, grad_fn=<DivBackward0>) tensor(1.1092, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.1890, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.1849, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.1890, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5794461 0.0 0.022395235 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.1889, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -25.636364 -0.4692639 24.653261 87.09467\n",
      "activ tensor(5.1889, grad_fn=<CopyBackwards>) tensor(0.0596, grad_fn=<DivBackward0>) tensor(0.9801, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.1957, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.1889, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.1957, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55917203 0.0 0.02119011 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.1957, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -25.657066 -0.5104236 25.186865 87.20803\n",
      "activ tensor(5.1957, grad_fn=<CopyBackwards>) tensor(0.0596, grad_fn=<DivBackward0>) tensor(0.9648, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.1998, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.1957, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.1998, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57109463 0.0 0.02169753 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.1998, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -25.681726 -0.506332 25.239698 87.32081\n",
      "activ tensor(5.1998, grad_fn=<CopyBackwards>) tensor(0.0595, grad_fn=<DivBackward0>) tensor(1.1625, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.1926, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.1998, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.1926, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5504066 0.0 0.020756686 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.1927, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -26.33384 -0.5040397 25.394136 87.423225\n",
      "activ tensor(5.1927, grad_fn=<CopyBackwards>) tensor(0.0594, grad_fn=<DivBackward0>) tensor(1.1939, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.1995, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.1927, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.1995, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5695537 0.0 0.021930099 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.1995, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -25.738916 -0.5050074 25.703325 87.52561\n",
      "activ tensor(5.1995, grad_fn=<CopyBackwards>) tensor(0.0594, grad_fn=<DivBackward0>) tensor(1.0677, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.2080, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.1995, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.2080, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5638674 0.0 0.021433247 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.2081, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -25.642262 -0.49571466 25.05446 87.62706\n",
      "activ tensor(5.2081, grad_fn=<CopyBackwards>) tensor(0.0594, grad_fn=<DivBackward0>) tensor(1.0489, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.2108, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.2081, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.2108, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.58085996 0.0 0.022648945 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.2107, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -26.026443 -0.49254784 25.205172 87.71427\n",
      "activ tensor(5.2107, grad_fn=<CopyBackwards>) tensor(0.0594, grad_fn=<DivBackward0>) tensor(1.0884, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.2101, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.2107, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.2101, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.53496784 0.0 0.020164695 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.2100, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -25.845285 -0.49867773 25.529142 87.794075\n",
      "activ tensor(5.2100, grad_fn=<CopyBackwards>) tensor(0.0593, grad_fn=<DivBackward0>) tensor(1.0839, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.2137, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.2100, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.2137, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57660794 0.0 0.021993525 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.2137, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -25.858082 -0.4977673 25.86312 87.87566\n",
      "activ tensor(5.2137, grad_fn=<CopyBackwards>) tensor(0.0593, grad_fn=<DivBackward0>) tensor(1.2450, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.2184, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.2137, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.2184, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5595909 0.0 0.021253537 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.2185, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -25.758173 -0.49319524 26.003937 87.950775\n",
      "activ tensor(5.2185, grad_fn=<CopyBackwards>) tensor(0.0593, grad_fn=<DivBackward0>) tensor(1.0875, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.2228, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.2185, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.2228, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5632437 0.0 0.02167639 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.2229, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -26.371454 -0.4962623 25.678736 88.026245\n",
      "activ tensor(5.2229, grad_fn=<CopyBackwards>) tensor(0.0593, grad_fn=<DivBackward0>) tensor(1.0336, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.2263, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.2229, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.2263, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57569283 0.0 0.022300093 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.2265, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -26.879932 -0.5076395 25.569784 88.10166\n",
      "activ tensor(5.2265, grad_fn=<CopyBackwards>) tensor(0.0593, grad_fn=<DivBackward0>) tensor(1.0567, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.2358, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.2265, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.2358, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.564698 0.0 0.021634102 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.2358, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -26.515001 -0.5059424 25.643272 88.18026\n",
      "activ tensor(5.2358, grad_fn=<CopyBackwards>) tensor(0.0594, grad_fn=<DivBackward0>) tensor(1.0772, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.2393, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.2358, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.2393, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5676991 0.0 0.021708103 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.2394, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -26.10197 -0.5323805 26.413473 88.26515\n",
      "activ tensor(5.2394, grad_fn=<CopyBackwards>) tensor(0.0594, grad_fn=<DivBackward0>) tensor(1.1524, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.2402, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.2394, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.2402, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56563115 0.0 0.02161296 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.2402, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -26.329487 -0.52903545 26.130854 88.36597\n",
      "activ tensor(5.2402, grad_fn=<CopyBackwards>) tensor(0.0593, grad_fn=<DivBackward0>) tensor(1.1714, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.2466, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.2402, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.2466, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56955373 0.0 0.021644674 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.2465, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -26.509272 -0.5085002 25.741459 88.47888\n",
      "activ tensor(5.2465, grad_fn=<CopyBackwards>) tensor(0.0593, grad_fn=<DivBackward0>) tensor(1.0072, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.2463, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.2465, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.2463, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57037604 0.0 0.021972382 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.2462, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -26.48648 -0.49810445 25.98177 88.59242\n",
      "activ tensor(5.2462, grad_fn=<CopyBackwards>) tensor(0.0592, grad_fn=<DivBackward0>) tensor(0.9811, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.2557, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.2462, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.2557, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5690391 0.0 0.021739814 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.2557, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -26.538532 -0.5140959 25.500242 88.709015\n",
      "activ tensor(5.2557, grad_fn=<CopyBackwards>) tensor(0.0592, grad_fn=<DivBackward0>) tensor(1.1555, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.2639, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.2557, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.2639, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5841792 0.0 0.022416377 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.2640, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -26.069172 -0.52182084 26.005648 88.82927\n",
      "activ tensor(5.2640, grad_fn=<CopyBackwards>) tensor(0.0593, grad_fn=<DivBackward0>) tensor(1.1097, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.2680, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.2640, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.2680, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5565462 0.0 0.020978685 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.2681, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -26.36105 -0.5177992 26.350918 88.945786\n",
      "activ tensor(5.2681, grad_fn=<CopyBackwards>) tensor(0.0592, grad_fn=<DivBackward0>) tensor(1.0818, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.2826, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.2681, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.2826, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5543313 0.0 0.021158395 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.2825, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -25.963905 -0.5099122 25.797115 89.067215\n",
      "activ tensor(5.2825, grad_fn=<CopyBackwards>) tensor(0.0593, grad_fn=<DivBackward0>) tensor(1.0456, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.2766, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.2825, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.2766, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5801537 0.0 0.022088667 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.2766, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -26.293262 -0.50525916 25.493887 89.18359\n",
      "activ tensor(5.2766, grad_fn=<CopyBackwards>) tensor(0.0592, grad_fn=<DivBackward0>) tensor(1.1218, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.2780, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.2766, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.2780, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56862706 0.0 0.021623531 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.2780, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -26.48284 -0.5062366 25.58285 89.2917\n",
      "activ tensor(5.2780, grad_fn=<CopyBackwards>) tensor(0.0591, grad_fn=<DivBackward0>) tensor(1.2170, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.2831, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.2780, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.2831, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5549648 0.0 0.020946972 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.2832, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -26.295168 -0.50175345 26.062193 89.393875\n",
      "activ tensor(5.2832, grad_fn=<CopyBackwards>) tensor(0.0591, grad_fn=<DivBackward0>) tensor(1.2136, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.2913, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.2832, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.2913, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57283616 0.0 0.021930097 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.2912, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -26.979221 -0.4851058 25.986565 89.494255\n",
      "activ tensor(5.2912, grad_fn=<CopyBackwards>) tensor(0.0591, grad_fn=<DivBackward0>) tensor(1.0304, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.2876, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.2912, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.2876, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5493411 0.0 0.020830687 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.2875, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -26.327213 -0.501374 26.150618 89.58877\n",
      "activ tensor(5.2875, grad_fn=<CopyBackwards>) tensor(0.0590, grad_fn=<DivBackward0>) tensor(1.0349, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.2852, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.2875, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.2852, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55168253 0.0 0.020746117 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.2852, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -26.569921 -0.48567948 25.81145 89.67704\n",
      "activ tensor(5.2852, grad_fn=<CopyBackwards>) tensor(0.0589, grad_fn=<DivBackward0>) tensor(1.0465, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.2923, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.2852, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.2923, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5747761 0.0 0.0217821 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.2924, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -26.557604 -0.50308734 25.586214 89.75763\n",
      "activ tensor(5.2924, grad_fn=<CopyBackwards>) tensor(0.0590, grad_fn=<DivBackward0>) tensor(1.1330, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.2959, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.2924, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.2959, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57772475 0.0 0.022141526 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.2960, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -26.356316 -0.5225693 26.386621 89.83728\n",
      "activ tensor(5.2960, grad_fn=<CopyBackwards>) tensor(0.0590, grad_fn=<DivBackward0>) tensor(1.1376, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.3057, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.2960, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.3057, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56376356 0.0 0.021486107 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.3057, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -26.631788 -0.521211 26.357212 89.921585\n",
      "activ tensor(5.3057, grad_fn=<CopyBackwards>) tensor(0.0590, grad_fn=<DivBackward0>) tensor(1.0406, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.3121, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.3057, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.3121, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57497996 0.0 0.022289522 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.3121, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -26.363255 -0.52761984 26.542181 90.014534\n",
      "activ tensor(5.3121, grad_fn=<CopyBackwards>) tensor(0.0590, grad_fn=<DivBackward0>) tensor(1.2034, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.3189, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.3121, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.3189, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5735517 0.0 0.022141526 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.3189, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -26.994387 -0.51960886 26.39316 90.10919\n",
      "activ tensor(5.3189, grad_fn=<CopyBackwards>) tensor(0.0590, grad_fn=<DivBackward0>) tensor(1.0987, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.3270, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.3189, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.3270, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56188995 0.0 0.02179267 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.3271, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -27.465767 -0.5206175 25.934893 90.20383\n",
      "activ tensor(5.3271, grad_fn=<CopyBackwards>) tensor(0.0591, grad_fn=<DivBackward0>) tensor(1.1531, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.3291, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.3271, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.3291, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5611595 0.0 0.021443821 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.3291, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -27.090803 -0.53311694 26.5422 90.29837\n",
      "activ tensor(5.3291, grad_fn=<CopyBackwards>) tensor(0.0590, grad_fn=<DivBackward0>) tensor(1.0640, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.3369, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.3291, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.3369, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5716074 0.0 0.0220041 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.3370, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -26.646534 -0.5458661 26.876158 90.395874\n",
      "activ tensor(5.3370, grad_fn=<CopyBackwards>) tensor(0.0590, grad_fn=<DivBackward0>) tensor(1.0804, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.3511, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.3370, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.3511, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57528555 0.0 0.02210981 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.3514, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -26.987253 -0.5395139 26.8001 90.495575\n",
      "activ tensor(5.3514, grad_fn=<CopyBackwards>) tensor(0.0591, grad_fn=<DivBackward0>) tensor(1.1103, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.3537, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.3514, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.3537, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56924504 0.0 0.021961814 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.3536, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -27.224167 -0.515243 26.358587 90.59774\n",
      "activ tensor(5.3536, grad_fn=<CopyBackwards>) tensor(0.0591, grad_fn=<DivBackward0>) tensor(1.1338, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.3573, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.3536, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.3573, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5615769 0.0 0.02168696 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.3574, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -27.134808 -0.548166 26.422468 90.6983\n",
      "activ tensor(5.3574, grad_fn=<CopyBackwards>) tensor(0.0591, grad_fn=<DivBackward0>) tensor(1.3026, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.3650, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.3574, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.3650, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5734495 0.0 0.021824386 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.3648, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -27.12222 -0.529665 26.419245 90.79729\n",
      "activ tensor(5.3648, grad_fn=<CopyBackwards>) tensor(0.0591, grad_fn=<DivBackward0>) tensor(1.1606, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.3743, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.3648, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.3743, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56986225 0.0 0.021771532 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.3743, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -26.76823 -0.50771135 26.438421 90.8975\n",
      "activ tensor(5.3743, grad_fn=<CopyBackwards>) tensor(0.0591, grad_fn=<DivBackward0>) tensor(1.2403, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.3813, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.3743, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.3813, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5654241 0.0 0.021538962 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.3813, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -26.804028 -0.49409774 27.10251 91.00411\n",
      "activ tensor(5.3813, grad_fn=<CopyBackwards>) tensor(0.0591, grad_fn=<DivBackward0>) tensor(1.0266, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.3883, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.3813, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.3883, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5741643 0.0 0.022046382 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.3883, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -26.550097 -0.5003054 26.357422 91.106285\n",
      "activ tensor(5.3883, grad_fn=<CopyBackwards>) tensor(0.0591, grad_fn=<DivBackward0>) tensor(1.0816, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.3957, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.3883, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.3957, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5731429 0.0 0.022183811 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.3958, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -26.950567 -0.5017421 25.985079 91.20708\n",
      "activ tensor(5.3958, grad_fn=<CopyBackwards>) tensor(0.0592, grad_fn=<DivBackward0>) tensor(1.2191, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.3999, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.3958, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.3999, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.54419756 0.0 0.020418407 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.3999, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -26.871304 -0.5074387 26.437605 91.30354\n",
      "activ tensor(5.3999, grad_fn=<CopyBackwards>) tensor(0.0591, grad_fn=<DivBackward0>) tensor(1.0590, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.3958, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.3999, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.3958, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57324487 0.0 0.022004098 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.3959, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -27.117126 -0.53182095 26.470947 91.40397\n",
      "activ tensor(5.3959, grad_fn=<CopyBackwards>) tensor(0.0590, grad_fn=<DivBackward0>) tensor(1.1101, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.4009, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.3959, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.4009, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5528494 0.0 0.020925825 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.4010, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -27.459227 -0.51419914 26.822416 91.51082\n",
      "activ tensor(5.4010, grad_fn=<CopyBackwards>) tensor(0.0590, grad_fn=<DivBackward0>) tensor(1.0773, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.4182, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.4010, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.4182, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55464804 0.0 0.020978685 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.4180, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -26.899279 -0.52604 26.580008 91.62296\n",
      "activ tensor(5.4180, grad_fn=<CopyBackwards>) tensor(0.0591, grad_fn=<DivBackward0>) tensor(1.1231, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.4187, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.4180, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.4187, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55906725 0.0 0.021464964 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.4188, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -27.180857 -0.5368772 26.356096 91.737175\n",
      "activ tensor(5.4188, grad_fn=<CopyBackwards>) tensor(0.0591, grad_fn=<DivBackward0>) tensor(1.2182, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.4158, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.4188, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.4158, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.58648163 0.0 0.022828657 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.4158, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -26.978779 -0.5160964 26.358835 91.84349\n",
      "activ tensor(5.4158, grad_fn=<CopyBackwards>) tensor(0.0590, grad_fn=<DivBackward0>) tensor(1.0341, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.4197, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.4158, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.4197, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5538025 0.0 0.020925827 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.4197, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -26.855717 -0.5341013 26.812637 91.94811\n",
      "activ tensor(5.4197, grad_fn=<CopyBackwards>) tensor(0.0589, grad_fn=<DivBackward0>) tensor(1.1157, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.4199, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.4197, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.4199, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5637635 0.0 0.021507248 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.4200, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -27.149956 -0.5279398 27.192692 92.052505\n",
      "activ tensor(5.4200, grad_fn=<CopyBackwards>) tensor(0.0589, grad_fn=<DivBackward0>) tensor(1.0656, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.4238, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.4200, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.4238, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57170993 0.0 0.021803243 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.4239, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -26.979643 -0.5437732 26.914644 92.14717\n",
      "activ tensor(5.4239, grad_fn=<CopyBackwards>) tensor(0.0589, grad_fn=<DivBackward0>) tensor(1.2361, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.4223, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.4239, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.4223, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55927664 0.0 0.021390963 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.4222, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -27.59629 -0.53489596 27.028194 92.241936\n",
      "activ tensor(5.4222, grad_fn=<CopyBackwards>) tensor(0.0588, grad_fn=<DivBackward0>) tensor(1.1905, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.4303, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.4222, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.4303, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56157666 0.0 0.021306394 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.4303, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -28.058014 -0.550698 26.611776 92.32974\n",
      "activ tensor(5.4303, grad_fn=<CopyBackwards>) tensor(0.0588, grad_fn=<DivBackward0>) tensor(1.1346, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.4320, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.4303, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.4320, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5488076 0.0 0.020703832 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.4319, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -27.63707 -0.5421194 27.005713 92.41283\n",
      "activ tensor(5.4319, grad_fn=<CopyBackwards>) tensor(0.0588, grad_fn=<DivBackward0>) tensor(1.3041, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.4362, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.4319, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.4362, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5496609 0.0 0.020946972 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.4361, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -27.225494 -0.511567 27.768429 92.49636\n",
      "activ tensor(5.4361, grad_fn=<CopyBackwards>) tensor(0.0588, grad_fn=<DivBackward0>) tensor(1.1051, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.4457, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.4361, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.4457, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55475384 0.0 0.020894114 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.4456, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -27.623346 -0.5130747 27.148523 92.58229\n",
      "activ tensor(5.4456, grad_fn=<CopyBackwards>) tensor(0.0588, grad_fn=<DivBackward0>) tensor(1.1441, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.4556, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.4456, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.4556, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5703762 0.0 0.021623531 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.4556, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -27.946827 -0.5266103 27.076464 92.66324\n",
      "activ tensor(5.4556, grad_fn=<CopyBackwards>) tensor(0.0589, grad_fn=<DivBackward0>) tensor(1.1341, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.4612, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.4556, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.4612, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55759805 0.0 0.021327535 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.4614, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -27.772522 -0.54260147 27.01174 92.74585\n",
      "activ tensor(5.4614, grad_fn=<CopyBackwards>) tensor(0.0589, grad_fn=<DivBackward0>) tensor(1.0955, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.4694, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.4614, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.4694, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57078665 0.0 0.022046383 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.4696, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -27.694405 -0.5298559 27.002554 92.83206\n",
      "activ tensor(5.4696, grad_fn=<CopyBackwards>) tensor(0.0589, grad_fn=<DivBackward0>) tensor(1.0910, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.4673, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.4696, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.4673, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55485946 0.0 0.020989256 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.4674, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -27.30957 -0.53406054 27.298891 92.92465\n",
      "activ tensor(5.4674, grad_fn=<CopyBackwards>) tensor(0.0588, grad_fn=<DivBackward0>) tensor(1.1500, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.4766, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.4674, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.4766, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5651129 0.0 0.021422679 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.4766, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -27.489464 -0.5509313 27.453714 93.02895\n",
      "activ tensor(5.4766, grad_fn=<CopyBackwards>) tensor(0.0589, grad_fn=<DivBackward0>) tensor(1.2886, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.4795, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.4766, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.4795, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55083203 0.0 0.020915255 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.4795, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -27.49929 -0.5407807 27.14852 93.14247\n",
      "activ tensor(5.4795, grad_fn=<CopyBackwards>) tensor(0.0588, grad_fn=<DivBackward0>) tensor(1.1215, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.4877, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.4795, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.4877, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5552818 0.0 0.021158395 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.4877, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -27.584604 -0.5442982 26.487932 93.257965\n",
      "activ tensor(5.4877, grad_fn=<CopyBackwards>) tensor(0.0588, grad_fn=<DivBackward0>) tensor(1.1732, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.4961, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.4877, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.4961, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57894033 0.0 0.022500949 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.4962, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -27.445875 -0.5308986 27.063349 93.37667\n",
      "activ tensor(5.4962, grad_fn=<CopyBackwards>) tensor(0.0589, grad_fn=<DivBackward0>) tensor(1.0241, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.5040, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.4962, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.5040, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5688331 0.0 0.0218561 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.5041, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -27.74561 -0.5337942 27.278318 93.49791\n",
      "activ tensor(5.5041, grad_fn=<CopyBackwards>) tensor(0.0589, grad_fn=<DivBackward0>) tensor(1.1063, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.5149, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.5041, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.5149, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56542397 0.0 0.021792673 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.5149, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -28.18478 -0.52186334 27.199133 93.61688\n",
      "activ tensor(5.5149, grad_fn=<CopyBackwards>) tensor(0.0589, grad_fn=<DivBackward0>) tensor(1.2792, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.5173, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.5149, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.5173, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.554014 0.0 0.021084398 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.5174, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -27.359465 -0.5466535 27.379465 93.72693\n",
      "activ tensor(5.5174, grad_fn=<CopyBackwards>) tensor(0.0589, grad_fn=<DivBackward0>) tensor(1.1641, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.5218, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.5174, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.5218, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56095046 0.0 0.02145439 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.5216, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -27.57603 -0.53897464 26.728891 93.83548\n",
      "activ tensor(5.5216, grad_fn=<CopyBackwards>) tensor(0.0588, grad_fn=<DivBackward0>) tensor(1.0380, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.5265, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.5216, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.5265, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5493411 0.0 0.020809544 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.5265, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -27.610538 -0.5299594 27.074686 93.936386\n",
      "activ tensor(5.5265, grad_fn=<CopyBackwards>) tensor(0.0588, grad_fn=<DivBackward0>) tensor(1.1361, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.5358, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.5265, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.5358, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5673893 0.0 0.021908956 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.5356, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -27.39093 -0.5399427 27.521769 94.03286\n",
      "activ tensor(5.5356, grad_fn=<CopyBackwards>) tensor(0.0589, grad_fn=<DivBackward0>) tensor(1.3425, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.5495, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.5356, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.5495, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5674924 0.0 0.021634104 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.5493, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -27.924232 -0.5147918 27.605448 94.12832\n",
      "activ tensor(5.5493, grad_fn=<CopyBackwards>) tensor(0.0590, grad_fn=<DivBackward0>) tensor(1.1844, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.5438, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.5493, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.5438, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57375574 0.0 0.022226095 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.5437, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -27.61192 -0.50432205 27.700428 94.219246\n",
      "activ tensor(5.5437, grad_fn=<CopyBackwards>) tensor(0.0588, grad_fn=<DivBackward0>) tensor(1.0994, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.5473, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.5437, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.5473, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5661488 0.0 0.021475533 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.5472, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -28.186443 -0.5309228 27.371965 94.29972\n",
      "activ tensor(5.5472, grad_fn=<CopyBackwards>) tensor(0.0588, grad_fn=<DivBackward0>) tensor(1.0401, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.5481, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.5472, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.5481, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56313956 0.0 0.02144382 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.5483, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -28.629105 -0.5513616 27.378418 94.38079\n",
      "activ tensor(5.5483, grad_fn=<CopyBackwards>) tensor(0.0588, grad_fn=<DivBackward0>) tensor(1.0987, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.5478, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.5483, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.5478, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5655275 0.0 0.021549534 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.5480, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -28.19822 -0.5435413 27.628658 94.46425\n",
      "activ tensor(5.5480, grad_fn=<CopyBackwards>) tensor(0.0587, grad_fn=<DivBackward0>) tensor(1.2631, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.5585, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.5480, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.5585, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5689361 0.0 0.021708101 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.5585, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -27.788399 -0.55815005 28.292143 94.55181\n",
      "activ tensor(5.5585, grad_fn=<CopyBackwards>) tensor(0.0588, grad_fn=<DivBackward0>) tensor(1.1743, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.5734, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.5585, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.5734, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5731428 0.0 0.021940669 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.5734, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -28.28743 -0.5786921 27.942488 94.64574\n",
      "activ tensor(5.5734, grad_fn=<CopyBackwards>) tensor(0.0589, grad_fn=<DivBackward0>) tensor(1.0996, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.5792, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.5734, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.5792, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56532025 0.0 0.021644674 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.5792, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -28.6352 -0.5552807 27.386034 94.744995\n",
      "activ tensor(5.5792, grad_fn=<CopyBackwards>) tensor(0.0589, grad_fn=<DivBackward0>) tensor(1.1234, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.5879, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.5792, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.5879, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5624108 0.0 0.021623531 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.5880, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -28.422443 -0.5638471 27.858002 94.84378\n",
      "activ tensor(5.5880, grad_fn=<CopyBackwards>) tensor(0.0589, grad_fn=<DivBackward0>) tensor(1.2971, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.5927, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.5880, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.5927, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5775218 0.0 0.022226095 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.5928, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -28.25178 -0.5537984 27.568865 94.94431\n",
      "activ tensor(5.5928, grad_fn=<CopyBackwards>) tensor(0.0589, grad_fn=<DivBackward0>) tensor(1.2707, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.5987, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.5928, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.5987, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5569671 0.0 0.021137254 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.5987, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -27.968428 -0.5562333 27.93867 95.04871\n",
      "activ tensor(5.5987, grad_fn=<CopyBackwards>) tensor(0.0589, grad_fn=<DivBackward0>) tensor(1.2873, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.5981, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.5987, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.5981, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5457025 0.0 0.020735547 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.5981, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -27.89946 -0.54236776 28.180265 95.152115\n",
      "activ tensor(5.5981, grad_fn=<CopyBackwards>) tensor(0.0588, grad_fn=<DivBackward0>) tensor(1.2786, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.6058, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.5981, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.6058, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5660453 0.0 0.021623533 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.6060, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -28.07169 -0.55680406 27.50681 95.24877\n",
      "activ tensor(5.6060, grad_fn=<CopyBackwards>) tensor(0.0589, grad_fn=<DivBackward0>) tensor(1.0424, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.6140, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.6060, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.6140, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.554965 0.0 0.021253537 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.6140, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -28.224712 -0.54637146 27.352036 95.34753\n",
      "activ tensor(5.6140, grad_fn=<CopyBackwards>) tensor(0.0589, grad_fn=<DivBackward0>) tensor(1.1136, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.6200, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.6140, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.6200, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.562515 0.0 0.021401536 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.6200, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -28.195578 -0.53241813 27.535418 95.44665\n",
      "activ tensor(5.6200, grad_fn=<CopyBackwards>) tensor(0.0589, grad_fn=<DivBackward0>) tensor(1.1916, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.6268, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.6200, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.6268, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56417906 0.0 0.021686958 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.6267, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -28.395151 -0.55374813 27.99846 95.547966\n",
      "activ tensor(5.6267, grad_fn=<CopyBackwards>) tensor(0.0589, grad_fn=<DivBackward0>) tensor(1.2433, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.6332, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.6267, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.6332, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5816664 0.0 0.022426948 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.6332, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -28.71336 -0.5204203 27.87055 95.65221\n",
      "activ tensor(5.6332, grad_fn=<CopyBackwards>) tensor(0.0589, grad_fn=<DivBackward0>) tensor(1.1396, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.6335, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.6332, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.6335, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5723244 0.0 0.021993525 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.6335, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -28.158226 -0.51783305 27.747307 95.76071\n",
      "activ tensor(5.6335, grad_fn=<CopyBackwards>) tensor(0.0588, grad_fn=<DivBackward0>) tensor(1.1693, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.6425, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.6335, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.6425, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.562098 0.0 0.021380391 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.6425, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -28.210186 -0.5424979 27.548616 95.86488\n",
      "activ tensor(5.6425, grad_fn=<CopyBackwards>) tensor(0.0589, grad_fn=<DivBackward0>) tensor(1.1485, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.6414, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.6425, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.6414, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.58508104 0.0 0.022712374 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.6415, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -28.370462 -0.549701 27.479641 95.97052\n",
      "activ tensor(5.6415, grad_fn=<CopyBackwards>) tensor(0.0588, grad_fn=<DivBackward0>) tensor(1.2404, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.6536, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.6415, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.6536, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56438667 0.0 0.02151782 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.6537, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -28.05479 -0.5536126 28.331144 96.077034\n",
      "activ tensor(5.6537, grad_fn=<CopyBackwards>) tensor(0.0588, grad_fn=<DivBackward0>) tensor(1.2657, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.6541, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.6537, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.6541, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5525315 0.0 0.020925827 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.6541, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -28.572487 -0.5431931 28.151808 96.18427\n",
      "activ tensor(5.6541, grad_fn=<CopyBackwards>) tensor(0.0588, grad_fn=<DivBackward0>) tensor(1.1831, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.6499, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.6541, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.6499, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57344943 0.0 0.022067524 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.6499, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -28.24578 -0.5773598 28.142664 96.29054\n",
      "activ tensor(5.6499, grad_fn=<CopyBackwards>) tensor(0.0587, grad_fn=<DivBackward0>) tensor(1.1494, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.6493, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.6499, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.6493, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57263154 0.0 0.021803243 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.6493, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -28.776453 -0.55491406 28.152817 96.39529\n",
      "activ tensor(5.6493, grad_fn=<CopyBackwards>) tensor(0.0586, grad_fn=<DivBackward0>) tensor(1.3530, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.6490, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.6493, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.6490, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5781302 0.0 0.02218381 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.6490, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -29.178354 -0.5553587 27.761065 96.49634\n",
      "activ tensor(5.6490, grad_fn=<CopyBackwards>) tensor(0.0585, grad_fn=<DivBackward0>) tensor(1.3180, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.6501, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.6490, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.6501, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.572836 0.0 0.02210981 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.6501, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -28.759727 -0.5470816 28.48163 96.58863\n",
      "activ tensor(5.6501, grad_fn=<CopyBackwards>) tensor(0.0585, grad_fn=<DivBackward0>) tensor(1.1366, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.6504, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.6501, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.6504, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5721197 0.0 0.022226095 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.6504, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -28.369644 -0.5493487 28.804014 96.676956\n",
      "activ tensor(5.6504, grad_fn=<CopyBackwards>) tensor(0.0584, grad_fn=<DivBackward0>) tensor(1.1202, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.6439, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.6504, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.6439, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55896264 0.0 0.021168968 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.6440, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -28.931597 -0.56363785 28.497648 96.76289\n",
      "activ tensor(5.6440, grad_fn=<CopyBackwards>) tensor(0.0583, grad_fn=<DivBackward0>) tensor(1.2379, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.6473, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.6440, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.6473, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55538714 0.0 0.021179536 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.6474, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -29.339594 -0.5595565 28.10233 96.84048\n",
      "activ tensor(5.6474, grad_fn=<CopyBackwards>) tensor(0.0583, grad_fn=<DivBackward0>) tensor(1.1242, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.6486, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.6474, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.6486, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5722221 0.0 0.021919528 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.6485, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -29.036613 -0.5834567 28.25688 96.92461\n",
      "activ tensor(5.6485, grad_fn=<CopyBackwards>) tensor(0.0583, grad_fn=<DivBackward0>) tensor(1.2184, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.6572, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.6485, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.6572, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5604279 0.0 0.021602392 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.6573, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -28.831074 -0.5642752 28.506132 97.01148\n",
      "activ tensor(5.6573, grad_fn=<CopyBackwards>) tensor(0.0583, grad_fn=<DivBackward0>) tensor(1.1592, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.6609, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.6573, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.6609, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56552756 0.0 0.021655245 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.6609, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -28.71529 -0.5621381 28.39266 97.10092\n",
      "activ tensor(5.6609, grad_fn=<CopyBackwards>) tensor(0.0583, grad_fn=<DivBackward0>) tensor(1.2442, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.6687, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.6609, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.6687, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55843806 0.0 0.02134868 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.6688, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -28.347534 -0.5681386 28.838596 97.1987\n",
      "activ tensor(5.6688, grad_fn=<CopyBackwards>) tensor(0.0583, grad_fn=<DivBackward0>) tensor(1.1885, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.6685, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.6688, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.6685, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57396 0.0 0.022014668 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.6685, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -28.542177 -0.5479341 28.147858 97.30375\n",
      "activ tensor(5.6685, grad_fn=<CopyBackwards>) tensor(0.0583, grad_fn=<DivBackward0>) tensor(1.1991, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.6627, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.6685, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.6627, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5454879 0.0 0.0207144 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.6628, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -28.846416 -0.5744355 27.812445 97.409035\n",
      "activ tensor(5.6628, grad_fn=<CopyBackwards>) tensor(0.0581, grad_fn=<DivBackward0>) tensor(1.2891, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.6694, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.6628, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.6694, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5598004 0.0 0.021412106 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.6695, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -28.588121 -0.55781734 28.433157 97.52023\n",
      "activ tensor(5.6695, grad_fn=<CopyBackwards>) tensor(0.0581, grad_fn=<DivBackward0>) tensor(1.1983, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.6755, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.6695, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.6755, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57904154 0.0 0.022416376 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.6756, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -29.230854 -0.57090664 28.414204 97.63099\n",
      "activ tensor(5.6756, grad_fn=<CopyBackwards>) tensor(0.0581, grad_fn=<DivBackward0>) tensor(1.2632, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.6778, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.6756, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.6778, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5632437 0.0 0.021369822 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.6778, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -29.152262 -0.5557911 28.618395 97.74037\n",
      "activ tensor(5.6778, grad_fn=<CopyBackwards>) tensor(0.0581, grad_fn=<DivBackward0>) tensor(1.1609, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.6942, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.6778, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.6942, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57579464 0.0 0.022078095 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.6942, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -28.848934 -0.5571927 28.278368 97.8498\n",
      "activ tensor(5.6942, grad_fn=<CopyBackwards>) tensor(0.0582, grad_fn=<DivBackward0>) tensor(1.2549, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.7010, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.6942, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.7010, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56417906 0.0 0.021560103 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.7009, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -29.019886 -0.5713607 28.019659 97.95944\n",
      "activ tensor(5.7009, grad_fn=<CopyBackwards>) tensor(0.0582, grad_fn=<DivBackward0>) tensor(1.3877, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.7059, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.7009, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.7059, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57263154 0.0 0.021845529 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.7058, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -29.01044 -0.53995407 28.339468 98.06348\n",
      "activ tensor(5.7058, grad_fn=<CopyBackwards>) tensor(0.0582, grad_fn=<DivBackward0>) tensor(1.1565, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.7121, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.7058, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.7121, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56831807 0.0 0.021824384 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.7121, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -28.950146 -0.53660935 28.725939 98.16275\n",
      "activ tensor(5.7121, grad_fn=<CopyBackwards>) tensor(0.0582, grad_fn=<DivBackward0>) tensor(1.0974, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.7242, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.7121, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.7242, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5628273 0.0 0.021486107 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.7243, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -29.055653 -0.54926413 28.944376 98.259705\n",
      "activ tensor(5.7243, grad_fn=<CopyBackwards>) tensor(0.0583, grad_fn=<DivBackward0>) tensor(1.1035, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.7265, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.7243, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.7265, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5672859 0.0 0.021898385 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.7265, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -28.892557 -0.5582114 28.559916 98.35\n",
      "activ tensor(5.7265, grad_fn=<CopyBackwards>) tensor(0.0582, grad_fn=<DivBackward0>) tensor(1.2210, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.7321, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.7265, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.7321, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5714024 0.0 0.021729246 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.7321, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -29.357952 -0.5529692 28.725786 98.43731\n",
      "activ tensor(5.7321, grad_fn=<CopyBackwards>) tensor(0.0582, grad_fn=<DivBackward0>) tensor(1.2938, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.7389, grad_fn=<CopyBackwards>)\n",
      "input tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])\n",
      "qinput tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.7321, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.7389, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56852394 0.0 0.021866672 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.7389, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -29.220423 -0.5687605 28.03399 97.57342\n",
      "activ tensor(5.7389, grad_fn=<CopyBackwards>) tensor(0.0588, grad_fn=<DivBackward0>) tensor(1.1351, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.7382, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.7389, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.7382, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56084585 0.0 0.021486104 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.7383, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -28.813316 -0.58092046 28.647236 97.656075\n",
      "activ tensor(5.7383, grad_fn=<CopyBackwards>) tensor(0.0588, grad_fn=<DivBackward0>) tensor(1.0986, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.7481, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.7383, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.7481, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57150507 0.0 0.021930099 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.7481, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -28.466095 -0.57004285 29.406307 97.74718\n",
      "activ tensor(5.7481, grad_fn=<CopyBackwards>) tensor(0.0588, grad_fn=<DivBackward0>) tensor(1.2530, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.7567, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.7481, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.7567, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5590673 0.0 0.021242967 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.7567, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -29.326536 -0.58684933 28.387426 97.84361\n",
      "activ tensor(5.7567, grad_fn=<CopyBackwards>) tensor(0.0588, grad_fn=<DivBackward0>) tensor(1.3334, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.7637, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.7567, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.7637, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55210716 0.0 0.021052683 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.7636, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -29.519531 -0.5765427 28.494928 97.947\n",
      "activ tensor(5.7636, grad_fn=<CopyBackwards>) tensor(0.0588, grad_fn=<DivBackward0>) tensor(1.1581, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.7649, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.7636, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.7649, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55812305 0.0 0.021147823 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.7649, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -29.414982 -0.55034775 28.960533 98.05183\n",
      "activ tensor(5.7649, grad_fn=<CopyBackwards>) tensor(0.0588, grad_fn=<DivBackward0>) tensor(1.1295, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.7686, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.7649, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.7686, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.58206904 0.0 0.02251152 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.7687, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -29.257252 -0.5981519 28.38383 98.15978\n",
      "activ tensor(5.7687, grad_fn=<CopyBackwards>) tensor(0.0588, grad_fn=<DivBackward0>) tensor(1.4343, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.7816, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.7687, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.7816, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5885757 0.0 0.022659516 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.7817, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -29.014647 -0.6254648 29.34287 98.26487\n",
      "activ tensor(5.7817, grad_fn=<CopyBackwards>) tensor(0.0588, grad_fn=<DivBackward0>) tensor(1.1495, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.7828, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.7817, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.7828, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5687302 0.0 0.021845527 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.7828, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -28.808094 -0.60747796 29.017014 98.37156\n",
      "activ tensor(5.7828, grad_fn=<CopyBackwards>) tensor(0.0588, grad_fn=<DivBackward0>) tensor(1.3759, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.7904, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.7828, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.7904, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5632436 0.0 0.021581247 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.7904, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -29.44837 -0.5895923 28.472818 98.48292\n",
      "activ tensor(5.7904, grad_fn=<CopyBackwards>) tensor(0.0588, grad_fn=<DivBackward0>) tensor(1.2297, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.7921, grad_fn=<CopyBackwards>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "model_float = models[0]\n",
    "model_scaled = models[1]\n",
    "hidden = []\n",
    "hidden_scaled = []\n",
    "hidden_scaled_input = []\n",
    "hidden_scaled_quant = []\n",
    "hidden_scaled_rec = []\n",
    "for ii in range(1000):\n",
    "    print(\"input\",x0[0:1,ii])\n",
    "    print(\"qinput\",model_scaled.quant_input(x0[0:1,ii])/input_pre_scaling_factor)\n",
    "    #print(model.input_layer(x0[0:1,ii]))\n",
    "    #print(model_scaled.input_layer(x0[0:1,ii])*saved_quant_input_max_absolute*saved_alpha_u)\n",
    "    print(\"diff U.x\",torch.norm(model_float.input_layer(x0[0:1,ii])-model_scaled.input_layer(model_scaled.quant_input(x0[0:1,ii]))/input_pre_scaling_factor))\n",
    "    \n",
    "    if len(hidden)==0:\n",
    "        hidden.append(model_float.input_layer(x0[0:1,ii]))\n",
    "        hidden_scaled.append(model_scaled.input_layer(model_scaled.quant_input(x0[0:1,ii])))\n",
    "        #print(hidden_scaled[0])\n",
    "        #print(hidden_scaled[0]*2**(config.model['num_bits']))\n",
    "        hidden_scaled_quant.append(torch.zeros_like(hidden_scaled[0])) # fake\n",
    "        hidden_scaled_rec.append(torch.zeros_like(hidden_scaled[0])) # fake\n",
    "    else:\n",
    "        #print(\"hidden\",hidden[-1][0,0:10])\n",
    "        #print(\"hidden_rec\",model.recurrent_layer(hidden[-1])[0,0:10])\n",
    "        hidden.append(model_float.input_layer(x0[0:1,ii])+model_float.recurrent_layer(hidden[-1]))\n",
    "        #print(\"hidden_scaled\",hidden_scaled[-1][0,0:10])\n",
    "        #print(\"hidden_scaled\",hidden_scaled[-1][0,0:10]*saved_alpha_u/input_pre_scaling_factor)\n",
    "        hidden_scaled_quant.append(model_scaled.quant_feat(hidden_scaled[-1]))\n",
    "        #print(\"hidden_scaled_quant\",hidden_scaled_quant[-1][0,0:10])\n",
    "        #print(\"hidden_scaled_quant\",hidden_scaled_quant[-1][0,0:10]*saved_alpha_u/(input_pre_scaling_factor*model_scaled.quant_feat.pre_scaling_factor))\n",
    "        print(\"diff_hidden_scaled\",torch.norm(hidden[-2]-hidden_scaled[-1]/input_pre_scaling_factor))\n",
    "        print(\"diff_hidden_quant\",torch.norm(hidden[-2]-hidden_scaled_quant[-1]/(input_pre_scaling_factor*model_scaled.quant_feat.pre_scaling_factor)))\n",
    "        diff_hidden_scaled = (hidden_scaled_quant[-1]/model_scaled.quant_feat.pre_scaling_factor-hidden_scaled[-1])*saved_alpha_u/input_pre_scaling_factor\n",
    "        print(\"quant diff hidden\",torch.norm(diff_hidden_scaled).detach().numpy(),torch.min(torch.abs(diff_hidden_scaled)).detach().numpy(),torch.mean(torch.abs(diff_hidden_scaled)).detach().numpy(),torch.max(torch.abs(diff_hidden_scaled)).detach().numpy(),diff_hidden_scaled.shape)\n",
    "        #print(\"hidden_scaled_reccccc\",model.recurrent_layer(hidden_scaled_quant[-1]*saved_alpha_u/(input_pre_scaling_factor*model_scaled.quant_feat.pre_scaling_factor))[0,0:10])\n",
    "        #print(\"hidden_scaled_reccccc1\",model.recurrent_layer(hidden_scaled_quant[-1]/model_scaled.quant_feat.pre_scaling_factor)[0,0:10]*saved_alpha_u/input_pre_scaling_factor)\n",
    "        #print(\"hidden_scaled_rec\",model_scaled.recurrent_layer(hidden_scaled_quant[-1])[0,0:10]*saved_alpha_u/input_pre_scaling_factor)\n",
    "        #print(\"hidden_scaled_rec\",model_scaled.recurrent_layer(model_scaled.quant_feat(hidden_scaled[-1]))[0,0:10]*saved_alpha_u/input_pre_scaling_factor)\n",
    "        hidden_scaled.append(model_scaled.input_layer(model_scaled.quant_input(x0[0:1,ii]))+model_scaled.recurrent_layer(model_scaled.quant_feat(hidden_scaled[-1])))\n",
    "        hidden_scaled_rec.append(model_scaled.recurrent_layer(model_scaled.quant_feat(hidden_scaled[-1]))) \n",
    "    \n",
    "    hidden_scaled_input.append(model_scaled.input_layer(model_scaled.quant_input(x0[0:1,ii])))\n",
    "    #print(\"hidden_scaled_rec\",hidden_scaled_rec[-1]*saved_alpha_u/input_pre_scaling_factor)\n",
    "    print(\"diff h_t\",torch.norm(hidden[-1]-hidden_scaled[-1]/input_pre_scaling_factor))\n",
    "    print(\"h_t, min,mean,max,norm\",hidden[-1].min().detach().numpy(),hidden[-1].mean().detach().numpy(),hidden[-1].max().detach().numpy(),torch.norm(hidden[-1]).detach().numpy())\n",
    "    #print(\"diff h_t\",hidden[-1][0,0:10])\n",
    "    #print(\"diff h_t\",hidden_scaled[-1][0,0:10])\n",
    "    #print(\"diff h_t\",hidden_scaled[-1][0,0:10]*saved_alpha_u/input_pre_scaling_factor)\n",
    "    #if ii>2:\n",
    "    #    aaa\n",
    "    if 'activation.b' in new_weights:\n",
    "        print(\"diff h_t+bias\",torch.norm(hidden[-1]+model.activation.b-(hidden_scaled[-1]+model_scaled.activation.b)*saved_alpha_u/input_pre_scaling_factor))\n",
    "    #NO ACTIV IN SSM hidden[-1] = model.activation(hidden[-1])\n",
    "    #NO ACTIV IN SSM hidden_scaled[-1] = model_scaled.activation(hidden_scaled[-1])\n",
    "    print(\"activ\",torch.norm(hidden[-1]-hidden_scaled[-1]/input_pre_scaling_factor),torch.norm(hidden[-1]-hidden_scaled[-1]/input_pre_scaling_factor)/torch.norm(hidden[-1]),torch.max(torch.abs(hidden[-1]-hidden_scaled[-1]/input_pre_scaling_factor)))\n",
    "    #print(\"activ value\",hidden[-1],hidden_scaled[-1]*saved_alpha_u/input_pre_scaling_factor)\n",
    "    #print(\"activ valueq\",hidden_scaled[-1],model_scaled.quant_feat(hidden_scaled[-1])/model_scaled.quant_feat.pre_scaling_factor)\n",
    "    print(\"activ quantif\",torch.norm(hidden[-1]-model_scaled.quant_feat(hidden_scaled[-1])/model_scaled.quant_feat.pre_scaling_factor/input_pre_scaling_factor))\n",
    "    '''if torch.max(torch.abs(hidden[-1]-model_scaled.quant_feat(hidden_scaled[-1])/model_scaled.quant_feat.pre_scaling_factor/input_pre_scaling_factor))>0.1:\n",
    "        print(hidden[-1][0,290:300])\n",
    "        print(hidden_scaled[-1][0,290:300]/input_pre_scaling_factor)\n",
    "        print(model_scaled.quant_feat(hidden_scaled[-1])[0,290:300]/model_scaled.quant_feat.pre_scaling_factor/input_pre_scaling_factor)\n",
    "        diff = hidden[-1]-model_scaled.quant_feat(hidden_scaled[-1])/model_scaled.quant_feat.pre_scaling_factor/input_pre_scaling_factor\n",
    "        print(torch.argmax(torch.abs(diff)))\n",
    "        print(\"diff\",diff[0,290:300],diff.min().detach().numpy(),diff.mean().detach().numpy(),diff.max().detach().numpy(),torch.norm(diff).detach().numpy())\n",
    "        break'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'training': True,\n",
       " '_parameters': OrderedDict(),\n",
       " '_buffers': OrderedDict(),\n",
       " '_non_persistent_buffers_set': set(),\n",
       " '_backward_hooks': OrderedDict(),\n",
       " '_is_full_backward_hook': None,\n",
       " '_forward_hooks': OrderedDict(),\n",
       " '_forward_pre_hooks': OrderedDict(),\n",
       " '_state_dict_hooks': OrderedDict(),\n",
       " '_load_state_dict_pre_hooks': OrderedDict(),\n",
       " '_modules': OrderedDict(),\n",
       " 'num_bits': 30,\n",
       " 'enforce_true_quantized': True,\n",
       " 'delta_m': None,\n",
       " 'max_absolute': 8.0,\n",
       " 'stat_max_absolute': 1e-10,\n",
       " 'pre_scaling_factor': 0.25}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_scaled.quant_feat.__dict__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None {'test_loss': 9.902935005356994e-08}\n",
      "12 {'test_loss': 2.426349965389818e-05}\n",
      "None {'test_loss': 9.902935005356994e-08}\n",
      "12 {'test_loss': 2.426349965389818e-05}\n"
     ]
    }
   ],
   "source": [
    "from extra_layers import _Quantize_stats\n",
    "results = {}\n",
    "for qb in [None,12]:\n",
    "    config_scaled.model['num_bits_feat'] = qb #10 #12\n",
    "    model_scaled = make_model(**config_scaled.model).to(device)\n",
    "    if qb is None:   # No rescaling of h with qb=None\n",
    "        new_weights['recurrent_layer.weight'] = float_recurrent_weight\n",
    "        #model_scaled.load_state_dict(new_weights_normalphaU)\n",
    "    else:\n",
    "        new_weights['recurrent_layer.weight'] = requant_recurrent_weight\n",
    "    model_scaled.load_state_dict(new_weights)\n",
    "    if qb is not None:      \n",
    "        model_scaled.quant_input.pre_scaling_factor = input_pre_scaling_factor     \n",
    "        model_scaled.quant_input.max_absolute = saved_quant_input_max_absolute #saved_quant_input_max_absolute  ## set it twice to get 1 as max\n",
    "        model_scaled.quant_feat.max_absolute = new_quant_feat_max_absolute       \n",
    "        model_scaled.quant_feat.pre_scaling_factor = saved_alpha_w     \n",
    "        #print(model_scaled.activation.b)\n",
    "        #print(quant_bias(model_scaled.activation.b))\n",
    "        if ('activation.b' in new_weights) or ('recurrent_layer.bias' in new_weights):\n",
    "            #quantize biases\n",
    "            quant_bias = _Quantize_stats(weight=None,num_bits= model_scaled.quant_feat.num_bits, max_absolute= new_bias_max_absolute)\n",
    "            if 'activation.b' in new_weights:\n",
    "                model_scaled.activation.b.data = quant_bias(model_scaled.activation.b) \n",
    "            if 'recurrent_layer.bias' in new_weights:\n",
    "                model_scaled.recurrent_layer.bias.data = quant_bias(model_scaled.recurrent_layer.bias)\n",
    "    stat_test = evaluate(test_ds, test_batch_size, model_scaled, loss_fn=config.train['loss_fn'], metrics=config.train['metrics'], kind='test', torch_device=device)\n",
    "    print(qb, stat_test)\n",
    "    results[qb] =stat_test.copy()\n",
    "\n",
    "for q in results:\n",
    "    print(q,results[q])\n",
    "#save perf in txt file\n",
    "with open(\"results/\"+expe_name+\"_\"+sweep_name+\"_\"+short2fullname[expe_name]+\"_quant.txt\", \"w\") as f:\n",
    "    for q in results:\n",
    "        for kk in results[q]:\n",
    "            f.write(str(q)+\";\"+str(kk)+\";\"+str(results[q][kk])+\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verification of the intermediate computation: chek for quantization\n",
    "\n",
    "need to simulate the QRNN computation and save intermediate values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_scaled.quant_feat.num_bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1000, 10])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,y = next(iter(test_ds))\n",
    "x0 = x[None,:] #No batch_size [0:1]\n",
    "x0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'activation.b' in new_weights:\n",
    "    model.activation.b/ saved_alpha_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if 'activation.b' in new_weights:\n",
    "    model_scaled.activation.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1000, 10])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saved_quant_input_max_absolute\n",
    "input_pre_scaling_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff h_t tensor(0.0179, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -3.2515633 -0.0084897615 3.7618876 16.906162\n",
      "activ tensor(0.0179, grad_fn=<CopyBackwards>) tensor(0.0011, grad_fn=<DivBackward0>) tensor(0.0014, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(0.5847, grad_fn=<CopyBackwards>)\n",
      "input tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]])\n",
      "qinput tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(0.0179, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(0.5847, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5847806 0.0 0.022479804 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(0.5846, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -4.996379 -0.015067793 5.690657 25.236826\n",
      "activ tensor(0.5846, grad_fn=<CopyBackwards>) tensor(0.0232, grad_fn=<DivBackward0>) tensor(0.0821, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(0.8089, grad_fn=<CopyBackwards>)\n",
      "input tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(0.5846, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(0.8089, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.571505 0.0 0.021824388 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(0.8105, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -5.5541286 0.00019678939 5.37095 32.77709\n",
      "activ tensor(0.8105, grad_fn=<CopyBackwards>) tensor(0.0247, grad_fn=<DivBackward0>) tensor(0.1078, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(0.9990, grad_fn=<CopyBackwards>)\n",
      "input tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(0.8105, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(0.9990, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.54473555 0.0 0.020672116 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(0.9997, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -5.924661 -0.008609988 8.867098 39.42515\n",
      "activ tensor(0.9997, grad_fn=<CopyBackwards>) tensor(0.0254, grad_fn=<DivBackward0>) tensor(0.1442, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.1470, grad_fn=<CopyBackwards>)\n",
      "input tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]])\n",
      "qinput tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(0.9997, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.1470, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5626191 0.0 0.021369822 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.1471, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -6.119531 0.014562144 7.7276983 43.90282\n",
      "activ tensor(1.1471, grad_fn=<CopyBackwards>) tensor(0.0261, grad_fn=<DivBackward0>) tensor(0.1845, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.2671, grad_fn=<CopyBackwards>)\n",
      "input tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]])\n",
      "qinput tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.1471, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.2671, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55274355 0.0 0.020957543 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.2670, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -7.830131 0.024079122 8.932483 48.70744\n",
      "activ tensor(1.2670, grad_fn=<CopyBackwards>) tensor(0.0260, grad_fn=<DivBackward0>) tensor(0.2361, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.3839, grad_fn=<CopyBackwards>)\n",
      "input tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.2670, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.3839, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5565464 0.0 0.021232395 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.3840, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -8.821525 0.007655738 10.5328 52.82539\n",
      "activ tensor(1.3840, grad_fn=<CopyBackwards>) tensor(0.0262, grad_fn=<DivBackward0>) tensor(0.1909, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.5042, grad_fn=<CopyBackwards>)\n",
      "input tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.3840, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.5042, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56594175 0.0 0.021676388 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.5040, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -8.060261 0.024535337 9.130448 55.785694\n",
      "activ tensor(1.5040, grad_fn=<CopyBackwards>) tensor(0.0270, grad_fn=<DivBackward0>) tensor(0.2158, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.6356, grad_fn=<CopyBackwards>)\n",
      "input tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.5040, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.6356, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5667692 0.0 0.02169753 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.6372, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -10.04917 0.04515076 9.91796 57.441044\n",
      "activ tensor(1.6372, grad_fn=<CopyBackwards>) tensor(0.0285, grad_fn=<DivBackward0>) tensor(0.2810, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.7252, grad_fn=<CopyBackwards>)\n",
      "input tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.6372, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.7252, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.54225606 0.0 0.020555831 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.7254, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -13.786602 0.14106068 9.921342 59.416344\n",
      "activ tensor(1.7254, grad_fn=<CopyBackwards>) tensor(0.0290, grad_fn=<DivBackward0>) tensor(0.3083, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.8142, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.7254, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.8142, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57883906 0.0 0.022046383 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.8149, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -11.322305 0.3397061 10.739449 59.16346\n",
      "activ tensor(1.8149, grad_fn=<CopyBackwards>) tensor(0.0307, grad_fn=<DivBackward0>) tensor(0.2197, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.8811, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.8149, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.8811, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5602188 0.0 0.021422677 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.8809, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -13.056919 0.086471684 9.465958 58.336693\n",
      "activ tensor(1.8809, grad_fn=<CopyBackwards>) tensor(0.0322, grad_fn=<DivBackward0>) tensor(0.2979, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(1.9477, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.8809, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(1.9477, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5666659 0.0 0.021856098 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(1.9490, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -13.171631 0.024143051 9.2398615 57.43986\n",
      "activ tensor(1.9490, grad_fn=<CopyBackwards>) tensor(0.0339, grad_fn=<DivBackward0>) tensor(0.2928, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.0456, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(1.9490, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.0456, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5667694 0.0 0.021792673 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.0460, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -9.614121 0.1016705 8.709832 56.210064\n",
      "activ tensor(2.0460, grad_fn=<CopyBackwards>) tensor(0.0364, grad_fn=<DivBackward0>) tensor(0.2661, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.0975, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.0460, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.0975, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56334776 0.0 0.021708101 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.0976, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -9.030756 0.047837306 8.528063 55.332645\n",
      "activ tensor(2.0976, grad_fn=<CopyBackwards>) tensor(0.0379, grad_fn=<DivBackward0>) tensor(0.3612, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.1723, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.0976, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.1723, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5854815 0.0 0.022744088 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.1730, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -9.200963 0.009178776 11.003518 54.500458\n",
      "activ tensor(2.1730, grad_fn=<CopyBackwards>) tensor(0.0399, grad_fn=<DivBackward0>) tensor(0.3037, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.2635, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.1730, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.2635, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5617856 0.0 0.021570677 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.2636, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -8.442798 0.02332294 9.719771 53.91052\n",
      "activ tensor(2.2636, grad_fn=<CopyBackwards>) tensor(0.0420, grad_fn=<DivBackward0>) tensor(0.3293, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.3562, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.2636, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.3562, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5770145 0.0 0.022310665 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.3557, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -11.839392 0.024984792 11.4719715 53.472126\n",
      "activ tensor(2.3557, grad_fn=<CopyBackwards>) tensor(0.0441, grad_fn=<DivBackward0>) tensor(0.4082, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.4890, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.3557, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.4890, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5814648 0.0 0.022405807 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.4897, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -8.6434 0.023567405 7.9740896 53.45992\n",
      "activ tensor(2.4897, grad_fn=<CopyBackwards>) tensor(0.0466, grad_fn=<DivBackward0>) tensor(0.3451, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.5482, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.4897, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.5482, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5603234 0.0 0.021517817 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.5488, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -9.010622 0.009869492 7.979958 53.513943\n",
      "activ tensor(2.5488, grad_fn=<CopyBackwards>) tensor(0.0476, grad_fn=<DivBackward0>) tensor(0.4326, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.6581, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.5488, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.6581, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55369663 0.0 0.020978684 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.6578, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -8.005309 0.0900328 9.946439 53.59429\n",
      "activ tensor(2.6578, grad_fn=<CopyBackwards>) tensor(0.0496, grad_fn=<DivBackward0>) tensor(0.4247, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.7117, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.6578, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.7117, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5569674 0.0 0.02104211 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.7120, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -8.7957735 -0.18539123 9.305685 53.554295\n",
      "activ tensor(2.7120, grad_fn=<CopyBackwards>) tensor(0.0506, grad_fn=<DivBackward0>) tensor(0.4166, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.7618, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.7120, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.7618, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5674928 0.0 0.021708103 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.7613, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -9.2899685 -0.049593158 9.31025 53.652687\n",
      "activ tensor(2.7613, grad_fn=<CopyBackwards>) tensor(0.0515, grad_fn=<DivBackward0>) tensor(0.4095, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.8690, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.7613, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.8690, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5624109 0.0 0.02151782 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.8684, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -8.945 -0.0068795243 9.4803095 54.039272\n",
      "activ tensor(2.8684, grad_fn=<CopyBackwards>) tensor(0.0531, grad_fn=<DivBackward0>) tensor(0.4647, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.9287, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.8684, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.9287, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57140225 0.0 0.022046383 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.9288, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -7.280272 0.005713595 7.613018 54.68884\n",
      "activ tensor(2.9288, grad_fn=<CopyBackwards>) tensor(0.0536, grad_fn=<DivBackward0>) tensor(0.4311, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(2.9837, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.9288, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(2.9837, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5559144 0.0 0.021422677 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(2.9842, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -8.514194 0.13629644 10.937071 55.203617\n",
      "activ tensor(2.9842, grad_fn=<CopyBackwards>) tensor(0.0541, grad_fn=<DivBackward0>) tensor(0.4956, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.0679, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(2.9842, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.0679, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5793451 0.0 0.02227895 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.0686, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -9.150677 0.108577386 13.281797 55.65989\n",
      "activ tensor(3.0686, grad_fn=<CopyBackwards>) tensor(0.0551, grad_fn=<DivBackward0>) tensor(0.4705, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.1376, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.0686, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.1376, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5635556 0.0 0.021454392 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.1378, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -9.459226 0.06912605 10.152302 56.094116\n",
      "activ tensor(3.1378, grad_fn=<CopyBackwards>) tensor(0.0559, grad_fn=<DivBackward0>) tensor(0.4461, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.2111, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.1378, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.2111, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56676924 0.0 0.021739816 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.2121, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -9.44736 0.0015636384 12.697191 56.69116\n",
      "activ tensor(3.2121, grad_fn=<CopyBackwards>) tensor(0.0567, grad_fn=<DivBackward0>) tensor(0.5142, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.2509, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.2121, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.2509, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5511513 0.0 0.020915257 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.2508, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -9.781216 0.084163085 11.146384 57.26782\n",
      "activ tensor(3.2508, grad_fn=<CopyBackwards>) tensor(0.0568, grad_fn=<DivBackward0>) tensor(0.7250, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.3286, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.2508, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.3286, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57620144 0.0 0.022004096 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.3284, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -9.3477955 0.09999633 9.018244 57.998554\n",
      "activ tensor(3.3284, grad_fn=<CopyBackwards>) tensor(0.0574, grad_fn=<DivBackward0>) tensor(0.4756, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.4459, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.3284, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.4459, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5627233 0.0 0.021316966 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.4472, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -13.112688 -0.19063479 8.586685 58.59805\n",
      "activ tensor(3.4472, grad_fn=<CopyBackwards>) tensor(0.0588, grad_fn=<DivBackward0>) tensor(0.5233, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.4973, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.4472, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.4973, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.58085996 0.0 0.022374092 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.4975, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -11.427676 -0.023908999 10.495446 59.471657\n",
      "activ tensor(3.4975, grad_fn=<CopyBackwards>) tensor(0.0588, grad_fn=<DivBackward0>) tensor(0.4747, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.5703, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.4975, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.5703, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.54656106 0.0 0.020883542 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.5711, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -11.462356 0.0012452602 12.187377 60.13712\n",
      "activ tensor(3.5711, grad_fn=<CopyBackwards>) tensor(0.0594, grad_fn=<DivBackward0>) tensor(0.6371, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.5888, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.5711, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.5888, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5642829 0.0 0.021655247 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.5892, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -13.980494 0.088190235 8.970385 60.66982\n",
      "activ tensor(3.5892, grad_fn=<CopyBackwards>) tensor(0.0592, grad_fn=<DivBackward0>) tensor(0.5559, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.6503, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.5892, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.6503, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5453803 0.0 0.020830685 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.6500, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -11.173524 -0.035923723 9.613962 60.7242\n",
      "activ tensor(3.6500, grad_fn=<CopyBackwards>) tensor(0.0601, grad_fn=<DivBackward0>) tensor(0.5095, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.7115, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.6500, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.7115, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5697594 0.0 0.021686962 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.7122, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -10.2431135 -0.10789022 9.404529 60.406532\n",
      "activ tensor(3.7122, grad_fn=<CopyBackwards>) tensor(0.0615, grad_fn=<DivBackward0>) tensor(0.5853, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.7671, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.7122, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.7671, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5531673 0.0 0.021042112 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.7675, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -9.77657 -0.09316389 9.730084 59.77995\n",
      "activ tensor(3.7675, grad_fn=<CopyBackwards>) tensor(0.0630, grad_fn=<DivBackward0>) tensor(0.5825, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.7810, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.7675, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.7810, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57058144 0.0 0.021665815 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.7812, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -9.551164 -0.13902482 8.799842 59.091324\n",
      "activ tensor(3.7812, grad_fn=<CopyBackwards>) tensor(0.0640, grad_fn=<DivBackward0>) tensor(0.5486, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.8130, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.7812, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.8130, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5479527 0.0 0.020724975 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.8121, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -10.365722 0.014961262 8.695421 58.415997\n",
      "activ tensor(3.8121, grad_fn=<CopyBackwards>) tensor(0.0653, grad_fn=<DivBackward0>) tensor(0.7277, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.8697, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.8121, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.8697, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5662525 0.0 0.021591818 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.8702, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -10.578447 0.040178843 8.667168 57.570595\n",
      "activ tensor(3.8702, grad_fn=<CopyBackwards>) tensor(0.0672, grad_fn=<DivBackward0>) tensor(0.5188, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.8864, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.8702, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.8864, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56625223 0.0 0.021380393 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.8868, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -8.630501 -0.0774035 8.56899 56.74066\n",
      "activ tensor(3.8868, grad_fn=<CopyBackwards>) tensor(0.0685, grad_fn=<DivBackward0>) tensor(0.5623, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.9661, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.8868, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.9661, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.586881 0.0 0.02252209 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.9659, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -8.251362 0.058737826 8.2659855 56.201523\n",
      "activ tensor(3.9659, grad_fn=<CopyBackwards>) tensor(0.0706, grad_fn=<DivBackward0>) tensor(0.5840, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(3.9997, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.9659, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(3.9997, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5482735 0.0 0.020682689 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(3.9997, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -9.325998 -0.033568017 8.695584 55.776093\n",
      "activ tensor(3.9997, grad_fn=<CopyBackwards>) tensor(0.0717, grad_fn=<DivBackward0>) tensor(0.7004, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.0262, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(3.9997, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.0262, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55759823 0.0 0.021338107 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.0274, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -8.608928 -0.023950249 8.021122 55.868763\n",
      "activ tensor(4.0274, grad_fn=<CopyBackwards>) tensor(0.0721, grad_fn=<DivBackward0>) tensor(0.5419, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.0489, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.0274, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.0489, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55443686 0.0 0.02103154 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.0499, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -9.539892 -0.07284282 10.8131075 55.969994\n",
      "activ tensor(4.0499, grad_fn=<CopyBackwards>) tensor(0.0724, grad_fn=<DivBackward0>) tensor(0.5899, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.0520, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.0499, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.0520, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56334764 0.0 0.02143325 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.0517, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -8.099396 -0.09108132 8.491911 56.540215\n",
      "activ tensor(4.0517, grad_fn=<CopyBackwards>) tensor(0.0717, grad_fn=<DivBackward0>) tensor(0.6108, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.1172, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.0517, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.1172, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55885756 0.0 0.021295823 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.1176, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -9.495515 -0.13171008 9.31448 57.440453\n",
      "activ tensor(4.1176, grad_fn=<CopyBackwards>) tensor(0.0717, grad_fn=<DivBackward0>) tensor(0.6559, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.1662, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.1176, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.1662, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5851813 0.0 0.022574946 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.1671, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -11.745201 -0.13940722 12.484458 58.833946\n",
      "activ tensor(4.1671, grad_fn=<CopyBackwards>) tensor(0.0708, grad_fn=<DivBackward0>) tensor(0.6832, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.2023, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.1671, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.2023, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5613682 0.0 0.02127468 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.2018, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -9.217275 -0.014176905 11.697317 60.286713\n",
      "activ tensor(4.2018, grad_fn=<CopyBackwards>) tensor(0.0697, grad_fn=<DivBackward0>) tensor(0.7724, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.2236, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.2018, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.2236, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57752186 0.0 0.022067524 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.2253, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -10.434014 0.054722436 12.097019 61.607323\n",
      "activ tensor(4.2253, grad_fn=<CopyBackwards>) tensor(0.0686, grad_fn=<DivBackward0>) tensor(0.6150, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.2426, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.2253, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.2426, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55040663 0.0 0.020735545 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.2417, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -11.055539 -0.12456401 12.401929 62.675503\n",
      "activ tensor(4.2417, grad_fn=<CopyBackwards>) tensor(0.0677, grad_fn=<DivBackward0>) tensor(0.7005, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.2888, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.2417, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.2888, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56563115 0.0 0.021739814 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.2900, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -9.588211 -0.015941232 9.955862 63.48085\n",
      "activ tensor(4.2900, grad_fn=<CopyBackwards>) tensor(0.0676, grad_fn=<DivBackward0>) tensor(0.6027, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.3472, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.2900, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.3472, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56365967 0.0 0.021443818 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.3473, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -11.660644 -0.10365269 11.200827 64.004616\n",
      "activ tensor(4.3473, grad_fn=<CopyBackwards>) tensor(0.0679, grad_fn=<DivBackward0>) tensor(0.8424, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.3746, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.3473, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.3746, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5565464 0.0 0.021179538 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.3753, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -13.061817 -0.06550903 9.389543 64.512535\n",
      "activ tensor(4.3753, grad_fn=<CopyBackwards>) tensor(0.0678, grad_fn=<DivBackward0>) tensor(0.5728, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.4136, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.3753, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.4136, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5640752 0.0 0.021665815 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.4135, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -14.621598 -0.09638028 11.837396 64.72868\n",
      "activ tensor(4.4135, grad_fn=<CopyBackwards>) tensor(0.0682, grad_fn=<DivBackward0>) tensor(0.6680, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.4699, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.4135, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.4699, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56780225 0.0 0.021507248 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.4707, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -13.1454 -0.13527496 9.956971 64.837585\n",
      "activ tensor(4.4707, grad_fn=<CopyBackwards>) tensor(0.0690, grad_fn=<DivBackward0>) tensor(0.6258, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.5080, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.4707, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.5080, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5506193 0.0 0.0209364 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.5080, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -12.5720825 0.012219273 10.877924 64.774605\n",
      "activ tensor(4.5080, grad_fn=<CopyBackwards>) tensor(0.0696, grad_fn=<DivBackward0>) tensor(0.6806, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.5404, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.5080, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.5404, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55896246 0.0 0.021327535 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.5410, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -11.8317795 -0.042579446 10.0854435 64.50317\n",
      "activ tensor(4.5410, grad_fn=<CopyBackwards>) tensor(0.0704, grad_fn=<DivBackward0>) tensor(0.6766, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.5913, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.5410, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.5913, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.560637 0.0 0.02134868 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.5905, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -11.593482 -0.061948553 12.321003 64.13549\n",
      "activ tensor(4.5905, grad_fn=<CopyBackwards>) tensor(0.0716, grad_fn=<DivBackward0>) tensor(0.9443, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.6644, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.5905, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.6644, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5689362 0.0 0.0218561 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.6652, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -12.852515 -0.09204277 10.324708 63.823784\n",
      "activ tensor(4.6652, grad_fn=<CopyBackwards>) tensor(0.0731, grad_fn=<DivBackward0>) tensor(0.7317, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.7340, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.6652, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.7340, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.59461707 0.0 0.02301894 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.7349, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -13.847522 -0.20587091 10.90604 63.113945\n",
      "activ tensor(4.7349, grad_fn=<CopyBackwards>) tensor(0.0750, grad_fn=<DivBackward0>) tensor(0.8150, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.7569, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.7349, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.7569, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5802545 0.0 0.02252209 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.7580, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -11.936686 -0.09487595 11.014002 62.617023\n",
      "activ tensor(4.7580, grad_fn=<CopyBackwards>) tensor(0.0760, grad_fn=<DivBackward0>) tensor(0.6618, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.8133, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.7580, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.8133, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5638674 0.0 0.02143325 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.8145, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -12.670195 -0.2490007 10.158628 62.318344\n",
      "activ tensor(4.8145, grad_fn=<CopyBackwards>) tensor(0.0773, grad_fn=<DivBackward0>) tensor(0.9586, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.8365, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.8145, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.8365, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5701706 0.0 0.021739816 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.8362, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -11.608492 -0.088366345 11.52231 62.26463\n",
      "activ tensor(4.8362, grad_fn=<CopyBackwards>) tensor(0.0777, grad_fn=<DivBackward0>) tensor(0.7148, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.8564, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.8362, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.8564, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5398738 0.0 0.02053469 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.8555, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -12.003749 -0.14778543 10.122395 62.36387\n",
      "activ tensor(4.8555, grad_fn=<CopyBackwards>) tensor(0.0779, grad_fn=<DivBackward0>) tensor(0.7653, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.8887, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.8555, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.8887, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5552817 0.0 0.021063255 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.8884, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -12.296307 -0.06850482 9.264681 62.70272\n",
      "activ tensor(4.8884, grad_fn=<CopyBackwards>) tensor(0.0780, grad_fn=<DivBackward0>) tensor(0.7913, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.9425, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.8884, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.9425, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5718122 0.0 0.022109807 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.9426, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -11.5426 -0.08701256 9.9171915 62.841118\n",
      "activ tensor(4.9426, grad_fn=<CopyBackwards>) tensor(0.0787, grad_fn=<DivBackward0>) tensor(1.0055, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.9986, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.9426, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.9986, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.58518136 0.0 0.022670086 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.9998, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -13.65731 -0.22233237 11.559502 63.12379\n",
      "activ tensor(4.9998, grad_fn=<CopyBackwards>) tensor(0.0792, grad_fn=<DivBackward0>) tensor(0.8074, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(4.9901, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.9998, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(4.9901, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.54998046 0.0 0.020724973 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(4.9897, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -14.11887 -0.062564805 11.382815 63.721416\n",
      "activ tensor(4.9897, grad_fn=<CopyBackwards>) tensor(0.0783, grad_fn=<DivBackward0>) tensor(0.9405, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.0512, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(4.9897, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.0512, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56397146 0.0 0.021306394 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.0516, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -11.18696 -0.13280036 9.465702 64.494995\n",
      "activ tensor(5.0516, grad_fn=<CopyBackwards>) tensor(0.0783, grad_fn=<DivBackward0>) tensor(0.7935, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.1215, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.0516, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.1215, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5583331 0.0 0.021253537 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.1213, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -12.364747 -0.19417089 11.674482 65.38999\n",
      "activ tensor(5.1213, grad_fn=<CopyBackwards>) tensor(0.0783, grad_fn=<DivBackward0>) tensor(0.8356, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.1705, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.1213, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.1705, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56261915 0.0 0.021507248 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.1713, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -13.971263 -0.12934995 10.969041 66.55871\n",
      "activ tensor(5.1713, grad_fn=<CopyBackwards>) tensor(0.0777, grad_fn=<DivBackward0>) tensor(0.7886, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.1670, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.1713, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.1670, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5665625 0.0 0.021655247 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.1661, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -12.574623 -0.1602754 12.892375 67.61803\n",
      "activ tensor(5.1661, grad_fn=<CopyBackwards>) tensor(0.0764, grad_fn=<DivBackward0>) tensor(1.0823, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.1541, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.1661, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.1541, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5452729 0.0 0.020714402 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.1546, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -11.990564 -0.025429916 15.884167 68.80275\n",
      "activ tensor(5.1546, grad_fn=<CopyBackwards>) tensor(0.0749, grad_fn=<DivBackward0>) tensor(0.8129, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.1731, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.1546, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.1731, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57498 0.0 0.02212038 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.1731, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -14.330987 -0.12807244 11.431102 69.95389\n",
      "activ tensor(5.1731, grad_fn=<CopyBackwards>) tensor(0.0739, grad_fn=<DivBackward0>) tensor(0.7184, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.2219, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.1731, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.2219, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.59066236 0.0 0.022987226 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.2235, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -18.120321 -0.06392214 10.891158 70.855316\n",
      "activ tensor(5.2235, grad_fn=<CopyBackwards>) tensor(0.0737, grad_fn=<DivBackward0>) tensor(0.8126, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.2639, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.2235, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.2639, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5644907 0.0 0.021623531 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.2652, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -16.42586 -0.24140733 12.786267 71.21836\n",
      "activ tensor(5.2652, grad_fn=<CopyBackwards>) tensor(0.0739, grad_fn=<DivBackward0>) tensor(0.9843, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.3403, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.2652, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.3403, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.569348 0.0 0.021834956 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.3411, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -14.85092 -0.2689668 13.522395 71.516426\n",
      "activ tensor(5.3411, grad_fn=<CopyBackwards>) tensor(0.0747, grad_fn=<DivBackward0>) tensor(0.8416, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.3549, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.3411, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.3549, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57232445 0.0 0.022067524 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.3546, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -15.147472 -0.16262572 14.945232 71.7401\n",
      "activ tensor(5.3546, grad_fn=<CopyBackwards>) tensor(0.0746, grad_fn=<DivBackward0>) tensor(0.9087, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.3883, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.3546, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.3883, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5543311 0.0 0.021242965 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.3891, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -15.452107 -0.16134879 14.123064 71.848785\n",
      "activ tensor(5.3891, grad_fn=<CopyBackwards>) tensor(0.0750, grad_fn=<DivBackward0>) tensor(0.9076, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.4131, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.3891, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.4131, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.544628 0.0 0.020576976 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.4126, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -16.675367 -0.09064254 12.900759 71.67656\n",
      "activ tensor(5.4126, grad_fn=<CopyBackwards>) tensor(0.0755, grad_fn=<DivBackward0>) tensor(0.8905, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.4675, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.4126, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.4675, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55906725 0.0 0.021390965 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.4675, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -17.155945 -0.018462393 11.131327 71.20359\n",
      "activ tensor(5.4675, grad_fn=<CopyBackwards>) tensor(0.0768, grad_fn=<DivBackward0>) tensor(0.7999, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.5290, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.4675, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.5290, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.58086026 0.0 0.022405807 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.5280, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -15.157642 -0.2025927 14.099348 70.3047\n",
      "activ tensor(5.5280, grad_fn=<CopyBackwards>) tensor(0.0786, grad_fn=<DivBackward0>) tensor(1.1513, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.5791, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.5280, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.5791, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5764047 0.0 0.022268381 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.5788, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -15.442345 -0.19078761 14.862469 69.58112\n",
      "activ tensor(5.5788, grad_fn=<CopyBackwards>) tensor(0.0802, grad_fn=<DivBackward0>) tensor(0.7975, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.5892, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.5788, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.5892, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55780816 0.0 0.021179538 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.5894, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -15.20718 -0.13118505 13.351955 68.97665\n",
      "activ tensor(5.5894, grad_fn=<CopyBackwards>) tensor(0.0810, grad_fn=<DivBackward0>) tensor(0.7992, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.6621, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.5894, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.6621, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5813643 0.0 0.022310663 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.6625, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -14.476187 -0.14400317 13.790871 68.612144\n",
      "activ tensor(5.6625, grad_fn=<CopyBackwards>) tensor(0.0825, grad_fn=<DivBackward0>) tensor(0.8872, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.7172, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.6625, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.7172, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.576303 0.0 0.022289522 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.7176, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -15.815662 -0.34564382 10.343975 68.44286\n",
      "activ tensor(5.7176, grad_fn=<CopyBackwards>) tensor(0.0835, grad_fn=<DivBackward0>) tensor(1.1169, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.7294, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.7176, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.7294, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5449506 0.0 0.020418406 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.7294, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -16.061493 -0.36340564 13.023607 68.77258\n",
      "activ tensor(5.7294, grad_fn=<CopyBackwards>) tensor(0.0833, grad_fn=<DivBackward0>) tensor(0.9485, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.7719, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.7294, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.7719, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56934786 0.0 0.02194067 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.7713, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -16.872993 -0.21394965 15.987593 69.59004\n",
      "activ tensor(5.7713, grad_fn=<CopyBackwards>) tensor(0.0829, grad_fn=<DivBackward0>) tensor(0.8975, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.8186, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.7713, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.8186, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5837781 0.0 0.022553803 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.8186, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -18.037928 -0.16405377 13.300582 70.7287\n",
      "activ tensor(5.8186, grad_fn=<CopyBackwards>) tensor(0.0823, grad_fn=<DivBackward0>) tensor(1.0601, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.8594, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.8186, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.8594, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5662523 0.0 0.021665819 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.8595, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -14.998725 -0.22144832 13.527888 72.00166\n",
      "activ tensor(5.8595, grad_fn=<CopyBackwards>) tensor(0.0814, grad_fn=<DivBackward0>) tensor(0.9333, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.8764, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.8595, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.8764, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5661489 0.0 0.021496674 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.8783, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -16.86435 -0.20594756 13.123725 73.247925\n",
      "activ tensor(5.8783, grad_fn=<CopyBackwards>) tensor(0.0803, grad_fn=<DivBackward0>) tensor(0.8634, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.9042, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.8783, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.9042, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56282735 0.0 0.021560103 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.9045, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -16.774158 -0.36926645 13.5620365 74.18581\n",
      "activ tensor(5.9045, grad_fn=<CopyBackwards>) tensor(0.0796, grad_fn=<DivBackward0>) tensor(0.9411, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(5.9393, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.9045, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(5.9393, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56449044 0.0 0.021718673 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(5.9401, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -16.349165 -0.25786346 15.07332 75.321014\n",
      "activ tensor(5.9401, grad_fn=<CopyBackwards>) tensor(0.0789, grad_fn=<DivBackward0>) tensor(0.9564, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(6.0124, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(5.9401, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(6.0124, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5611593 0.0 0.021581247 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(6.0122, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -15.942179 -0.18183252 18.01359 76.44568\n",
      "activ tensor(6.0122, grad_fn=<CopyBackwards>) tensor(0.0786, grad_fn=<DivBackward0>) tensor(1.1314, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(6.0837, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(6.0122, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(6.0837, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.58417934 0.0 0.022426946 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(6.0847, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -17.794415 -0.06424395 15.984388 77.36289\n",
      "activ tensor(6.0847, grad_fn=<CopyBackwards>) tensor(0.0787, grad_fn=<DivBackward0>) tensor(0.7875, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(6.1017, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(6.0847, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(6.1017, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57201725 0.0 0.02169753 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(6.1014, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -21.409744 -0.121525064 12.75828 77.81561\n",
      "activ tensor(6.1014, grad_fn=<CopyBackwards>) tensor(0.0784, grad_fn=<DivBackward0>) tensor(1.1733, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(6.0894, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(6.1014, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(6.0894, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5532734 0.0 0.021264108 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(6.0889, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -19.73637 -0.12996206 17.168356 78.08969\n",
      "activ tensor(6.0889, grad_fn=<CopyBackwards>) tensor(0.0780, grad_fn=<DivBackward0>) tensor(0.9415, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(6.1180, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(6.0889, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(6.1180, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55061936 0.0 0.020894114 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(6.1171, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -17.843554 -0.081989475 16.986208 78.11034\n",
      "activ tensor(6.1171, grad_fn=<CopyBackwards>) tensor(0.0783, grad_fn=<DivBackward0>) tensor(0.8158, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(6.1167, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(6.1171, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(6.1167, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5733473 0.0 0.022004098 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(6.1172, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -18.45111 -0.14837675 17.80849 78.03673\n",
      "activ tensor(6.1172, grad_fn=<CopyBackwards>) tensor(0.0784, grad_fn=<DivBackward0>) tensor(0.8842, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(6.1564, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(6.1172, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(6.1564, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57006776 0.0 0.021877242 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(6.1574, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -19.351223 -0.19129723 18.397404 77.86018\n",
      "activ tensor(6.1574, grad_fn=<CopyBackwards>) tensor(0.0791, grad_fn=<DivBackward0>) tensor(1.0285, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(6.2346, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(6.1574, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(6.2346, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55221313 0.0 0.021179538 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(6.2343, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -20.433264 -0.21906796 14.724731 77.698685\n",
      "activ tensor(6.2343, grad_fn=<CopyBackwards>) tensor(0.0802, grad_fn=<DivBackward0>) tensor(0.9363, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(6.2374, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(6.2343, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(6.2374, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5824718 0.0 0.0227018 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(6.2376, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -20.424221 -0.39712042 15.994513 77.51402\n",
      "activ tensor(6.2376, grad_fn=<CopyBackwards>) tensor(0.0805, grad_fn=<DivBackward0>) tensor(1.1062, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(6.2811, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(6.2376, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(6.2811, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5522132 0.0 0.020925827 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(6.2811, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -18.404203 -0.3808902 17.196728 77.73627\n",
      "activ tensor(6.2811, grad_fn=<CopyBackwards>) tensor(0.0808, grad_fn=<DivBackward0>) tensor(1.0641, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(6.3376, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(6.2811, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(6.3376, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57222193 0.0 0.02202524 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(6.3377, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -18.873434 -0.25235575 18.167225 78.204254\n",
      "activ tensor(6.3377, grad_fn=<CopyBackwards>) tensor(0.0810, grad_fn=<DivBackward0>) tensor(0.9647, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(6.4490, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(6.3377, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(6.4490, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5775219 0.0 0.022141524 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(6.4499, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -17.366156 -0.13492276 17.462067 78.59897\n",
      "activ tensor(6.4499, grad_fn=<CopyBackwards>) tensor(0.0821, grad_fn=<DivBackward0>) tensor(0.8731, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(6.4923, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(6.4499, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(6.4923, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56021875 0.0 0.021486105 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(6.4915, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -19.087957 -0.38059017 15.736904 78.88322\n",
      "activ tensor(6.4915, grad_fn=<CopyBackwards>) tensor(0.0823, grad_fn=<DivBackward0>) tensor(1.1214, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(6.4815, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(6.4915, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(6.4815, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5582281 0.0 0.021507248 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(6.4822, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -19.4199 -0.37416294 14.830239 79.18298\n",
      "activ tensor(6.4822, grad_fn=<CopyBackwards>) tensor(0.0819, grad_fn=<DivBackward0>) tensor(1.0808, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(6.5602, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(6.4822, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(6.5602, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57365364 0.0 0.021930099 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(6.5613, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -18.925732 -0.33881518 15.5540905 79.33294\n",
      "activ tensor(6.5613, grad_fn=<CopyBackwards>) tensor(0.0827, grad_fn=<DivBackward0>) tensor(1.0643, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(6.5768, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(6.5613, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(6.5768, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5914555 0.0 0.022828657 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(6.5774, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -20.29965 -0.37007445 20.014963 79.711075\n",
      "activ tensor(6.5774, grad_fn=<CopyBackwards>) tensor(0.0825, grad_fn=<DivBackward0>) tensor(0.9844, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(6.6139, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(6.5774, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(6.6139, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.58437973 0.0 0.022469234 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(6.6143, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -20.74175 -0.32724312 16.830898 80.19262\n",
      "activ tensor(6.6143, grad_fn=<CopyBackwards>) tensor(0.0825, grad_fn=<DivBackward0>) tensor(1.3532, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(6.6945, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(6.6143, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(6.6945, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55906725 0.0 0.021380395 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(6.6942, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -19.566593 -0.20404822 15.537027 80.61404\n",
      "activ tensor(6.6942, grad_fn=<CopyBackwards>) tensor(0.0830, grad_fn=<DivBackward0>) tensor(0.9219, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(6.7495, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(6.6942, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(6.7495, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.556125 0.0 0.021010399 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(6.7496, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -20.411385 -0.23397928 17.847801 80.867485\n",
      "activ tensor(6.7496, grad_fn=<CopyBackwards>) tensor(0.0835, grad_fn=<DivBackward0>) tensor(1.0423, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(6.7764, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(6.7496, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(6.7764, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5779276 0.0 0.022078097 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(6.7761, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -19.527372 -0.15021959 15.890606 81.33926\n",
      "activ tensor(6.7761, grad_fn=<CopyBackwards>) tensor(0.0833, grad_fn=<DivBackward0>) tensor(1.1128, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(6.8011, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(6.7761, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(6.8011, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5381347 0.0 0.020407837 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(6.8010, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -19.767328 -0.15839353 19.376245 81.72826\n",
      "activ tensor(6.8010, grad_fn=<CopyBackwards>) tensor(0.0832, grad_fn=<DivBackward0>) tensor(1.0752, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(6.8205, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(6.8010, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(6.8205, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5591719 0.0 0.021401536 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(6.8215, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -19.302095 -0.22128506 21.015661 82.2923\n",
      "activ tensor(6.8215, grad_fn=<CopyBackwards>) tensor(0.0829, grad_fn=<DivBackward0>) tensor(1.1049, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(6.8217, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(6.8215, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(6.8217, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5734496 0.0 0.02176096 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(6.8224, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -21.121597 -0.2908732 18.47289 82.92791\n",
      "activ tensor(6.8224, grad_fn=<CopyBackwards>) tensor(0.0823, grad_fn=<DivBackward0>) tensor(1.2490, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(6.8567, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(6.8224, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(6.8567, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.562515 0.0 0.021644674 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(6.8568, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -24.773676 -0.26825804 17.267681 83.593765\n",
      "activ tensor(6.8568, grad_fn=<CopyBackwards>) tensor(0.0820, grad_fn=<DivBackward0>) tensor(1.0977, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(6.8487, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(6.8568, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(6.8487, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56178546 0.0 0.021369822 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(6.8490, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -22.853687 -0.31711003 19.20838 84.39989\n",
      "activ tensor(6.8490, grad_fn=<CopyBackwards>) tensor(0.0811, grad_fn=<DivBackward0>) tensor(1.2867, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(6.9005, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(6.8490, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(6.9005, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5623066 0.0 0.02144382 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(6.9015, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -20.98356 -0.33858544 21.8553 85.45994\n",
      "activ tensor(6.9015, grad_fn=<CopyBackwards>) tensor(0.0808, grad_fn=<DivBackward0>) tensor(1.2100, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(6.9685, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(6.9015, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(6.9685, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5606371 0.0 0.021422679 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(6.9682, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -21.720358 -0.3007266 20.519552 86.503555\n",
      "activ tensor(6.9682, grad_fn=<CopyBackwards>) tensor(0.0806, grad_fn=<DivBackward0>) tensor(1.2267, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(6.9640, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(6.9682, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(6.9640, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5662523 0.0 0.02158125 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(6.9646, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -23.226448 -0.2187145 21.42397 87.38814\n",
      "activ tensor(6.9646, grad_fn=<CopyBackwards>) tensor(0.0797, grad_fn=<DivBackward0>) tensor(1.1746, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(6.9567, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(6.9646, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(6.9567, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57487804 0.0 0.022014668 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(6.9562, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -24.201365 -0.35820448 18.777624 88.06412\n",
      "activ tensor(6.9562, grad_fn=<CopyBackwards>) tensor(0.0790, grad_fn=<DivBackward0>) tensor(1.0100, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(6.9758, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(6.9562, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(6.9758, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56542385 0.0 0.021538962 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(6.9767, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -23.602835 -0.312704 18.390165 88.460075\n",
      "activ tensor(6.9767, grad_fn=<CopyBackwards>) tensor(0.0789, grad_fn=<DivBackward0>) tensor(1.0922, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(7.0114, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(6.9767, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(7.0114, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5718122 0.0 0.022130955 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(7.0127, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -21.665241 -0.3631106 22.314157 88.46897\n",
      "activ tensor(7.0127, grad_fn=<CopyBackwards>) tensor(0.0793, grad_fn=<DivBackward0>) tensor(1.2843, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(7.0566, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(7.0127, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(7.0566, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5501934 0.0 0.020872971 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(7.0563, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -22.862303 -0.35544127 20.505613 88.57178\n",
      "activ tensor(7.0563, grad_fn=<CopyBackwards>) tensor(0.0797, grad_fn=<DivBackward0>) tensor(1.0555, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(7.1376, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(7.0563, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(7.1376, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5722221 0.0 0.022152096 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(7.1379, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -20.35843 -0.3727283 21.074692 88.48905\n",
      "activ tensor(7.1379, grad_fn=<CopyBackwards>) tensor(0.0807, grad_fn=<DivBackward0>) tensor(1.6394, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(7.1846, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(7.1379, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(7.1846, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56718284 0.0 0.021813814 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(7.1839, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -22.14457 -0.31341425 19.451801 88.32456\n",
      "activ tensor(7.1839, grad_fn=<CopyBackwards>) tensor(0.0813, grad_fn=<DivBackward0>) tensor(1.1310, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(7.2262, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(7.1839, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(7.2262, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5775219 0.0 0.022226097 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(7.2263, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -22.81247 -0.20958655 17.276205 88.07882\n",
      "activ tensor(7.2263, grad_fn=<CopyBackwards>) tensor(0.0820, grad_fn=<DivBackward0>) tensor(1.1307, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(7.2294, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(7.2263, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(7.2294, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57823163 0.0 0.022067526 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(7.2292, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -21.121447 -0.21560934 20.606176 87.86896\n",
      "activ tensor(7.2292, grad_fn=<CopyBackwards>) tensor(0.0823, grad_fn=<DivBackward0>) tensor(1.2405, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(7.2724, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(7.2292, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(7.2724, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5788392 0.0 0.022416376 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(7.2717, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -24.670874 -0.35160607 22.182922 87.67654\n",
      "activ tensor(7.2717, grad_fn=<CopyBackwards>) tensor(0.0829, grad_fn=<DivBackward0>) tensor(1.1287, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(7.2649, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(7.2717, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(7.2649, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.58216965 0.0 0.022553803 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(7.2651, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -23.075932 -0.34501725 20.931458 87.787125\n",
      "activ tensor(7.2651, grad_fn=<CopyBackwards>) tensor(0.0828, grad_fn=<DivBackward0>) tensor(1.2734, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(7.2966, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(7.2651, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(7.2966, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.53998226 0.0 0.020386692 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(7.2985, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -23.02632 -0.41713873 18.384321 88.03988\n",
      "activ tensor(7.2985, grad_fn=<CopyBackwards>) tensor(0.0829, grad_fn=<DivBackward0>) tensor(1.2609, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(7.3338, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(7.2985, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(7.3338, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5838781 0.0 0.022754658 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(7.3341, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -22.873026 -0.4274018 20.658064 88.54255\n",
      "activ tensor(7.3341, grad_fn=<CopyBackwards>) tensor(0.0828, grad_fn=<DivBackward0>) tensor(1.1434, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(7.3939, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(7.3341, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(7.3939, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57863677 0.0 0.022141526 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(7.3934, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -22.315842 -0.45343632 20.7223 89.33682\n",
      "activ tensor(7.3934, grad_fn=<CopyBackwards>) tensor(0.0828, grad_fn=<DivBackward0>) tensor(1.5326, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(7.4250, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(7.3934, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(7.4250, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55665165 0.0 0.02120068 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(7.4265, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -24.601503 -0.3839327 21.396097 90.42787\n",
      "activ tensor(7.4265, grad_fn=<CopyBackwards>) tensor(0.0821, grad_fn=<DivBackward0>) tensor(1.2497, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(7.4825, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(7.4265, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(7.4825, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.58427954 0.0 0.022564376 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(7.4816, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -21.90399 -0.40245926 25.313292 91.60441\n",
      "activ tensor(7.4816, grad_fn=<CopyBackwards>) tensor(0.0817, grad_fn=<DivBackward0>) tensor(1.1605, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(7.5082, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(7.4816, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(7.5082, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56115925 0.0 0.021729244 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(7.5087, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -24.497793 -0.31203333 20.952353 92.76587\n",
      "activ tensor(7.5087, grad_fn=<CopyBackwards>) tensor(0.0809, grad_fn=<DivBackward0>) tensor(1.4543, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(7.5121, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(7.5087, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(7.5121, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5479527 0.0 0.020746116 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(7.5124, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -27.945566 -0.29260665 20.484377 93.75916\n",
      "activ tensor(7.5124, grad_fn=<CopyBackwards>) tensor(0.0801, grad_fn=<DivBackward0>) tensor(1.1889, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(7.5646, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(7.5124, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(7.5646, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5704785 0.0 0.0220041 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(7.5647, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -26.077612 -0.3963661 23.579073 94.62123\n",
      "activ tensor(7.5647, grad_fn=<CopyBackwards>) tensor(0.0799, grad_fn=<DivBackward0>) tensor(1.2151, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(7.6022, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(7.5647, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(7.6022, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5685241 0.0 0.021538962 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(7.6028, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -24.007568 -0.39415634 24.075998 95.23264\n",
      "activ tensor(7.6028, grad_fn=<CopyBackwards>) tensor(0.0798, grad_fn=<DivBackward0>) tensor(1.3522, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(7.6618, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(7.6028, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(7.6618, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55707264 0.0 0.021221826 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(7.6616, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -25.153475 -0.22965583 25.316835 95.81211\n",
      "activ tensor(7.6616, grad_fn=<CopyBackwards>) tensor(0.0800, grad_fn=<DivBackward0>) tensor(1.0875, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(7.6759, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(7.6616, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(7.6759, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57314265 0.0 0.02209924 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(7.6772, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -26.972815 -0.39411122 23.418806 96.20227\n",
      "activ tensor(7.6772, grad_fn=<CopyBackwards>) tensor(0.0798, grad_fn=<DivBackward0>) tensor(1.6871, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(7.7094, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(7.6772, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(7.7094, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55759805 0.0 0.02127468 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(7.7095, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -27.967693 -0.34123993 22.503792 96.49354\n",
      "activ tensor(7.7095, grad_fn=<CopyBackwards>) tensor(0.0799, grad_fn=<DivBackward0>) tensor(1.2824, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(7.7479, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(7.7095, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(7.7479, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.542688 0.0 0.020640403 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(7.7477, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -26.769009 -0.2250541 22.425623 96.66949\n",
      "activ tensor(7.7477, grad_fn=<CopyBackwards>) tensor(0.0801, grad_fn=<DivBackward0>) tensor(1.1553, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(7.7897, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(7.7477, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(7.7897, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5570725 0.0 0.02105268 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(7.7894, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -24.867598 -0.22347906 25.039572 96.87238\n",
      "activ tensor(7.7894, grad_fn=<CopyBackwards>) tensor(0.0804, grad_fn=<DivBackward0>) tensor(1.2660, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(7.8223, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(7.7894, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(7.8223, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5649055 0.0 0.021549532 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(7.8217, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -25.128822 -0.38779086 25.222673 96.92728\n",
      "activ tensor(7.8217, grad_fn=<CopyBackwards>) tensor(0.0807, grad_fn=<DivBackward0>) tensor(1.1772, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(7.8656, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(7.8217, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(7.8656, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56282747 0.0 0.02160239 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(7.8657, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -24.903378 -0.35583413 22.946136 97.03754\n",
      "activ tensor(7.8657, grad_fn=<CopyBackwards>) tensor(0.0811, grad_fn=<DivBackward0>) tensor(1.2934, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(7.8709, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(7.8657, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(7.8709, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5618897 0.0 0.021538964 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(7.8723, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -25.461582 -0.37313378 23.687359 97.077324\n",
      "activ tensor(7.8723, grad_fn=<CopyBackwards>) tensor(0.0811, grad_fn=<DivBackward0>) tensor(1.5040, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(7.9133, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(7.8723, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(7.9133, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5737559 0.0 0.021856101 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(7.9140, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -26.238022 -0.5513506 20.691038 97.213356\n",
      "activ tensor(7.9140, grad_fn=<CopyBackwards>) tensor(0.0814, grad_fn=<DivBackward0>) tensor(1.4168, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(7.9106, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(7.9140, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(7.9106, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55401415 0.0 0.021105539 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(7.9097, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -25.110586 -0.47516966 23.596806 97.54788\n",
      "activ tensor(7.9097, grad_fn=<CopyBackwards>) tensor(0.0811, grad_fn=<DivBackward0>) tensor(1.3456, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(7.9594, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(7.9097, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(7.9594, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5802545 0.0 0.022363521 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(7.9605, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -27.380623 -0.41460094 26.895447 98.099205\n",
      "activ tensor(7.9605, grad_fn=<CopyBackwards>) tensor(0.0811, grad_fn=<DivBackward0>) tensor(1.4815, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(8.0028, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(7.9605, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(8.0028, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5501936 0.0 0.020883542 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(8.0027, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -26.738497 -0.5678647 22.656315 98.60922\n",
      "activ tensor(8.0027, grad_fn=<CopyBackwards>) tensor(0.0812, grad_fn=<DivBackward0>) tensor(1.5359, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(8.0364, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(8.0027, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(8.0364, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5559144 0.0 0.021020971 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(8.0360, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -25.699015 -0.41100934 22.720434 99.08823\n",
      "activ tensor(8.0360, grad_fn=<CopyBackwards>) tensor(0.0811, grad_fn=<DivBackward0>) tensor(1.4634, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(8.0635, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(8.0360, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(8.0635, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57201713 0.0 0.022130953 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(8.0639, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -26.922293 -0.45126075 23.380304 99.60725\n",
      "activ tensor(8.0639, grad_fn=<CopyBackwards>) tensor(0.0810, grad_fn=<DivBackward0>) tensor(1.4232, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(8.0942, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(8.0639, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(8.0942, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5846807 0.0 0.02227895 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(8.0943, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -26.188148 -0.44675317 24.331484 100.093094\n",
      "activ tensor(8.0943, grad_fn=<CopyBackwards>) tensor(0.0809, grad_fn=<DivBackward0>) tensor(1.2515, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(8.1493, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(8.0943, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(8.1493, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5733471 0.0 0.0218561 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(8.1501, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -27.856955 -0.53303987 25.59248 100.488014\n",
      "activ tensor(8.1501, grad_fn=<CopyBackwards>) tensor(0.0811, grad_fn=<DivBackward0>) tensor(1.4703, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(8.1799, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(8.1501, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(8.1799, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5735518 0.0 0.021877242 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(8.1799, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -25.556807 -0.4171106 27.074121 101.086685\n",
      "activ tensor(8.1799, grad_fn=<CopyBackwards>) tensor(0.0809, grad_fn=<DivBackward0>) tensor(1.3539, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(8.2327, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(8.1799, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(8.2327, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57497996 0.0 0.022257809 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(8.2329, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -27.902426 -0.44882637 25.462477 101.62749\n",
      "activ tensor(8.2329, grad_fn=<CopyBackwards>) tensor(0.0810, grad_fn=<DivBackward0>) tensor(1.7999, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(8.2480, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(8.2329, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(8.2480, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55969566 0.0 0.02126411 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(8.2484, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -31.047152 -0.47582984 22.819647 102.24341\n",
      "activ tensor(8.2484, grad_fn=<CopyBackwards>) tensor(0.0807, grad_fn=<DivBackward0>) tensor(1.4570, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(8.3300, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(8.2484, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(8.3300, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5547537 0.0 0.02109497 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(8.3292, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -29.193619 -0.20756628 27.510778 102.92036\n",
      "activ tensor(8.3292, grad_fn=<CopyBackwards>) tensor(0.0809, grad_fn=<DivBackward0>) tensor(1.3176, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(8.3614, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(8.3292, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(8.3614, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5708895 0.0 0.021993525 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(8.3618, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -27.204556 -0.24062683 27.967344 103.69852\n",
      "activ tensor(8.3618, grad_fn=<CopyBackwards>) tensor(0.0806, grad_fn=<DivBackward0>) tensor(1.3556, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(8.4225, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(8.3618, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(8.4225, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5481665 0.0 0.020820115 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(8.4223, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -28.456964 -0.45724988 27.580915 104.28236\n",
      "activ tensor(8.4223, grad_fn=<CopyBackwards>) tensor(0.0808, grad_fn=<DivBackward0>) tensor(1.3556, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(8.4726, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(8.4223, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(8.4726, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56666607 0.0 0.021824384 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(8.4738, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -30.830933 -0.4020029 27.954136 104.94105\n",
      "activ tensor(8.4738, grad_fn=<CopyBackwards>) tensor(0.0807, grad_fn=<DivBackward0>) tensor(1.4000, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(8.5013, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(8.4738, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(8.5013, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.568627 0.0 0.02169753 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(8.5026, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -31.561258 -0.38774404 24.433743 105.54867\n",
      "activ tensor(8.5026, grad_fn=<CopyBackwards>) tensor(0.0806, grad_fn=<DivBackward0>) tensor(1.5531, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(8.5214, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(8.5026, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(8.5214, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5850812 0.0 0.022754656 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(8.5220, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -29.991257 -0.37638244 27.066885 106.01596\n",
      "activ tensor(8.5220, grad_fn=<CopyBackwards>) tensor(0.0804, grad_fn=<DivBackward0>) tensor(1.5028, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(8.5143, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(8.5220, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(8.5143, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5695537 0.0 0.02176096 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(8.5149, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -28.049698 -0.5182399 28.552265 106.51127\n",
      "activ tensor(8.5149, grad_fn=<CopyBackwards>) tensor(0.0799, grad_fn=<DivBackward0>) tensor(1.5263, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(8.5549, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(8.5149, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(8.5549, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5716075 0.0 0.021982953 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(8.5543, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -28.596153 -0.44318694 27.959774 107.074455\n",
      "activ tensor(8.5543, grad_fn=<CopyBackwards>) tensor(0.0799, grad_fn=<DivBackward0>) tensor(1.4681, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(8.5772, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(8.5543, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(8.5772, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5624109 0.0 0.02152839 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(8.5772, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -28.832718 -0.53464526 27.280607 107.47268\n",
      "activ tensor(8.5772, grad_fn=<CopyBackwards>) tensor(0.0798, grad_fn=<DivBackward0>) tensor(1.5336, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(8.6474, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(8.5772, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(8.6474, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5765066 0.0 0.021898383 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(8.6477, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -29.43953 -0.44659737 25.699854 107.80144\n",
      "activ tensor(8.6477, grad_fn=<CopyBackwards>) tensor(0.0802, grad_fn=<DivBackward0>) tensor(1.3665, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(8.6569, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(8.6477, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(8.6569, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55927694 0.0 0.021338107 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(8.6562, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -29.614222 -0.33744937 25.509882 108.05955\n",
      "activ tensor(8.6562, grad_fn=<CopyBackwards>) tensor(0.0801, grad_fn=<DivBackward0>) tensor(1.3856, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(8.7352, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(8.6562, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(8.7352, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55093855 0.0 0.021147823 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(8.7348, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -29.52331 -0.4648748 26.552042 108.15899\n",
      "activ tensor(8.7348, grad_fn=<CopyBackwards>) tensor(0.0808, grad_fn=<DivBackward0>) tensor(1.4117, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(8.7798, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(8.7348, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(8.7798, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56313956 0.0 0.02135925 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(8.7801, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -30.724413 -0.5902548 30.224817 108.12658\n",
      "activ tensor(8.7801, grad_fn=<CopyBackwards>) tensor(0.0812, grad_fn=<DivBackward0>) tensor(1.5846, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(8.7922, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(8.7801, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(8.7922, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57995176 0.0 0.022363521 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(8.7923, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -28.975574 -0.5585133 26.686028 108.392784\n",
      "activ tensor(8.7923, grad_fn=<CopyBackwards>) tensor(0.0811, grad_fn=<DivBackward0>) tensor(1.5371, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(8.8621, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(8.7923, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(8.8621, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5741643 0.0 0.021982957 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(8.8624, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -29.281206 -0.5646818 24.495146 108.70861\n",
      "activ tensor(8.8624, grad_fn=<CopyBackwards>) tensor(0.0815, grad_fn=<DivBackward0>) tensor(1.6317, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(8.8765, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(8.8624, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(8.8765, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5741641 0.0 0.02212038 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(8.8767, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -30.389639 -0.52090114 28.27025 109.17818\n",
      "activ tensor(8.8767, grad_fn=<CopyBackwards>) tensor(0.0813, grad_fn=<DivBackward0>) tensor(1.4380, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(8.9472, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(8.8767, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(8.9472, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5667694 0.0 0.021655245 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(8.9468, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -29.216005 -0.4859156 26.895979 109.768555\n",
      "activ tensor(8.9468, grad_fn=<CopyBackwards>) tensor(0.0815, grad_fn=<DivBackward0>) tensor(1.5463, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(8.9557, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(8.9468, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(8.9557, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5544368 0.0 0.021147825 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(8.9560, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -31.301474 -0.41295996 29.420567 110.40149\n",
      "activ tensor(8.9560, grad_fn=<CopyBackwards>) tensor(0.0811, grad_fn=<DivBackward0>) tensor(1.3744, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(9.0251, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(8.9560, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(9.0251, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.58658135 0.0 0.022352949 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(9.0251, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -28.304056 -0.604449 30.411875 111.01712\n",
      "activ tensor(9.0251, grad_fn=<CopyBackwards>) tensor(0.0813, grad_fn=<DivBackward0>) tensor(1.7795, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(9.0262, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(9.0251, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(9.0262, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5689361 0.0 0.021908958 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(9.0269, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -31.251448 -0.5240051 27.600965 111.83153\n",
      "activ tensor(9.0269, grad_fn=<CopyBackwards>) tensor(0.0807, grad_fn=<DivBackward0>) tensor(1.4422, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(9.0310, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(9.0269, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(9.0310, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5754891 0.0 0.022056952 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(9.0327, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -34.12 -0.43767673 27.629206 112.46805\n",
      "activ tensor(9.0327, grad_fn=<CopyBackwards>) tensor(0.0803, grad_fn=<DivBackward0>) tensor(1.5814, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(9.0566, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(9.0327, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(9.0566, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5678022 0.0 0.021507246 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(9.0569, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -32.25533 -0.544374 29.729559 113.0587\n",
      "activ tensor(9.0569, grad_fn=<CopyBackwards>) tensor(0.0801, grad_fn=<DivBackward0>) tensor(1.7730, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(9.0784, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(9.0569, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(9.0784, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.58397853 0.0 0.022574946 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(9.0785, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -30.425337 -0.57319254 32.295307 113.68507\n",
      "activ tensor(9.0785, grad_fn=<CopyBackwards>) tensor(0.0799, grad_fn=<DivBackward0>) tensor(1.5115, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(9.0892, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(9.0785, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(9.0892, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57396 0.0 0.022078097 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(9.0896, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -31.87536 -0.47372025 30.706877 114.29916\n",
      "activ tensor(9.0896, grad_fn=<CopyBackwards>) tensor(0.0795, grad_fn=<DivBackward0>) tensor(1.7488, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(9.1011, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(9.0896, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(9.1011, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5698622 0.0 0.021824388 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(9.1004, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -34.498283 -0.42495587 30.464489 114.8909\n",
      "activ tensor(9.1004, grad_fn=<CopyBackwards>) tensor(0.0792, grad_fn=<DivBackward0>) tensor(1.5668, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(9.1142, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(9.1004, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(9.1142, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5898681 0.0 0.022870943 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(9.1144, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -35.262356 -0.3109426 28.985704 115.395256\n",
      "activ tensor(9.1144, grad_fn=<CopyBackwards>) tensor(0.0790, grad_fn=<DivBackward0>) tensor(1.4979, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(9.1587, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(9.1144, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(9.1587, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55791324 0.0 0.021264108 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(9.1591, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -33.05248 -0.45015198 29.338818 115.74533\n",
      "activ tensor(9.1591, grad_fn=<CopyBackwards>) tensor(0.0791, grad_fn=<DivBackward0>) tensor(1.4536, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(9.1838, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(9.1591, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(9.1838, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56449056 0.0 0.021676388 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(9.1840, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -31.34983 -0.46129125 33.507755 115.99399\n",
      "activ tensor(9.1840, grad_fn=<CopyBackwards>) tensor(0.0792, grad_fn=<DivBackward0>) tensor(1.4275, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(9.2023, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(9.1840, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(9.2023, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.58307475 0.0 0.02253266 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(9.2028, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -31.956953 -0.53871775 30.528418 116.22121\n",
      "activ tensor(9.2028, grad_fn=<CopyBackwards>) tensor(0.0792, grad_fn=<DivBackward0>) tensor(1.8197, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(9.2413, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(9.2028, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(9.2413, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56417906 0.0 0.021433251 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(9.2412, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -31.471544 -0.47643858 30.379997 116.658325\n",
      "activ tensor(9.2412, grad_fn=<CopyBackwards>) tensor(0.0792, grad_fn=<DivBackward0>) tensor(1.4032, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(9.2921, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(9.2412, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(9.2921, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5523192 0.0 0.021073826 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(9.2923, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -32.260998 -0.5016506 29.950842 117.08315\n",
      "activ tensor(9.2923, grad_fn=<CopyBackwards>) tensor(0.0794, grad_fn=<DivBackward0>) tensor(1.6592, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(9.2925, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(9.2923, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(9.2925, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56053245 0.0 0.021338107 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(9.2920, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -32.886784 -0.6229298 27.878668 117.661644\n",
      "activ tensor(9.2920, grad_fn=<CopyBackwards>) tensor(0.0790, grad_fn=<DivBackward0>) tensor(1.5067, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(9.3314, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(9.2920, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(9.3314, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5599049 0.0 0.021517819 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(9.3315, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -32.509716 -0.5590278 31.520094 118.1841\n",
      "activ tensor(9.3315, grad_fn=<CopyBackwards>) tensor(0.0790, grad_fn=<DivBackward0>) tensor(1.9227, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(9.3472, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(9.3315, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(9.3472, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5679054 0.0 0.021813814 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(9.3470, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -34.63194 -0.52349544 32.559963 118.84334\n",
      "activ tensor(9.3470, grad_fn=<CopyBackwards>) tensor(0.0786, grad_fn=<DivBackward0>) tensor(1.7128, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(9.4217, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(9.3470, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(9.4217, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5748779 0.0 0.022035811 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(9.4212, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -31.360996 -0.5245844 30.277222 119.44798\n",
      "activ tensor(9.4212, grad_fn=<CopyBackwards>) tensor(0.0789, grad_fn=<DivBackward0>) tensor(1.6746, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(9.4648, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(9.4212, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(9.4648, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.58055735 0.0 0.02243752 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(9.4649, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -33.36316 -0.543792 28.03126 120.00431\n",
      "activ tensor(9.4649, grad_fn=<CopyBackwards>) tensor(0.0789, grad_fn=<DivBackward0>) tensor(1.4783, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(9.4815, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(9.4649, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(9.4815, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55359095 0.0 0.020883542 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(9.4828, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -33.371758 -0.73408914 30.702284 120.46966\n",
      "activ tensor(9.4828, grad_fn=<CopyBackwards>) tensor(0.0787, grad_fn=<DivBackward0>) tensor(1.6753, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(9.4895, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(9.4828, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(9.4895, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57406217 0.0 0.021930099 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(9.4897, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -32.26777 -0.6397321 31.984724 120.840706\n",
      "activ tensor(9.4897, grad_fn=<CopyBackwards>) tensor(0.0785, grad_fn=<DivBackward0>) tensor(1.6623, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(9.5378, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(9.4897, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(9.5378, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5511514 0.0 0.0209364 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(9.5380, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -35.69607 -0.76622385 31.38598 121.2783\n",
      "activ tensor(9.5380, grad_fn=<CopyBackwards>) tensor(0.0786, grad_fn=<DivBackward0>) tensor(1.8874, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(9.5364, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(9.5380, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(9.5364, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57752186 0.0 0.022300093 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(9.5368, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -32.266644 -0.56950927 34.328926 121.79801\n",
      "activ tensor(9.5368, grad_fn=<CopyBackwards>) tensor(0.0783, grad_fn=<DivBackward0>) tensor(1.6603, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(9.5543, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(9.5368, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(9.5543, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5696565 0.0 0.021813814 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(9.5537, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -34.57172 -0.45065576 30.545206 122.26287\n",
      "activ tensor(9.5537, grad_fn=<CopyBackwards>) tensor(0.0781, grad_fn=<DivBackward0>) tensor(1.5901, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(9.5582, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(9.5537, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(9.5582, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5633477 0.0 0.021560103 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(9.5591, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -37.091167 -0.4914442 30.55335 122.60698\n",
      "activ tensor(9.5591, grad_fn=<CopyBackwards>) tensor(0.0780, grad_fn=<DivBackward0>) tensor(1.7172, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(9.6211, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(9.5591, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(9.6211, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56542397 0.0 0.021644676 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(9.6205, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -35.34016 -0.55092263 34.435314 122.83406\n",
      "activ tensor(9.6205, grad_fn=<CopyBackwards>) tensor(0.0783, grad_fn=<DivBackward0>) tensor(1.9663, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(9.6190, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(9.6205, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(9.6190, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.569142 0.0 0.021845529 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(9.6193, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -33.660133 -0.5778035 34.29591 123.184814\n",
      "activ tensor(9.6193, grad_fn=<CopyBackwards>) tensor(0.0781, grad_fn=<DivBackward0>) tensor(1.6008, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(9.6345, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(9.6193, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(9.6345, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5649055 0.0 0.021697529 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(9.6354, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -35.32492 -0.44728625 35.15659 123.48276\n",
      "activ tensor(9.6354, grad_fn=<CopyBackwards>) tensor(0.0780, grad_fn=<DivBackward0>) tensor(1.8760, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(9.6703, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(9.6354, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(9.6703, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.562515 0.0 0.021443821 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(9.6705, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -38.15298 -0.5628477 32.9689 123.986694\n",
      "activ tensor(9.6705, grad_fn=<CopyBackwards>) tensor(0.0780, grad_fn=<DivBackward0>) tensor(1.7620, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(9.6986, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(9.6705, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(9.6986, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5610548 0.0 0.021390963 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(9.6991, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -38.80026 -0.60532296 32.248486 124.58565\n",
      "activ tensor(9.6991, grad_fn=<CopyBackwards>) tensor(0.0779, grad_fn=<DivBackward0>) tensor(1.7308, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(9.7755, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(9.6991, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(9.7755, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57559085 0.0 0.022025242 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(9.7751, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -36.25904 -0.5181532 33.908154 125.26262\n",
      "activ tensor(9.7751, grad_fn=<CopyBackwards>) tensor(0.0780, grad_fn=<DivBackward0>) tensor(1.8103, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(9.7722, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(9.7751, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(9.7722, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5614725 0.0 0.021581247 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(9.7724, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -34.50386 -0.6763743 35.830563 125.98711\n",
      "activ tensor(9.7724, grad_fn=<CopyBackwards>) tensor(0.0776, grad_fn=<DivBackward0>) tensor(1.9419, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(9.8048, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(9.7724, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(9.8048, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57324505 0.0 0.021972382 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(9.8050, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -35.352985 -0.4211257 35.209255 126.85328\n",
      "activ tensor(9.8050, grad_fn=<CopyBackwards>) tensor(0.0773, grad_fn=<DivBackward0>) tensor(1.8326, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(9.8252, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(9.8050, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(9.8252, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5639712 0.0 0.021538962 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(9.8256, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -35.355377 -0.530989 32.417637 127.634735\n",
      "activ tensor(9.8256, grad_fn=<CopyBackwards>) tensor(0.0770, grad_fn=<DivBackward0>) tensor(1.8643, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(9.8643, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(9.8256, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(9.8643, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56241083 0.0 0.021644674 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(9.8649, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -35.901165 -0.63226897 33.913506 128.3039\n",
      "activ tensor(9.8649, grad_fn=<CopyBackwards>) tensor(0.0769, grad_fn=<DivBackward0>) tensor(1.6549, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(9.8617, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(9.8649, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(9.8617, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5605327 0.0 0.021549532 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(9.8617, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -36.1292 -0.692206 31.88943 128.81767\n",
      "activ tensor(9.8617, grad_fn=<CopyBackwards>) tensor(0.0766, grad_fn=<DivBackward0>) tensor(1.9528, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(9.9166, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(9.8617, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(9.9166, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56532043 0.0 0.021877242 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(9.9169, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -37.00624 -0.64069164 34.086086 129.30325\n",
      "activ tensor(9.9169, grad_fn=<CopyBackwards>) tensor(0.0767, grad_fn=<DivBackward0>) tensor(1.9692, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(9.9329, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(9.9169, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(9.9329, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5620983 0.0 0.02134868 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(9.9328, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -36.810207 -0.6176074 37.271893 129.73346\n",
      "activ tensor(9.9328, grad_fn=<CopyBackwards>) tensor(0.0766, grad_fn=<DivBackward0>) tensor(1.9124, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(10.0165, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(9.9328, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(10.0165, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57140225 0.0 0.022046383 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(10.0165, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -35.564735 -0.6662594 32.145443 130.15839\n",
      "activ tensor(10.0165, grad_fn=<CopyBackwards>) tensor(0.0770, grad_fn=<DivBackward0>) tensor(1.9184, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(10.0498, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(10.0165, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(10.0498, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.54613197 0.0 0.020714402 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(10.0486, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -37.06839 -0.46163607 32.124603 130.49086\n",
      "activ tensor(10.0486, grad_fn=<CopyBackwards>) tensor(0.0770, grad_fn=<DivBackward0>) tensor(1.8619, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(10.0804, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(10.0486, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(10.0804, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55538726 0.0 0.021031544 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(10.0804, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -36.564816 -0.5056283 34.05225 130.67453\n",
      "activ tensor(10.0804, grad_fn=<CopyBackwards>) tensor(0.0771, grad_fn=<DivBackward0>) tensor(1.9269, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(10.0565, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(10.0804, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(10.0565, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.58287394 0.0 0.022384666 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(10.0558, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -36.69646 -0.7149506 35.020527 130.82243\n",
      "activ tensor(10.0558, grad_fn=<CopyBackwards>) tensor(0.0769, grad_fn=<DivBackward0>) tensor(1.6951, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(10.0583, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(10.0558, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(10.0583, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56635565 0.0 0.021771528 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(10.0588, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -38.428295 -0.5814214 35.910656 131.07396\n",
      "activ tensor(10.0588, grad_fn=<CopyBackwards>) tensor(0.0767, grad_fn=<DivBackward0>) tensor(1.7561, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(10.1005, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(10.0588, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(10.1005, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.54634655 0.0 0.02085183 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(10.1017, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -35.733612 -0.6988791 35.85081 131.34862\n",
      "activ tensor(10.1017, grad_fn=<CopyBackwards>) tensor(0.0769, grad_fn=<DivBackward0>) tensor(2.0968, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(10.1442, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(10.1017, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(10.1442, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5670794 0.0 0.02184553 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(10.1439, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -37.991543 -0.7289424 34.98364 131.84734\n",
      "activ tensor(10.1439, grad_fn=<CopyBackwards>) tensor(0.0769, grad_fn=<DivBackward0>) tensor(1.7615, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(10.1610, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(10.1439, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(10.1610, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5659417 0.0 0.022004098 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(10.1616, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -39.94066 -0.7841542 33.37653 132.38356\n",
      "activ tensor(10.1616, grad_fn=<CopyBackwards>) tensor(0.0768, grad_fn=<DivBackward0>) tensor(2.1543, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(10.1761, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(10.1616, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(10.1761, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.58397853 0.0 0.022543233 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(10.1758, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -38.402004 -0.7286446 37.92941 133.13951\n",
      "activ tensor(10.1758, grad_fn=<CopyBackwards>) tensor(0.0764, grad_fn=<DivBackward0>) tensor(1.8905, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(10.2663, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(10.1758, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(10.2663, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57964844 0.0 0.02226838 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(10.2664, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -36.993305 -0.69315195 38.428734 133.96388\n",
      "activ tensor(10.2664, grad_fn=<CopyBackwards>) tensor(0.0766, grad_fn=<DivBackward0>) tensor(1.7091, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(10.3191, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(10.2664, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(10.3191, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5655275 0.0 0.021612959 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(10.3199, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -38.74592 -0.61986846 37.073475 134.8336\n",
      "activ tensor(10.3199, grad_fn=<CopyBackwards>) tensor(0.0765, grad_fn=<DivBackward0>) tensor(1.9608, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(10.3662, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(10.3199, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(10.3662, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5674925 0.0 0.02179267 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(10.3660, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -41.77209 -0.58167577 37.54488 135.61104\n",
      "activ tensor(10.3660, grad_fn=<CopyBackwards>) tensor(0.0764, grad_fn=<DivBackward0>) tensor(2.1088, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(10.3915, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(10.3660, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(10.3915, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5577032 0.0 0.021200681 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(10.3924, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -42.288895 -0.6211692 34.592472 136.25418\n",
      "activ tensor(10.3924, grad_fn=<CopyBackwards>) tensor(0.0763, grad_fn=<DivBackward0>) tensor(1.8719, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(10.3909, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(10.3924, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(10.3909, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56924474 0.0 0.021771528 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(10.3915, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -39.403423 -0.7244862 38.035477 136.71907\n",
      "activ tensor(10.3915, grad_fn=<CopyBackwards>) tensor(0.0760, grad_fn=<DivBackward0>) tensor(1.9508, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(10.4270, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(10.3915, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(10.4270, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56656253 0.0 0.021803243 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(10.4278, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -37.80126 -0.5961431 39.822136 137.13107\n",
      "activ tensor(10.4278, grad_fn=<CopyBackwards>) tensor(0.0760, grad_fn=<DivBackward0>) tensor(2.0799, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(10.4895, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(10.4278, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(10.4895, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5596958 0.0 0.021422677 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(10.4902, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -39.151253 -0.5942655 37.381363 137.56067\n",
      "activ tensor(10.4902, grad_fn=<CopyBackwards>) tensor(0.0763, grad_fn=<DivBackward0>) tensor(2.1118, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(10.5421, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(10.4902, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(10.5421, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55422544 0.0 0.021010397 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(10.5421, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -39.40118 -0.54142666 36.992817 137.92221\n",
      "activ tensor(10.5421, grad_fn=<CopyBackwards>) tensor(0.0764, grad_fn=<DivBackward0>) tensor(2.1989, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(10.5685, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(10.5421, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(10.5685, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56074154 0.0 0.021380393 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(10.5674, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -39.381298 -0.5026743 36.072548 138.15645\n",
      "activ tensor(10.5674, grad_fn=<CopyBackwards>) tensor(0.0765, grad_fn=<DivBackward0>) tensor(1.9622, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(10.5927, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(10.5674, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(10.5927, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5757944 0.0 0.02219438 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(10.5922, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -39.409573 -0.48791623 36.54957 138.40144\n",
      "activ tensor(10.5922, grad_fn=<CopyBackwards>) tensor(0.0765, grad_fn=<DivBackward0>) tensor(1.8549, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(10.5840, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(10.5922, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(10.5840, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5618899 0.0 0.02152839 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(10.5838, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -40.836544 -0.6196516 37.404453 138.6088\n",
      "activ tensor(10.5838, grad_fn=<CopyBackwards>) tensor(0.0764, grad_fn=<DivBackward0>) tensor(1.8930, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(10.5959, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(10.5838, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(10.5959, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5749801 0.0 0.02209924 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(10.5956, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -40.180946 -0.73166096 40.009853 139.00613\n",
      "activ tensor(10.5956, grad_fn=<CopyBackwards>) tensor(0.0762, grad_fn=<DivBackward0>) tensor(1.8759, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(10.6095, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(10.5956, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(10.6095, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5646982 0.0 0.021708105 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(10.6096, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -39.61051 -0.64339805 36.507412 139.47076\n",
      "activ tensor(10.6096, grad_fn=<CopyBackwards>) tensor(0.0761, grad_fn=<DivBackward0>) tensor(2.4045, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(10.6720, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(10.6096, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(10.6720, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5765065 0.0 0.022088666 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(10.6713, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -40.01798 -0.6976538 34.001568 140.06053\n",
      "activ tensor(10.6713, grad_fn=<CopyBackwards>) tensor(0.0762, grad_fn=<DivBackward0>) tensor(1.9410, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(10.7097, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(10.6713, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(10.7097, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55938154 0.0 0.021316964 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(10.7104, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -38.62212 -0.9300289 38.79688 140.73582\n",
      "activ tensor(10.7104, grad_fn=<CopyBackwards>) tensor(0.0761, grad_fn=<DivBackward0>) tensor(2.0491, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(10.7574, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(10.7104, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(10.7574, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5499806 0.0 0.020999826 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(10.7573, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -41.176006 -0.7762709 37.988163 141.57294\n",
      "activ tensor(10.7573, grad_fn=<CopyBackwards>) tensor(0.0760, grad_fn=<DivBackward0>) tensor(1.8853, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(10.7548, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(10.7573, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(10.7548, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.58857584 0.0 0.0227758 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(10.7544, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -41.78171 -0.7814203 39.093567 142.44757\n",
      "activ tensor(10.7544, grad_fn=<CopyBackwards>) tensor(0.0755, grad_fn=<DivBackward0>) tensor(1.8387, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(10.7953, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(10.7544, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(10.7953, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56261915 0.0 0.0217821 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(10.7957, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -38.981686 -0.7079217 39.661194 143.34776\n",
      "activ tensor(10.7957, grad_fn=<CopyBackwards>) tensor(0.0753, grad_fn=<DivBackward0>) tensor(2.1540, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(10.8009, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(10.7957, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(10.8009, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5651128 0.0 0.021306394 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(10.7999, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -41.23598 -0.7921181 36.8414 144.09068\n",
      "activ tensor(10.7999, grad_fn=<CopyBackwards>) tensor(0.0750, grad_fn=<DivBackward0>) tensor(1.9954, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(10.8473, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(10.7999, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(10.8473, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56996506 0.0 0.022035811 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(10.8481, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -42.876118 -0.74636394 38.3879 144.65707\n",
      "activ tensor(10.8481, grad_fn=<CopyBackwards>) tensor(0.0750, grad_fn=<DivBackward0>) tensor(1.7787, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(10.8925, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(10.8481, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(10.8925, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56955373 0.0 0.021834956 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(10.8938, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -41.350243 -0.8025278 40.35153 145.03423\n",
      "activ tensor(10.8938, grad_fn=<CopyBackwards>) tensor(0.0751, grad_fn=<DivBackward0>) tensor(2.1097, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(10.9052, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(10.8938, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(10.9052, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5717099 0.0 0.021634104 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(10.9056, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -40.42187 -0.81053597 42.185345 145.40268\n",
      "activ tensor(10.9056, grad_fn=<CopyBackwards>) tensor(0.0750, grad_fn=<DivBackward0>) tensor(2.1580, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(10.9194, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(10.9056, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(10.9194, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57670975 0.0 0.02226838 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(10.9198, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -42.204002 -0.5894008 40.55143 145.71628\n",
      "activ tensor(10.9198, grad_fn=<CopyBackwards>) tensor(0.0749, grad_fn=<DivBackward0>) tensor(2.2894, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(10.9603, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(10.9198, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(10.9603, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5480597 0.0 0.020862399 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(10.9601, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -45.268936 -0.67267454 39.783504 145.98616\n",
      "activ tensor(10.9601, grad_fn=<CopyBackwards>) tensor(0.0751, grad_fn=<DivBackward0>) tensor(2.0259, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(10.9958, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(10.9601, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(10.9958, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57436824 0.0 0.02219438 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(10.9952, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -45.764187 -0.67249775 39.402294 146.2034\n",
      "activ tensor(10.9952, grad_fn=<CopyBackwards>) tensor(0.0752, grad_fn=<DivBackward0>) tensor(1.9553, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(11.0435, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(10.9952, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(11.0435, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57109463 0.0 0.021792673 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(11.0440, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -42.5418 -0.5001436 40.403294 146.39671\n",
      "activ tensor(11.0440, grad_fn=<CopyBackwards>) tensor(0.0754, grad_fn=<DivBackward0>) tensor(1.7182, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(11.0488, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(11.0440, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(11.0488, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5631397 0.0 0.021401536 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(11.0485, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -41.096783 -0.6508343 44.287724 146.63911\n",
      "activ tensor(11.0485, grad_fn=<CopyBackwards>) tensor(0.0753, grad_fn=<DivBackward0>) tensor(2.0692, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(11.0825, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(11.0485, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(11.0825, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5646981 0.0 0.02136982 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(11.0829, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -43.040264 -0.6565774 40.414883 147.04202\n",
      "activ tensor(11.0829, grad_fn=<CopyBackwards>) tensor(0.0754, grad_fn=<DivBackward0>) tensor(2.1274, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(11.1037, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(11.0829, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(11.1037, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56324357 0.0 0.021549534 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(11.1045, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -43.304646 -0.72454053 39.549316 147.44966\n",
      "activ tensor(11.1045, grad_fn=<CopyBackwards>) tensor(0.0753, grad_fn=<DivBackward0>) tensor(2.3664, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(11.1202, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(11.1045, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(11.1202, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55401415 0.0 0.021010399 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(11.1206, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -42.561268 -0.7112201 40.854176 148.0325\n",
      "activ tensor(11.1206, grad_fn=<CopyBackwards>) tensor(0.0751, grad_fn=<DivBackward0>) tensor(1.9691, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(11.1530, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(11.1206, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(11.1530, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5588576 0.0 0.021338107 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(11.1535, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -42.50836 -0.8764558 38.851337 148.67975\n",
      "activ tensor(11.1535, grad_fn=<CopyBackwards>) tensor(0.0750, grad_fn=<DivBackward0>) tensor(2.1435, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(11.1808, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(11.1535, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(11.1808, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56438684 0.0 0.021295821 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(11.1805, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -43.71962 -0.75613534 42.177326 149.47803\n",
      "activ tensor(11.1805, grad_fn=<CopyBackwards>) tensor(0.0748, grad_fn=<DivBackward0>) tensor(1.9083, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(11.1918, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(11.1805, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(11.1918, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56645906 0.0 0.0218561 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(11.1912, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -43.46493 -0.6452553 42.54637 150.32712\n",
      "activ tensor(11.1912, grad_fn=<CopyBackwards>) tensor(0.0744, grad_fn=<DivBackward0>) tensor(2.1881, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(11.2553, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(11.1912, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(11.2553, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56790537 0.0 0.021665815 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(11.2558, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -41.901104 -0.7366861 39.68594 151.14218\n",
      "activ tensor(11.2558, grad_fn=<CopyBackwards>) tensor(0.0745, grad_fn=<DivBackward0>) tensor(2.2180, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(11.2908, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(11.2558, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(11.2908, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57078683 0.0 0.021919528 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(11.2905, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -43.486683 -0.79303384 38.124855 151.81119\n",
      "activ tensor(11.2905, grad_fn=<CopyBackwards>) tensor(0.0744, grad_fn=<DivBackward0>) tensor(2.1411, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(11.3450, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(11.2905, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(11.3450, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.566976 0.0 0.021581247 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(11.3442, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -43.098347 -0.77353823 41.061676 152.4254\n",
      "activ tensor(11.3442, grad_fn=<CopyBackwards>) tensor(0.0744, grad_fn=<DivBackward0>) tensor(2.0078, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(11.3588, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(11.3442, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(11.3588, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57691294 0.0 0.022247236 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(11.3584, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -44.201466 -0.7955484 42.970154 152.93404\n",
      "activ tensor(11.3584, grad_fn=<CopyBackwards>) tensor(0.0743, grad_fn=<DivBackward0>) tensor(2.3395, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(11.4155, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(11.3584, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(11.4155, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57109445 0.0 0.021961812 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(11.4155, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -45.71522 -0.7517875 41.284267 153.415\n",
      "activ tensor(11.4155, grad_fn=<CopyBackwards>) tensor(0.0744, grad_fn=<DivBackward0>) tensor(2.2228, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(11.4681, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(11.4155, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(11.4681, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57518363 0.0 0.022278951 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(11.4682, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -42.6413 -0.81152487 43.055717 153.88287\n",
      "activ tensor(11.4682, grad_fn=<CopyBackwards>) tensor(0.0745, grad_fn=<DivBackward0>) tensor(2.5105, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(11.4848, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(11.4682, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(11.4848, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5798505 0.0 0.022458661 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(11.4850, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -45.086124 -0.83557427 40.421436 154.3608\n",
      "activ tensor(11.4850, grad_fn=<CopyBackwards>) tensor(0.0744, grad_fn=<DivBackward0>) tensor(2.1923, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(11.4869, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(11.4850, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(11.4869, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5798506 0.0 0.022479806 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(11.4868, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -45.613087 -0.8302368 40.951626 154.76529\n",
      "activ tensor(11.4868, grad_fn=<CopyBackwards>) tensor(0.0742, grad_fn=<DivBackward0>) tensor(2.1296, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(11.5215, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(11.4868, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(11.5215, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5786366 0.0 0.022310665 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(11.5217, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -44.44374 -0.70941395 45.301697 155.23502\n",
      "activ tensor(11.5217, grad_fn=<CopyBackwards>) tensor(0.0742, grad_fn=<DivBackward0>) tensor(2.1611, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(11.5328, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(11.5217, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(11.5328, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5654241 0.0 0.021517819 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(11.5328, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -44.126186 -0.7709113 44.034157 155.74124\n",
      "activ tensor(11.5328, grad_fn=<CopyBackwards>) tensor(0.0741, grad_fn=<DivBackward0>) tensor(2.4688, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(11.5447, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(11.5328, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(11.5447, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57762325 0.0 0.022067523 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(11.5463, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -45.758175 -0.81524646 44.630096 156.20284\n",
      "activ tensor(11.5463, grad_fn=<CopyBackwards>) tensor(0.0739, grad_fn=<DivBackward0>) tensor(1.9610, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(11.6032, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(11.5463, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(11.6032, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5829746 0.0 0.022670086 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(11.6045, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -48.66862 -0.87628865 42.83173 156.56963\n",
      "activ tensor(11.6045, grad_fn=<CopyBackwards>) tensor(0.0741, grad_fn=<DivBackward0>) tensor(2.4590, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(11.6127, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(11.6045, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(11.6127, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5350774 0.0 0.02035498 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(11.6131, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -49.14616 -0.818534 42.329247 157.02383\n",
      "activ tensor(11.6131, grad_fn=<CopyBackwards>) tensor(0.0740, grad_fn=<DivBackward0>) tensor(1.8675, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(11.6869, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(11.6131, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(11.6869, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56490546 0.0 0.021750387 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(11.6870, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -45.75504 -0.8476498 45.222828 157.48189\n",
      "activ tensor(11.6870, grad_fn=<CopyBackwards>) tensor(0.0742, grad_fn=<DivBackward0>) tensor(2.4368, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(11.7218, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(11.6870, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(11.7218, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56852424 0.0 0.02168696 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(11.7216, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -44.382282 -0.7633354 46.428627 158.02371\n",
      "activ tensor(11.7216, grad_fn=<CopyBackwards>) tensor(0.0742, grad_fn=<DivBackward0>) tensor(1.9732, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(11.7771, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(11.7216, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(11.7771, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56852406 0.0 0.021908956 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(11.7769, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -46.928905 -0.7149439 44.817436 158.5444\n",
      "activ tensor(11.7769, grad_fn=<CopyBackwards>) tensor(0.0743, grad_fn=<DivBackward0>) tensor(2.1727, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(11.8203, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(11.7769, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(11.8203, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5830748 0.0 0.02253266 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(11.8208, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -47.238888 -0.67409325 42.068302 159.0213\n",
      "activ tensor(11.8208, grad_fn=<CopyBackwards>) tensor(0.0743, grad_fn=<DivBackward0>) tensor(2.3266, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(11.8594, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(11.8208, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(11.8594, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57487786 0.0 0.022067526 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(11.8596, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -45.892273 -0.7553897 44.36481 159.34355\n",
      "activ tensor(11.8596, grad_fn=<CopyBackwards>) tensor(0.0744, grad_fn=<DivBackward0>) tensor(2.3689, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(11.8785, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(11.8596, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(11.8785, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5615769 0.0 0.021517819 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(11.8781, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -45.73173 -0.74405026 43.459545 159.73323\n",
      "activ tensor(11.8781, grad_fn=<CopyBackwards>) tensor(0.0744, grad_fn=<DivBackward0>) tensor(2.4457, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(11.9211, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(11.8781, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(11.9211, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.54870075 0.0 0.020904684 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(11.9211, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -48.047333 -0.73855746 44.313656 160.16508\n",
      "activ tensor(11.9211, grad_fn=<CopyBackwards>) tensor(0.0744, grad_fn=<DivBackward0>) tensor(2.4037, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(11.9841, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(11.9211, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(11.9841, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57426614 0.0 0.021972384 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(11.9840, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -45.391575 -0.8000517 47.198742 160.63705\n",
      "activ tensor(11.9840, grad_fn=<CopyBackwards>) tensor(0.0746, grad_fn=<DivBackward0>) tensor(2.0255, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(12.0177, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(11.9840, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(12.0177, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56676924 0.0 0.021665817 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(12.0173, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -45.37855 -0.88854563 41.80932 161.19792\n",
      "activ tensor(12.0173, grad_fn=<CopyBackwards>) tensor(0.0746, grad_fn=<DivBackward0>) tensor(2.4941, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(12.0450, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(12.0173, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(12.0450, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5816665 0.0 0.022331808 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(12.0443, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -47.063057 -0.8206462 42.004097 161.83528\n",
      "activ tensor(12.0443, grad_fn=<CopyBackwards>) tensor(0.0744, grad_fn=<DivBackward0>) tensor(2.0983, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(12.0573, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(12.0443, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(12.0573, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5614723 0.0 0.021433247 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(12.0568, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -46.037506 -0.80334955 44.923607 162.51665\n",
      "activ tensor(12.0568, grad_fn=<CopyBackwards>) tensor(0.0742, grad_fn=<DivBackward0>) tensor(2.6644, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(12.0325, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(12.0568, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(12.0325, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.555598 0.0 0.021073826 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(12.0329, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -48.715725 -0.75499237 45.542957 163.2927\n",
      "activ tensor(12.0329, grad_fn=<CopyBackwards>) tensor(0.0737, grad_fn=<DivBackward0>) tensor(2.1656, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(12.0179, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(12.0329, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(12.0179, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5641789 0.0 0.02161296 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(12.0176, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -47.921635 -0.84421575 45.882915 164.0535\n",
      "activ tensor(12.0176, grad_fn=<CopyBackwards>) tensor(0.0733, grad_fn=<DivBackward0>) tensor(2.1751, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(12.0825, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(12.0176, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(12.0825, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5646981 0.0 0.021655245 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(12.0840, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -47.184418 -0.87009096 44.714424 164.80524\n",
      "activ tensor(12.0840, grad_fn=<CopyBackwards>) tensor(0.0733, grad_fn=<DivBackward0>) tensor(2.0664, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(12.1018, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(12.0840, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(12.1018, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5665627 0.0 0.02152839 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(12.1022, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -48.806572 -0.9531913 44.557762 165.37991\n",
      "activ tensor(12.1022, grad_fn=<CopyBackwards>) tensor(0.0732, grad_fn=<DivBackward0>) tensor(2.4225, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(12.1483, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(12.1022, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(12.1483, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57823145 0.0 0.022289522 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(12.1482, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -48.405632 -0.9257208 44.409794 165.9729\n",
      "activ tensor(12.1482, grad_fn=<CopyBackwards>) tensor(0.0732, grad_fn=<DivBackward0>) tensor(2.1144, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(12.2251, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(12.1482, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(12.2251, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5757947 0.0 0.02202524 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(12.2255, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -47.396473 -0.902665 48.215748 166.55249\n",
      "activ tensor(12.2255, grad_fn=<CopyBackwards>) tensor(0.0734, grad_fn=<DivBackward0>) tensor(2.3957, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(12.2174, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(12.2255, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(12.2174, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5568621 0.0 0.021253537 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(12.2169, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -47.34086 -0.902403 48.41428 167.1126\n",
      "activ tensor(12.2169, grad_fn=<CopyBackwards>) tensor(0.0731, grad_fn=<DivBackward0>) tensor(2.4090, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(12.2478, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(12.2169, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(12.2478, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5763031 0.0 0.022109812 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(12.2478, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -49.17627 -0.9091971 46.298832 167.58138\n",
      "activ tensor(12.2478, grad_fn=<CopyBackwards>) tensor(0.0731, grad_fn=<DivBackward0>) tensor(2.2509, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(12.3032, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(12.2478, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(12.3032, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55242544 0.0 0.020978685 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(12.3038, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -52.1022 -0.7471233 47.417744 167.97704\n",
      "activ tensor(12.3038, grad_fn=<CopyBackwards>) tensor(0.0732, grad_fn=<DivBackward0>) tensor(2.3792, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(12.3229, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(12.3038, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(12.3229, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.570684 0.0 0.021919526 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(12.3225, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -52.40545 -0.85189736 45.12169 168.25507\n",
      "activ tensor(12.3225, grad_fn=<CopyBackwards>) tensor(0.0732, grad_fn=<DivBackward0>) tensor(2.4379, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(12.3924, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(12.3225, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(12.3924, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56542397 0.0 0.021581246 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(12.3931, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -48.999237 -0.85207826 48.817173 168.50273\n",
      "activ tensor(12.3931, grad_fn=<CopyBackwards>) tensor(0.0735, grad_fn=<DivBackward0>) tensor(2.1591, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(12.4281, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(12.3931, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(12.4281, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5650092 0.0 0.021782102 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(12.4289, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -47.745937 -0.8106874 50.700485 168.75371\n",
      "activ tensor(12.4289, grad_fn=<CopyBackwards>) tensor(0.0737, grad_fn=<DivBackward0>) tensor(2.6686, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(12.4350, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(12.4289, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(12.4350, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5928408 0.0 0.022596091 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(12.4358, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -50.748447 -0.83765304 46.714943 169.0945\n",
      "activ tensor(12.4358, grad_fn=<CopyBackwards>) tensor(0.0735, grad_fn=<DivBackward0>) tensor(2.1524, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(12.4497, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(12.4358, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(12.4497, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5444128 0.0 0.020756686 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(12.4491, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -51.178116 -0.9536322 46.66506 169.43849\n",
      "activ tensor(12.4491, grad_fn=<CopyBackwards>) tensor(0.0735, grad_fn=<DivBackward0>) tensor(2.6535, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(12.4910, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(12.4491, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(12.4910, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56873006 0.0 0.021782102 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(12.4905, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -49.21111 -0.9223035 46.92718 169.96777\n",
      "activ tensor(12.4905, grad_fn=<CopyBackwards>) tensor(0.0735, grad_fn=<DivBackward0>) tensor(2.2250, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(12.5471, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(12.4905, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(12.5471, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55717766 0.0 0.021063255 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(12.5467, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -48.87457 -0.74584097 47.668453 170.5605\n",
      "activ tensor(12.5467, grad_fn=<CopyBackwards>) tensor(0.0736, grad_fn=<DivBackward0>) tensor(2.3018, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(12.5966, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(12.5467, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(12.5966, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5462392 0.0 0.020915255 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(12.5979, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -51.285503 -0.75939286 48.103626 171.13052\n",
      "activ tensor(12.5979, grad_fn=<CopyBackwards>) tensor(0.0736, grad_fn=<DivBackward0>) tensor(2.3386, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(12.6076, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(12.5979, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(12.6076, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5728361 0.0 0.022152096 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(12.6076, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -48.717396 -0.8715027 49.34013 171.77133\n",
      "activ tensor(12.6076, grad_fn=<CopyBackwards>) tensor(0.0734, grad_fn=<DivBackward0>) tensor(2.3782, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(12.6394, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(12.6076, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(12.6394, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5593814 0.0 0.021338109 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(12.6397, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -50.54541 -0.7913843 46.50005 172.40784\n",
      "activ tensor(12.6397, grad_fn=<CopyBackwards>) tensor(0.0733, grad_fn=<DivBackward0>) tensor(2.1906, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(12.7045, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(12.6397, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(12.7045, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57068413 0.0 0.0217821 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(12.7049, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -50.189587 -0.98779875 44.085567 172.9091\n",
      "activ tensor(12.7049, grad_fn=<CopyBackwards>) tensor(0.0735, grad_fn=<DivBackward0>) tensor(2.5128, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(12.7128, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(12.7049, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(12.7128, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5488077 0.0 0.020904686 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(12.7127, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -48.26978 -0.9402945 49.433178 173.53381\n",
      "activ tensor(12.7127, grad_fn=<CopyBackwards>) tensor(0.0733, grad_fn=<DivBackward0>) tensor(2.1335, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(12.7875, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(12.7127, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(12.7875, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57263154 0.0 0.022152094 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(12.7881, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -52.564106 -0.879016 48.829433 174.17816\n",
      "activ tensor(12.7881, grad_fn=<CopyBackwards>) tensor(0.0734, grad_fn=<DivBackward0>) tensor(2.5747, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(12.8044, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(12.7881, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(12.8044, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57334733 0.0 0.021930099 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(12.8035, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -51.319485 -0.84365 48.54926 174.78723\n",
      "activ tensor(12.8035, grad_fn=<CopyBackwards>) tensor(0.0733, grad_fn=<DivBackward0>) tensor(2.6460, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(12.7820, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(12.8035, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(12.7820, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56975937 0.0 0.02177153 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(12.7805, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -49.925304 -0.90358526 48.872 175.37401\n",
      "activ tensor(12.7805, grad_fn=<CopyBackwards>) tensor(0.0729, grad_fn=<DivBackward0>) tensor(2.7335, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(12.7968, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(12.7805, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(12.7968, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5725293 0.0 0.02186667 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(12.7968, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -51.11896 -0.8267524 46.442074 176.03343\n",
      "activ tensor(12.7968, grad_fn=<CopyBackwards>) tensor(0.0727, grad_fn=<DivBackward0>) tensor(2.2664, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(12.8478, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(12.7968, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(12.8478, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5666662 0.0 0.02151782 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(12.8471, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -51.185867 -0.87496376 49.27647 176.54196\n",
      "activ tensor(12.8471, grad_fn=<CopyBackwards>) tensor(0.0728, grad_fn=<DivBackward0>) tensor(2.3054, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(12.8826, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(12.8471, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(12.8826, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5725292 0.0 0.021982957 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(12.8832, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -50.398815 -1.0206591 51.043545 177.0898\n",
      "activ tensor(12.8832, grad_fn=<CopyBackwards>) tensor(0.0727, grad_fn=<DivBackward0>) tensor(2.2485, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(12.9431, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(12.8832, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(12.9431, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5686271 0.0 0.021665815 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(12.9437, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -50.844128 -0.95419216 51.452354 177.6994\n",
      "activ tensor(12.9437, grad_fn=<CopyBackwards>) tensor(0.0728, grad_fn=<DivBackward0>) tensor(2.7381, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(12.9756, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(12.9437, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(12.9756, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5661488 0.0 0.02161296 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(12.9764, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -52.750694 -0.9511706 50.23595 178.25235\n",
      "activ tensor(12.9764, grad_fn=<CopyBackwards>) tensor(0.0728, grad_fn=<DivBackward0>) tensor(2.3399, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(12.9934, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(12.9764, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(12.9934, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55433124 0.0 0.02113725 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(12.9930, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -55.296635 -1.1512545 49.443348 178.82695\n",
      "activ tensor(12.9930, grad_fn=<CopyBackwards>) tensor(0.0727, grad_fn=<DivBackward0>) tensor(2.6357, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(13.0093, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(12.9930, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(13.0093, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5665626 0.0 0.021581246 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(13.0098, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -55.743237 -1.0161784 50.107952 179.50928\n",
      "activ tensor(13.0098, grad_fn=<CopyBackwards>) tensor(0.0725, grad_fn=<DivBackward0>) tensor(2.3012, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(13.0041, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(13.0098, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(13.0041, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5673893 0.0 0.021676388 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(13.0048, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -52.18168 -0.91971445 51.28331 180.10492\n",
      "activ tensor(13.0048, grad_fn=<CopyBackwards>) tensor(0.0722, grad_fn=<DivBackward0>) tensor(2.4217, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(13.0415, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(13.0048, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(13.0415, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5748781 0.0 0.021972384 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(13.0428, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -51.190628 -0.88646805 54.62676 180.6101\n",
      "activ tensor(13.0428, grad_fn=<CopyBackwards>) tensor(0.0722, grad_fn=<DivBackward0>) tensor(2.3807, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(13.0960, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(13.0428, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(13.0960, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5478459 0.0 0.020735545 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(13.0956, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -54.557102 -0.90651286 50.18699 181.04558\n",
      "activ tensor(13.0956, grad_fn=<CopyBackwards>) tensor(0.0723, grad_fn=<DivBackward0>) tensor(2.6096, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(13.1084, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(13.0956, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(13.1084, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5399822 0.0 0.020661548 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(13.1088, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -55.029972 -0.8603707 48.92587 181.39229\n",
      "activ tensor(13.1088, grad_fn=<CopyBackwards>) tensor(0.0723, grad_fn=<DivBackward0>) tensor(2.1258, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(13.1696, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(13.1088, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(13.1696, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5585431 0.0 0.021168968 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(13.1700, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -52.755623 -0.95360446 51.947636 181.53644\n",
      "activ tensor(13.1700, grad_fn=<CopyBackwards>) tensor(0.0725, grad_fn=<DivBackward0>) tensor(2.3296, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(13.1570, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(13.1700, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(13.1570, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56604534 0.0 0.021549532 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(13.1576, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -51.993294 -0.93611205 50.119274 181.7765\n",
      "activ tensor(13.1576, grad_fn=<CopyBackwards>) tensor(0.0724, grad_fn=<DivBackward0>) tensor(2.0413, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(13.1774, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(13.1576, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(13.1774, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5511512 0.0 0.020946968 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(13.1790, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -53.979603 -0.9032874 52.398922 182.0722\n",
      "activ tensor(13.1790, grad_fn=<CopyBackwards>) tensor(0.0724, grad_fn=<DivBackward0>) tensor(2.7588, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(13.1892, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(13.1790, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(13.1892, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56563115 0.0 0.021380391 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(13.1885, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -51.77887 -0.8748256 52.311657 182.36823\n",
      "activ tensor(13.1885, grad_fn=<CopyBackwards>) tensor(0.0723, grad_fn=<DivBackward0>) tensor(2.5230, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(13.1796, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(13.1885, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(13.1796, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5593814 0.0 0.021475533 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(13.1781, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -54.015606 -0.8694575 49.207058 182.73802\n",
      "activ tensor(13.1781, grad_fn=<CopyBackwards>) tensor(0.0721, grad_fn=<DivBackward0>) tensor(2.2795, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(13.1986, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(13.1781, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(13.1986, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5664592 0.0 0.021824386 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(13.1989, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -53.545574 -0.7142017 48.809906 183.2184\n",
      "activ tensor(13.1989, grad_fn=<CopyBackwards>) tensor(0.0720, grad_fn=<DivBackward0>) tensor(2.2036, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(13.2355, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(13.1989, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(13.2355, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5727339 0.0 0.02210981 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(13.2347, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -52.921185 -0.93428636 51.553272 183.69232\n",
      "activ tensor(13.2347, grad_fn=<CopyBackwards>) tensor(0.0720, grad_fn=<DivBackward0>) tensor(2.4122, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(13.2547, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(13.2347, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(13.2547, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5711972 0.0 0.022056954 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(13.2544, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -55.442677 -1.0300628 53.57183 184.34128\n",
      "activ tensor(13.2544, grad_fn=<CopyBackwards>) tensor(0.0719, grad_fn=<DivBackward0>) tensor(2.3358, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(13.2740, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(13.2544, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(13.2740, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.560846 0.0 0.021348678 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(13.2744, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -54.656414 -0.98359615 51.017788 185.08563\n",
      "activ tensor(13.2744, grad_fn=<CopyBackwards>) tensor(0.0717, grad_fn=<DivBackward0>) tensor(2.7208, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(13.3152, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(13.2744, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(13.3152, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56480193 0.0 0.021803245 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(13.3157, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -53.562767 -1.0614787 51.879047 185.91289\n",
      "activ tensor(13.3157, grad_fn=<CopyBackwards>) tensor(0.0716, grad_fn=<DivBackward0>) tensor(2.3923, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(13.3260, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(13.3157, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(13.3260, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.53900486 0.0 0.020291552 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(13.3252, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -54.37144 -1.0538297 50.533787 186.77904\n",
      "activ tensor(13.3252, grad_fn=<CopyBackwards>) tensor(0.0713, grad_fn=<DivBackward0>) tensor(2.4496, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(13.3817, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(13.3252, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(13.3817, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57467407 0.0 0.02219438 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(13.3823, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -54.020145 -1.0542412 51.652294 187.73628\n",
      "activ tensor(13.3823, grad_fn=<CopyBackwards>) tensor(0.0713, grad_fn=<DivBackward0>) tensor(2.2601, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(13.4043, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(13.3823, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(13.4043, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.58267313 0.0 0.022490377 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(13.4045, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -53.443424 -1.0548599 55.90756 188.64441\n",
      "activ tensor(13.4045, grad_fn=<CopyBackwards>) tensor(0.0711, grad_fn=<DivBackward0>) tensor(2.5939, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(13.5132, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(13.4045, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(13.5132, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57273376 0.0 0.021898383 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(13.5136, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -54.39443 -0.997954 53.46043 189.45789\n",
      "activ tensor(13.5136, grad_fn=<CopyBackwards>) tensor(0.0713, grad_fn=<DivBackward0>) tensor(2.5521, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(13.5736, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(13.5136, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(13.5736, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5578083 0.0 0.021295823 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(13.5734, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -56.261997 -1.0696664 53.7684 190.17068\n",
      "activ tensor(13.5734, grad_fn=<CopyBackwards>) tensor(0.0714, grad_fn=<DivBackward0>) tensor(2.4533, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(13.5753, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(13.5734, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(13.5753, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57436824 0.0 0.021898385 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(13.5761, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -58.53493 -0.9488302 53.191086 190.71132\n",
      "activ tensor(13.5761, grad_fn=<CopyBackwards>) tensor(0.0712, grad_fn=<DivBackward0>) tensor(2.5406, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(13.6135, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(13.5761, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(13.6135, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5714023 0.0 0.021729246 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(13.6136, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -58.86692 -1.0938811 52.64883 191.06558\n",
      "activ tensor(13.6136, grad_fn=<CopyBackwards>) tensor(0.0713, grad_fn=<DivBackward0>) tensor(2.6407, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(13.6526, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(13.6136, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(13.6526, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5727338 0.0 0.022004098 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(13.6532, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -55.56033 -1.0082676 56.248363 191.39832\n",
      "activ tensor(13.6532, grad_fn=<CopyBackwards>) tensor(0.0713, grad_fn=<DivBackward0>) tensor(2.6231, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(13.7028, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(13.6532, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(13.7028, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5697594 0.0 0.021708103 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(13.7033, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -54.55051 -1.0745778 56.633087 191.65126\n",
      "activ tensor(13.7033, grad_fn=<CopyBackwards>) tensor(0.0715, grad_fn=<DivBackward0>) tensor(2.8657, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(13.7669, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(13.7033, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(13.7669, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57559115 0.0 0.02236352 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(13.7665, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -58.41023 -0.96871877 54.315792 191.89207\n",
      "activ tensor(13.7665, grad_fn=<CopyBackwards>) tensor(0.0717, grad_fn=<DivBackward0>) tensor(2.4658, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(13.8105, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(13.7665, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(13.8105, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5757945 0.0 0.022173237 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(13.8108, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -58.812096 -0.7792948 51.969433 192.06453\n",
      "activ tensor(13.8108, grad_fn=<CopyBackwards>) tensor(0.0719, grad_fn=<DivBackward0>) tensor(2.3713, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(13.8628, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(13.8108, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(13.8628, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5718124 0.0 0.022078095 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(13.8637, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -55.874622 -0.8882739 55.08225 192.27928\n",
      "activ tensor(13.8637, grad_fn=<CopyBackwards>) tensor(0.0721, grad_fn=<DivBackward0>) tensor(2.2744, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(13.8823, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(13.8637, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(13.8823, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.583778 0.0 0.022490377 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(13.8813, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -55.172173 -0.92156935 55.034847 192.46811\n",
      "activ tensor(13.8813, grad_fn=<CopyBackwards>) tensor(0.0721, grad_fn=<DivBackward0>) tensor(2.6133, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(13.9399, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(13.8813, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(13.9399, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5749799 0.0 0.022226095 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(13.9400, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -58.083324 -0.99758524 54.373875 192.76111\n",
      "activ tensor(13.9400, grad_fn=<CopyBackwards>) tensor(0.0723, grad_fn=<DivBackward0>) tensor(2.3905, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(13.9924, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(13.9400, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(13.9924, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5687301 0.0 0.021866672 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(13.9926, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -55.683754 -1.0694978 56.66367 193.13698\n",
      "activ tensor(13.9926, grad_fn=<CopyBackwards>) tensor(0.0724, grad_fn=<DivBackward0>) tensor(2.9204, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(13.9835, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(13.9926, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(13.9835, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.588277 0.0 0.022818085 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(13.9842, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -56.033897 -0.99697685 51.86823 193.63206\n",
      "activ tensor(13.9842, grad_fn=<CopyBackwards>) tensor(0.0722, grad_fn=<DivBackward0>) tensor(2.4234, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(14.0566, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(13.9842, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(14.0566, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.58297443 0.0 0.02252209 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(14.0568, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -56.741077 -1.0764823 52.307686 194.24138\n",
      "activ tensor(14.0568, grad_fn=<CopyBackwards>) tensor(0.0724, grad_fn=<DivBackward0>) tensor(2.5224, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(14.0707, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(14.0568, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(14.0707, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57538766 0.0 0.022257809 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(14.0704, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -55.731136 -1.0988792 55.950394 194.99678\n",
      "activ tensor(14.0704, grad_fn=<CopyBackwards>) tensor(0.0722, grad_fn=<DivBackward0>) tensor(2.7147, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(14.1131, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(14.0704, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(14.1131, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.571505 0.0 0.021993527 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(14.1127, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -59.75969 -1.1740812 55.657654 195.81865\n",
      "activ tensor(14.1127, grad_fn=<CopyBackwards>) tensor(0.0721, grad_fn=<DivBackward0>) tensor(2.4448, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(14.1288, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(14.1127, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(14.1288, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5706843 0.0 0.021866672 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(14.1294, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -56.641033 -0.9206085 55.64208 196.63615\n",
      "activ tensor(14.1294, grad_fn=<CopyBackwards>) tensor(0.0719, grad_fn=<DivBackward0>) tensor(2.8320, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(14.1597, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(14.1294, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(14.1597, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57140225 0.0 0.02176096 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(14.1593, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -58.387836 -1.0359063 53.82545 197.45287\n",
      "activ tensor(14.1593, grad_fn=<CopyBackwards>) tensor(0.0717, grad_fn=<DivBackward0>) tensor(2.5679, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(14.1747, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(14.1593, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(14.1747, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.569965 0.0 0.02177153 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(14.1751, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -58.679554 -1.1136904 54.37647 198.23145\n",
      "activ tensor(14.1751, grad_fn=<CopyBackwards>) tensor(0.0715, grad_fn=<DivBackward0>) tensor(2.7456, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(14.1931, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(14.1751, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(14.1931, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5606371 0.0 0.02135925 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(14.1924, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -57.447857 -1.0350847 55.592953 198.85997\n",
      "activ tensor(14.1924, grad_fn=<CopyBackwards>) tensor(0.0714, grad_fn=<DivBackward0>) tensor(2.7182, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(14.2401, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(14.1924, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(14.2401, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5772175 0.0 0.02209924 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(14.2404, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -57.532753 -1.0837123 58.381165 199.52528\n",
      "activ tensor(14.2404, grad_fn=<CopyBackwards>) tensor(0.0714, grad_fn=<DivBackward0>) tensor(2.4813, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(14.2567, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(14.2404, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(14.2567, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5477388 0.0 0.020735545 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(14.2572, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -58.023518 -1.119296 57.8704 200.13713\n",
      "activ tensor(14.2572, grad_fn=<CopyBackwards>) tensor(0.0712, grad_fn=<DivBackward0>) tensor(2.8918, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(14.2925, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(14.2572, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(14.2925, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5581231 0.0 0.021264108 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(14.2922, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -59.74975 -0.970464 55.5331 200.717\n",
      "activ tensor(14.2922, grad_fn=<CopyBackwards>) tensor(0.0712, grad_fn=<DivBackward0>) tensor(2.5097, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(14.3436, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(14.2922, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(14.3436, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55759805 0.0 0.02134868 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(14.3433, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -61.67155 -0.903903 57.53185 201.23167\n",
      "activ tensor(14.3433, grad_fn=<CopyBackwards>) tensor(0.0713, grad_fn=<DivBackward0>) tensor(2.8955, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(14.3965, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(14.3433, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(14.3965, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5682148 0.0 0.02168696 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(14.3976, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -62.02159 -1.0150325 56.05909 201.71863\n",
      "activ tensor(14.3976, grad_fn=<CopyBackwards>) tensor(0.0714, grad_fn=<DivBackward0>) tensor(3.0410, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(14.3799, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(14.3976, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(14.3799, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.560219 0.0 0.021285253 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(14.3795, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -58.845947 -1.1281612 59.159016 202.09251\n",
      "activ tensor(14.3795, grad_fn=<CopyBackwards>) tensor(0.0712, grad_fn=<DivBackward0>) tensor(2.6315, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(14.3960, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(14.3795, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(14.3960, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55717766 0.0 0.021264108 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(14.3969, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -58.114143 -1.0394342 61.168823 202.51007\n",
      "activ tensor(14.3969, grad_fn=<CopyBackwards>) tensor(0.0711, grad_fn=<DivBackward0>) tensor(2.5161, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(14.4421, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(14.3969, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(14.4421, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56903917 0.0 0.02194067 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(14.4429, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -62.070606 -1.1369979 56.059124 202.92365\n",
      "activ tensor(14.4429, grad_fn=<CopyBackwards>) tensor(0.0712, grad_fn=<DivBackward0>) tensor(2.8608, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(14.4688, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(14.4429, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(14.4688, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56428295 0.0 0.021412104 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(14.4694, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -62.65038 -1.139686 56.5673 203.32242\n",
      "activ tensor(14.4694, grad_fn=<CopyBackwards>) tensor(0.0712, grad_fn=<DivBackward0>) tensor(2.4661, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(14.5613, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(14.4694, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(14.5613, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.58927196 0.0 0.022712372 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(14.5612, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -59.121346 -1.1427414 58.036655 203.75064\n",
      "activ tensor(14.5612, grad_fn=<CopyBackwards>) tensor(0.0715, grad_fn=<DivBackward0>) tensor(2.7163, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(14.5923, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(14.5612, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(14.5923, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5835772 0.0 0.022236666 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(14.5926, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -58.51964 -1.1584381 58.760006 204.21927\n",
      "activ tensor(14.5926, grad_fn=<CopyBackwards>) tensor(0.0715, grad_fn=<DivBackward0>) tensor(2.4278, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(14.6418, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(14.5926, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(14.6418, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5814649 0.0 0.022617232 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(14.6421, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -60.61848 -1.1749599 58.48724 204.66908\n",
      "activ tensor(14.6421, grad_fn=<CopyBackwards>) tensor(0.0715, grad_fn=<DivBackward0>) tensor(2.6855, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(14.6796, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(14.6421, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(14.6796, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56790555 0.0 0.021655247 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(14.6805, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -58.54688 -0.9685182 58.503178 205.15674\n",
      "activ tensor(14.6805, grad_fn=<CopyBackwards>) tensor(0.0716, grad_fn=<DivBackward0>) tensor(2.7448, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(14.7226, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(14.6805, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(14.7226, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5726315 0.0 0.021919528 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(14.7219, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -59.88352 -1.0298098 56.630512 205.6197\n",
      "activ tensor(14.7219, grad_fn=<CopyBackwards>) tensor(0.0716, grad_fn=<DivBackward0>) tensor(2.9128, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(14.7602, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(14.7219, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(14.7602, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55948627 0.0 0.021306396 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(14.7604, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -59.902115 -1.091913 54.859295 206.00772\n",
      "activ tensor(14.7604, grad_fn=<CopyBackwards>) tensor(0.0716, grad_fn=<DivBackward0>) tensor(2.4752, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(14.8044, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(14.7604, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(14.8044, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5550706 0.0 0.021211252 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(14.8037, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -59.44202 -1.1020324 59.957912 206.3381\n",
      "activ tensor(14.8037, grad_fn=<CopyBackwards>) tensor(0.0717, grad_fn=<DivBackward0>) tensor(2.8707, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(14.8214, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(14.8037, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(14.8214, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56738913 0.0 0.021739814 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(14.8212, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -62.99537 -1.0524147 59.359543 206.7835\n",
      "activ tensor(14.8212, grad_fn=<CopyBackwards>) tensor(0.0717, grad_fn=<DivBackward0>) tensor(2.7665, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(14.8295, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(14.8212, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(14.8295, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5559142 0.0 0.021264108 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(14.8296, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -60.035557 -1.0877205 57.78845 207.26955\n",
      "activ tensor(14.8296, grad_fn=<CopyBackwards>) tensor(0.0715, grad_fn=<DivBackward0>) tensor(2.8748, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(14.8813, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(14.8296, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(14.8813, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.54848695 0.0 0.020894114 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(14.8800, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -60.637287 -1.0836961 58.363697 207.86636\n",
      "activ tensor(14.8800, grad_fn=<CopyBackwards>) tensor(0.0716, grad_fn=<DivBackward0>) tensor(2.4874, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(14.9388, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(14.8800, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(14.9388, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5571778 0.0 0.021369822 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(14.9379, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -61.561096 -0.9775852 56.37655 208.49817\n",
      "activ tensor(14.9379, grad_fn=<CopyBackwards>) tensor(0.0716, grad_fn=<DivBackward0>) tensor(3.1795, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(14.9444, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(14.9379, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(14.9444, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57150495 0.0 0.022056954 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(14.9451, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -59.664314 -1.0006466 60.185986 209.17035\n",
      "activ tensor(14.9451, grad_fn=<CopyBackwards>) tensor(0.0714, grad_fn=<DivBackward0>) tensor(2.4459, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(14.9313, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(14.9451, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(14.9313, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5504066 0.0 0.020989254 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(14.9312, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -61.327847 -1.1692978 61.559597 209.84555\n",
      "activ tensor(14.9312, grad_fn=<CopyBackwards>) tensor(0.0712, grad_fn=<DivBackward0>) tensor(2.7556, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(14.9493, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(14.9312, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(14.9493, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5639714 0.0 0.021549532 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(14.9507, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -61.643864 -1.1206576 60.356903 210.65672\n",
      "activ tensor(14.9507, grad_fn=<CopyBackwards>) tensor(0.0710, grad_fn=<DivBackward0>) tensor(2.6650, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(14.9672, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(14.9507, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(14.9672, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5626192 0.0 0.021560105 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(14.9681, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -63.282104 -1.1844801 59.775143 211.44005\n",
      "activ tensor(14.9681, grad_fn=<CopyBackwards>) tensor(0.0708, grad_fn=<DivBackward0>) tensor(2.8842, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(14.9205, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(14.9681, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(14.9205, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56903905 0.0 0.021813815 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(14.9208, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -64.696045 -1.2166555 59.620396 212.20264\n",
      "activ tensor(14.9208, grad_fn=<CopyBackwards>) tensor(0.0703, grad_fn=<DivBackward0>) tensor(2.5605, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(14.9794, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(14.9208, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(14.9794, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5684209 0.0 0.021834958 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(14.9796, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -65.13387 -1.2999461 60.89696 212.97025\n",
      "activ tensor(14.9796, grad_fn=<CopyBackwards>) tensor(0.0703, grad_fn=<DivBackward0>) tensor(2.9685, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(15.0180, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(14.9796, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(15.0180, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5590673 0.0 0.021338107 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(15.0184, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -62.22213 -1.19415 61.92854 213.70274\n",
      "activ tensor(15.0184, grad_fn=<CopyBackwards>) tensor(0.0703, grad_fn=<DivBackward0>) tensor(2.6742, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(15.0289, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(15.0184, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(15.0289, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56604534 0.0 0.021782102 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(15.0291, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -61.61775 -1.2096008 64.381516 214.37878\n",
      "activ tensor(15.0291, grad_fn=<CopyBackwards>) tensor(0.0701, grad_fn=<DivBackward0>) tensor(3.1380, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(15.0241, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(15.0291, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(15.0241, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57211965 0.0 0.021898383 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(15.0253, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -65.82849 -1.1118271 60.092953 214.94423\n",
      "activ tensor(15.0253, grad_fn=<CopyBackwards>) tensor(0.0699, grad_fn=<DivBackward0>) tensor(3.1222, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(15.0759, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(15.0253, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(15.0759, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57304054 0.0 0.022215521 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(15.0751, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -66.26359 -1.0786009 58.602844 215.35577\n",
      "activ tensor(15.0751, grad_fn=<CopyBackwards>) tensor(0.0700, grad_fn=<DivBackward0>) tensor(2.6015, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(15.0849, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(15.0751, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(15.0849, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5517887 0.0 0.02103154 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(15.0851, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -62.509895 -1.1485125 63.172382 215.62885\n",
      "activ tensor(15.0851, grad_fn=<CopyBackwards>) tensor(0.0700, grad_fn=<DivBackward0>) tensor(2.7651, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(15.0959, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(15.0851, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(15.0959, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5706841 0.0 0.02195124 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(15.0960, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -62.87653 -1.1510899 61.347218 215.76764\n",
      "activ tensor(15.0960, grad_fn=<CopyBackwards>) tensor(0.0700, grad_fn=<DivBackward0>) tensor(2.8114, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(15.1349, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(15.0960, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(15.1349, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5667694 0.0 0.021718673 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(15.1355, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -63.30097 -1.0863137 62.302067 215.9801\n",
      "activ tensor(15.1355, grad_fn=<CopyBackwards>) tensor(0.0701, grad_fn=<DivBackward0>) tensor(2.6874, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(15.1198, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(15.1355, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(15.1198, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56635576 0.0 0.021676388 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(15.1201, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -61.927097 -1.1198183 61.894444 216.16399\n",
      "activ tensor(15.1201, grad_fn=<CopyBackwards>) tensor(0.0699, grad_fn=<DivBackward0>) tensor(2.9456, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(15.1178, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(15.1201, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(15.1178, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57752174 0.0 0.022321234 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(15.1166, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -64.46935 -1.1342231 59.069355 216.4225\n",
      "activ tensor(15.1166, grad_fn=<CopyBackwards>) tensor(0.0698, grad_fn=<DivBackward0>) tensor(2.9552, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(15.1468, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(15.1166, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(15.1468, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5692451 0.0 0.021803247 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(15.1468, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -63.6335 -1.0816828 59.896954 216.71457\n",
      "activ tensor(15.1468, grad_fn=<CopyBackwards>) tensor(0.0699, grad_fn=<DivBackward0>) tensor(2.7446, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(15.2024, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(15.1468, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(15.2024, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56188965 0.0 0.021718673 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(15.2033, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -64.2939 -1.0395879 62.181973 217.1296\n",
      "activ tensor(15.2033, grad_fn=<CopyBackwards>) tensor(0.0700, grad_fn=<DivBackward0>) tensor(2.5876, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(15.2305, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(15.2033, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(15.2305, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5697592 0.0 0.021697529 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(15.2301, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -65.67295 -1.2177569 63.570446 217.63734\n",
      "activ tensor(15.2301, grad_fn=<CopyBackwards>) tensor(0.0700, grad_fn=<DivBackward0>) tensor(2.9227, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(15.2645, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(15.2301, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(15.2645, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56272346 0.0 0.021528391 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(15.2651, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -63.45018 -1.1490222 60.76614 218.24208\n",
      "activ tensor(15.2651, grad_fn=<CopyBackwards>) tensor(0.0699, grad_fn=<DivBackward0>) tensor(2.7860, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(15.2864, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(15.2651, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(15.2864, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5664593 0.0 0.021591818 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(15.2870, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -64.80099 -1.2210106 60.955265 218.89575\n",
      "activ tensor(15.2870, grad_fn=<CopyBackwards>) tensor(0.0698, grad_fn=<DivBackward0>) tensor(2.6790, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(15.3424, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(15.2870, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(15.3424, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.6025454 0.0 0.023441792 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(15.3431, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -64.01744 -1.252234 60.98849 219.59628\n",
      "activ tensor(15.3431, grad_fn=<CopyBackwards>) tensor(0.0699, grad_fn=<DivBackward0>) tensor(2.8243, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(15.3440, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(15.3431, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(15.3440, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.58065826 0.0 0.022522092 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(15.3437, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -63.304848 -1.3352268 62.358532 220.36284\n",
      "activ tensor(15.3437, grad_fn=<CopyBackwards>) tensor(0.0696, grad_fn=<DivBackward0>) tensor(3.2829, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(15.3559, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(15.3437, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(15.3559, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56614876 0.0 0.021887813 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(15.3555, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -64.139084 -1.2329962 66.22844 221.2285\n",
      "activ tensor(15.3555, grad_fn=<CopyBackwards>) tensor(0.0694, grad_fn=<DivBackward0>) tensor(2.8995, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(15.3951, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(15.3555, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(15.3951, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5510447 0.0 0.0208624 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(15.3942, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -65.213554 -1.1492617 62.620914 222.07356\n",
      "activ tensor(15.3942, grad_fn=<CopyBackwards>) tensor(0.0693, grad_fn=<DivBackward0>) tensor(3.1113, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(15.4726, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(15.3942, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(15.4726, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57426643 0.0 0.022088667 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(15.4728, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -66.803894 -1.1390563 62.86668 222.84189\n",
      "activ tensor(15.4728, grad_fn=<CopyBackwards>) tensor(0.0694, grad_fn=<DivBackward0>) tensor(2.6504, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(15.4929, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(15.4728, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(15.4929, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5695535 0.0 0.021919528 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(15.4921, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -67.698784 -1.1158866 63.90885 223.50163\n",
      "activ tensor(15.4921, grad_fn=<CopyBackwards>) tensor(0.0693, grad_fn=<DivBackward0>) tensor(2.7603, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(15.5472, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(15.4921, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(15.5472, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5688329 0.0 0.021708101 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(15.5485, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -68.149895 -1.2020636 63.245167 224.10313\n",
      "activ tensor(15.5485, grad_fn=<CopyBackwards>) tensor(0.0694, grad_fn=<DivBackward0>) tensor(2.8350, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(15.5503, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(15.5485, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(15.5503, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5482735 0.0 0.020640403 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(15.5507, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -65.67648 -1.2631434 66.71888 224.60748\n",
      "activ tensor(15.5507, grad_fn=<CopyBackwards>) tensor(0.0692, grad_fn=<DivBackward0>) tensor(3.2340, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(15.6075, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(15.5507, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(15.6075, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5613681 0.0 0.021665817 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(15.6076, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -65.17249 -1.1989611 66.53809 225.16301\n",
      "activ tensor(15.6076, grad_fn=<CopyBackwards>) tensor(0.0693, grad_fn=<DivBackward0>) tensor(2.6799, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(15.6328, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(15.6076, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(15.6328, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5839786 0.0 0.0226278 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(15.6335, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -69.465546 -1.244997 63.77795 225.70334\n",
      "activ tensor(15.6335, grad_fn=<CopyBackwards>) tensor(0.0693, grad_fn=<DivBackward0>) tensor(3.4236, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(15.6924, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(15.6335, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(15.6924, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5756927 0.0 0.022321235 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(15.6924, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -69.92633 -1.221246 62.343204 226.19984\n",
      "activ tensor(15.6924, grad_fn=<CopyBackwards>) tensor(0.0694, grad_fn=<DivBackward0>) tensor(2.9653, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(15.7242, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(15.6924, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(15.7242, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5704788 0.0 0.022004098 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(15.7247, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -65.75009 -1.2659233 65.842125 226.64699\n",
      "activ tensor(15.7247, grad_fn=<CopyBackwards>) tensor(0.0694, grad_fn=<DivBackward0>) tensor(3.2277, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(15.7028, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(15.7247, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(15.7028, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5602188 0.0 0.021042114 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(15.7044, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -65.63118 -1.0756023 66.44752 227.12907\n",
      "activ tensor(15.7044, grad_fn=<CopyBackwards>) tensor(0.0691, grad_fn=<DivBackward0>) tensor(2.7005, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(15.7457, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(15.7044, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(15.7457, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55284935 0.0 0.021063255 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(15.7456, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -66.98085 -1.2214285 64.20844 227.51292\n",
      "activ tensor(15.7456, grad_fn=<CopyBackwards>) tensor(0.0692, grad_fn=<DivBackward0>) tensor(2.8644, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(15.7602, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(15.7456, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(15.7602, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5543313 0.0 0.02103154 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(15.7606, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -66.056305 -1.2442559 65.96289 227.90945\n",
      "activ tensor(15.7606, grad_fn=<CopyBackwards>) tensor(0.0692, grad_fn=<DivBackward0>) tensor(2.8479, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(15.7645, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(15.7606, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(15.7645, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5805575 0.0 0.022152096 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(15.7649, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -67.891174 -1.1968663 62.268314 228.24928\n",
      "activ tensor(15.7649, grad_fn=<CopyBackwards>) tensor(0.0691, grad_fn=<DivBackward0>) tensor(3.2540, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(15.8348, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(15.7649, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(15.8348, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5707869 0.0 0.02177153 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(15.8361, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -66.158165 -1.2867758 63.085365 228.51193\n",
      "activ tensor(15.8361, grad_fn=<CopyBackwards>) tensor(0.0693, grad_fn=<DivBackward0>) tensor(2.8628, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(15.8549, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(15.8361, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(15.8549, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56136805 0.0 0.021486104 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(15.8550, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -67.423706 -1.3075628 66.86355 228.81664\n",
      "activ tensor(15.8550, grad_fn=<CopyBackwards>) tensor(0.0693, grad_fn=<DivBackward0>) tensor(3.2441, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(15.8809, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(15.8550, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(15.8809, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5742662 0.0 0.022247236 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(15.8800, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -69.7698 -1.238449 65.433395 229.22255\n",
      "activ tensor(15.8800, grad_fn=<CopyBackwards>) tensor(0.0693, grad_fn=<DivBackward0>) tensor(3.0104, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(15.9306, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(15.8800, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(15.9306, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5477388 0.0 0.020629833 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(15.9296, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -67.53474 -1.1425048 65.164635 229.66087\n",
      "activ tensor(15.9296, grad_fn=<CopyBackwards>) tensor(0.0694, grad_fn=<DivBackward0>) tensor(3.3502, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(15.9261, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(15.9296, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(15.9261, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55728287 0.0 0.021443818 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(15.9259, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -69.02646 -1.1506046 63.501316 230.12128\n",
      "activ tensor(15.9259, grad_fn=<CopyBackwards>) tensor(0.0692, grad_fn=<DivBackward0>) tensor(2.8219, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(15.9199, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(15.9259, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(15.9199, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5535909 0.0 0.021137252 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(15.9183, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -67.151764 -1.2883264 64.368484 230.61497\n",
      "activ tensor(15.9183, grad_fn=<CopyBackwards>) tensor(0.0690, grad_fn=<DivBackward0>) tensor(3.4197, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(15.9173, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(15.9183, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(15.9173, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5694506 0.0 0.02160239 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(15.9179, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -67.329155 -1.2043643 66.77171 231.15471\n",
      "activ tensor(15.9179, grad_fn=<CopyBackwards>) tensor(0.0689, grad_fn=<DivBackward0>) tensor(3.0282, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(15.9488, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(15.9179, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(15.9488, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56303555 0.0 0.021729244 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(15.9489, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -68.39102 -1.3243263 68.256165 231.7334\n",
      "activ tensor(15.9489, grad_fn=<CopyBackwards>) tensor(0.0688, grad_fn=<DivBackward0>) tensor(2.9940, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(15.9479, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(15.9489, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(15.9479, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55906737 0.0 0.02135925 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(15.9475, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -68.92414 -1.2907045 67.0577 232.53815\n",
      "activ tensor(15.9475, grad_fn=<CopyBackwards>) tensor(0.0686, grad_fn=<DivBackward0>) tensor(3.0875, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(15.9383, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(15.9475, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(15.9383, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5572827 0.0 0.02135925 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(15.9389, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -70.20826 -1.3215221 64.85364 233.3772\n",
      "activ tensor(15.9389, grad_fn=<CopyBackwards>) tensor(0.0683, grad_fn=<DivBackward0>) tensor(3.2738, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(15.9518, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(15.9389, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(15.9518, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57681125 0.0 0.02212038 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(15.9526, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -70.689766 -1.3528954 67.97044 234.25984\n",
      "activ tensor(15.9526, grad_fn=<CopyBackwards>) tensor(0.0681, grad_fn=<DivBackward0>) tensor(2.9893, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(15.9683, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(15.9526, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(15.9683, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.58578163 0.0 0.023040082 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(15.9684, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -71.143456 -1.2880378 67.14416 235.12895\n",
      "activ tensor(15.9684, grad_fn=<CopyBackwards>) tensor(0.0679, grad_fn=<DivBackward0>) tensor(2.9951, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(15.9778, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(15.9684, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(15.9778, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.54934096 0.0 0.020650975 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(15.9789, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -69.12525 -1.1877732 69.10164 235.97289\n",
      "activ tensor(15.9789, grad_fn=<CopyBackwards>) tensor(0.0677, grad_fn=<DivBackward0>) tensor(3.1510, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(15.9991, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(15.9789, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(15.9991, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56687266 0.0 0.021634104 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(15.9990, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -68.81609 -1.2195451 71.09406 236.74034\n",
      "activ tensor(15.9990, grad_fn=<CopyBackwards>) tensor(0.0676, grad_fn=<DivBackward0>) tensor(3.0448, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(16.0496, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(15.9990, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(16.0496, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5672861 0.0 0.02161296 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(16.0509, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -73.021065 -1.2522357 65.70921 237.41287\n",
      "activ tensor(16.0509, grad_fn=<CopyBackwards>) tensor(0.0676, grad_fn=<DivBackward0>) tensor(3.3043, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(16.1007, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(16.0509, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(16.1007, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5469897 0.0 0.020682689 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(16.1010, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -73.48702 -1.3699894 66.70235 237.90625\n",
      "activ tensor(16.1010, grad_fn=<CopyBackwards>) tensor(0.0677, grad_fn=<DivBackward0>) tensor(3.5285, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(16.1033, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(16.1010, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(16.1033, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5692449 0.0 0.021686958 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(16.1039, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -69.08677 -1.3105199 69.328865 238.29037\n",
      "activ tensor(16.1039, grad_fn=<CopyBackwards>) tensor(0.0676, grad_fn=<DivBackward0>) tensor(2.9125, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(16.1140, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(16.1039, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(16.1140, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.581465 0.0 0.02234238 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(16.1146, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -69.459335 -1.313383 69.51479 238.61932\n",
      "activ tensor(16.1146, grad_fn=<CopyBackwards>) tensor(0.0675, grad_fn=<DivBackward0>) tensor(3.2339, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(16.1314, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(16.1146, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(16.1314, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55507076 0.0 0.021190109 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(16.1309, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -69.3319 -1.2321028 68.672195 238.93513\n",
      "activ tensor(16.1309, grad_fn=<CopyBackwards>) tensor(0.0675, grad_fn=<DivBackward0>) tensor(3.1050, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(16.1523, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(16.1309, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(16.1523, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5601142 0.0 0.021486105 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(16.1525, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -69.72794 -1.1624144 67.636665 239.1994\n",
      "activ tensor(16.1525, grad_fn=<CopyBackwards>) tensor(0.0675, grad_fn=<DivBackward0>) tensor(3.3220, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(16.1633, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(16.1525, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(16.1633, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57017034 0.0 0.021919526 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(16.1642, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -70.32056 -1.163538 67.03555 239.40457\n",
      "activ tensor(16.1642, grad_fn=<CopyBackwards>) tensor(0.0675, grad_fn=<DivBackward0>) tensor(2.9357, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(16.2051, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(16.1642, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(16.2051, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5766078 0.0 0.022247238 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(16.2038, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -69.17352 -1.3073523 66.089554 239.58023\n",
      "activ tensor(16.2038, grad_fn=<CopyBackwards>) tensor(0.0676, grad_fn=<DivBackward0>) tensor(3.2176, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(16.2384, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(16.2038, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(16.2384, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57548934 0.0 0.022236666 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(16.2385, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -71.272545 -1.3726125 70.37694 239.86191\n",
      "activ tensor(16.2385, grad_fn=<CopyBackwards>) tensor(0.0677, grad_fn=<DivBackward0>) tensor(3.1073, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(16.2569, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(16.2385, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(16.2569, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5539083 0.0 0.020968111 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(16.2571, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -72.31241 -1.2999463 69.40623 240.22514\n",
      "activ tensor(16.2571, grad_fn=<CopyBackwards>) tensor(0.0677, grad_fn=<DivBackward0>) tensor(3.1636, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(16.3440, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(16.2571, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(16.3440, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56594193 0.0 0.021898383 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(16.3442, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -69.47981 -1.3101758 67.06777 240.7285\n",
      "activ tensor(16.3442, grad_fn=<CopyBackwards>) tensor(0.0679, grad_fn=<DivBackward0>) tensor(2.8736, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(16.3384, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(16.3442, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(16.3384, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.58688104 0.0 0.022701802 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(16.3383, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -71.11269 -1.5127481 68.186646 241.29185\n",
      "activ tensor(16.3383, grad_fn=<CopyBackwards>) tensor(0.0677, grad_fn=<DivBackward0>) tensor(3.2958, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(16.3612, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(16.3383, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(16.3612, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5496609 0.0 0.020830687 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(16.3613, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -71.05181 -1.3669376 66.757034 241.97693\n",
      "activ tensor(16.3613, grad_fn=<CopyBackwards>) tensor(0.0676, grad_fn=<DivBackward0>) tensor(2.8732, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(16.3705, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(16.3613, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(16.3705, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5772174 0.0 0.02201467 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(16.3707, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -70.78926 -1.3897337 70.79144 242.71117\n",
      "activ tensor(16.3707, grad_fn=<CopyBackwards>) tensor(0.0674, grad_fn=<DivBackward0>) tensor(3.0450, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(16.3832, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(16.3707, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(16.3832, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5404162 0.0 0.020576976 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(16.3841, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -71.57866 -1.3095826 71.89844 243.53468\n",
      "activ tensor(16.3841, grad_fn=<CopyBackwards>) tensor(0.0673, grad_fn=<DivBackward0>) tensor(2.9920, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(16.4066, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(16.3841, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(16.4066, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57620156 0.0 0.022257807 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(16.4067, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -72.46935 -1.2453434 69.03305 244.31638\n",
      "activ tensor(16.4067, grad_fn=<CopyBackwards>) tensor(0.0672, grad_fn=<DivBackward0>) tensor(3.6932, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(16.4758, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(16.4067, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(16.4758, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57681143 0.0 0.022226095 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(16.4762, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -73.74481 -1.3000864 69.4136 245.00093\n",
      "activ tensor(16.4762, grad_fn=<CopyBackwards>) tensor(0.0672, grad_fn=<DivBackward0>) tensor(3.0943, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(16.4789, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(16.4762, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(16.4789, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.58065826 0.0 0.022426952 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(16.4784, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -73.50808 -1.400654 70.16637 245.59749\n",
      "activ tensor(16.4784, grad_fn=<CopyBackwards>) tensor(0.0671, grad_fn=<DivBackward0>) tensor(3.4161, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(16.5108, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(16.4784, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(16.5108, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5566516 0.0 0.02128525 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(16.5109, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -74.18942 -1.3641376 71.724396 246.20155\n",
      "activ tensor(16.5109, grad_fn=<CopyBackwards>) tensor(0.0671, grad_fn=<DivBackward0>) tensor(2.9424, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(16.5453, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(16.5109, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(16.5453, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5603234 0.0 0.021549532 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(16.5463, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -72.590836 -1.2700281 72.1461 246.82347\n",
      "activ tensor(16.5463, grad_fn=<CopyBackwards>) tensor(0.0670, grad_fn=<DivBackward0>) tensor(3.2203, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(16.5605, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(16.5463, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(16.5605, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5623066 0.0 0.021380395 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(16.5606, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -72.46575 -1.3083274 73.731445 247.42844\n",
      "activ tensor(16.5606, grad_fn=<CopyBackwards>) tensor(0.0669, grad_fn=<DivBackward0>) tensor(3.2038, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(16.5748, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(16.5606, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(16.5748, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5567568 0.0 0.021147825 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(16.5739, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -76.5666 -1.2705699 70.14163 247.97794\n",
      "activ tensor(16.5739, grad_fn=<CopyBackwards>) tensor(0.0668, grad_fn=<DivBackward0>) tensor(3.6513, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(16.6085, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(16.5739, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(16.6085, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55231947 0.0 0.021211252 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(16.6097, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -76.92623 -1.1291945 68.80493 248.478\n",
      "activ tensor(16.6097, grad_fn=<CopyBackwards>) tensor(0.0668, grad_fn=<DivBackward0>) tensor(3.2568, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(16.6434, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(16.6097, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(16.6434, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5580184 0.0 0.021168966 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(16.6427, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -72.44744 -1.4502008 74.23152 248.94887\n",
      "activ tensor(16.6427, grad_fn=<CopyBackwards>) tensor(0.0669, grad_fn=<DivBackward0>) tensor(2.9886, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(16.6519, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(16.6427, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(16.6519, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5491276 0.0 0.020693261 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(16.6527, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -73.347046 -1.3959754 72.42066 249.51765\n",
      "activ tensor(16.6527, grad_fn=<CopyBackwards>) tensor(0.0667, grad_fn=<DivBackward0>) tensor(3.7270, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(16.6955, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(16.6527, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(16.6955, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56625235 0.0 0.021771528 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(16.6967, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -73.08767 -1.3315018 71.86782 250.03117\n",
      "activ tensor(16.6967, grad_fn=<CopyBackwards>) tensor(0.0668, grad_fn=<DivBackward0>) tensor(3.7055, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(16.7028, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(16.6967, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(16.7028, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5737559 0.0 0.021803243 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(16.7028, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -73.13244 -1.4779102 71.591866 250.54604\n",
      "activ tensor(16.7028, grad_fn=<CopyBackwards>) tensor(0.0667, grad_fn=<DivBackward0>) tensor(3.0719, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(16.7467, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(16.7028, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(16.7467, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5738579 0.0 0.021908958 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(16.7467, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -73.19723 -1.4963613 69.22971 251.06024\n",
      "activ tensor(16.7467, grad_fn=<CopyBackwards>) tensor(0.0667, grad_fn=<DivBackward0>) tensor(3.0305, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(16.7644, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(16.7467, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(16.7644, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5448433 0.0 0.0207884 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(16.7649, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -73.45056 -1.3674346 71.30519 251.57965\n",
      "activ tensor(16.7649, grad_fn=<CopyBackwards>) tensor(0.0666, grad_fn=<DivBackward0>) tensor(3.0343, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(16.7743, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(16.7649, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(16.7743, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57569265 0.0 0.022300094 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(16.7748, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -75.64255 -1.3500872 72.73782 252.06255\n",
      "activ tensor(16.7748, grad_fn=<CopyBackwards>) tensor(0.0665, grad_fn=<DivBackward0>) tensor(3.7318, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(16.7753, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(16.7748, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(16.7753, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5724269 0.0 0.02195124 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(16.7760, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -74.99742 -1.3587909 73.089066 252.54451\n",
      "activ tensor(16.7760, grad_fn=<CopyBackwards>) tensor(0.0664, grad_fn=<DivBackward0>) tensor(3.5289, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(16.8073, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(16.7760, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(16.8073, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55359066 0.0 0.021063253 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(16.8074, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -73.12288 -1.3369843 70.51871 252.92027\n",
      "activ tensor(16.8074, grad_fn=<CopyBackwards>) tensor(0.0665, grad_fn=<DivBackward0>) tensor(3.4286, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(16.8361, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(16.8074, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(16.8361, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56438684 0.0 0.02143325 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(16.8362, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -75.488846 -1.3356702 70.58205 253.23538\n",
      "activ tensor(16.8362, grad_fn=<CopyBackwards>) tensor(0.0665, grad_fn=<DivBackward0>) tensor(3.3738, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(16.9049, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(16.8362, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(16.9049, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5757946 0.0 0.022056952 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(16.9042, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -74.63638 -1.5168595 71.61549 253.54121\n",
      "activ tensor(16.9042, grad_fn=<CopyBackwards>) tensor(0.0667, grad_fn=<DivBackward0>) tensor(3.5049, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(16.9521, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(16.9042, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(16.9521, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5686272 0.0 0.021676388 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(16.9524, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -75.61284 -1.2940145 72.98247 253.91159\n",
      "activ tensor(16.9524, grad_fn=<CopyBackwards>) tensor(0.0668, grad_fn=<DivBackward0>) tensor(3.1283, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(16.9668, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(16.9524, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(16.9668, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57589614 0.0 0.022183811 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(16.9668, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -74.203186 -1.3721194 76.089096 254.32591\n",
      "activ tensor(16.9668, grad_fn=<CopyBackwards>) tensor(0.0667, grad_fn=<DivBackward0>) tensor(3.9404, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(17.0285, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(16.9668, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(17.0285, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56914216 0.0 0.021813814 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(17.0276, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -76.0781 -1.4970934 71.830475 254.83876\n",
      "activ tensor(17.0276, grad_fn=<CopyBackwards>) tensor(0.0668, grad_fn=<DivBackward0>) tensor(3.0959, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(17.0731, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(17.0276, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(17.0731, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5672859 0.0 0.021750387 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(17.0724, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -77.11673 -1.3426292 72.017334 255.42902\n",
      "activ tensor(17.0724, grad_fn=<CopyBackwards>) tensor(0.0668, grad_fn=<DivBackward0>) tensor(3.1543, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(17.1014, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(17.0724, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(17.1014, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5716073 0.0 0.021919526 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(17.1026, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -76.452805 -1.292471 74.95849 256.08304\n",
      "activ tensor(17.1026, grad_fn=<CopyBackwards>) tensor(0.0668, grad_fn=<DivBackward0>) tensor(3.3500, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(17.1300, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(17.1026, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(17.1300, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5607415 0.0 0.02161296 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(17.1297, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -77.06514 -1.4024206 73.88243 256.80243\n",
      "activ tensor(17.1297, grad_fn=<CopyBackwards>) tensor(0.0667, grad_fn=<DivBackward0>) tensor(3.7085, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(17.1739, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(17.1297, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(17.1739, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5568621 0.0 0.021306394 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(17.1753, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -76.205925 -1.4516053 76.6841 257.59094\n",
      "activ tensor(17.1753, grad_fn=<CopyBackwards>) tensor(0.0667, grad_fn=<DivBackward0>) tensor(3.3983, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(17.1959, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(17.1753, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(17.1959, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5711973 0.0 0.0217821 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(17.1962, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -76.08154 -1.4457314 76.10995 258.35315\n",
      "activ tensor(17.1962, grad_fn=<CopyBackwards>) tensor(0.0666, grad_fn=<DivBackward0>) tensor(3.3666, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(17.1854, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(17.1962, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(17.1854, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5633476 0.0 0.021549534 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(17.1855, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -80.05245 -1.4850658 73.46789 259.16995\n",
      "activ tensor(17.1855, grad_fn=<CopyBackwards>) tensor(0.0663, grad_fn=<DivBackward0>) tensor(3.1099, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(17.1814, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(17.1855, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(17.1814, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5741642 0.0 0.022067526 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(17.1819, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -80.32683 -1.578351 73.102135 259.9487\n",
      "activ tensor(17.1819, grad_fn=<CopyBackwards>) tensor(0.0661, grad_fn=<DivBackward0>) tensor(3.5069, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(17.2087, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(17.1819, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(17.2087, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5822705 0.0 0.02244809 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(17.2091, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -75.774925 -1.3480905 76.61311 260.72455\n",
      "activ tensor(17.2091, grad_fn=<CopyBackwards>) tensor(0.0660, grad_fn=<DivBackward0>) tensor(3.4025, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(17.2608, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(17.2091, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(17.2608, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56769884 0.0 0.021644674 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(17.2610, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -75.73139 -1.3940763 77.360374 261.3964\n",
      "activ tensor(17.2610, grad_fn=<CopyBackwards>) tensor(0.0660, grad_fn=<DivBackward0>) tensor(3.9659, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(17.2534, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(17.2610, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(17.2534, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5479527 0.0 0.021063255 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(17.2536, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -76.851494 -1.3751266 74.02216 261.97003\n",
      "activ tensor(17.2536, grad_fn=<CopyBackwards>) tensor(0.0659, grad_fn=<DivBackward0>) tensor(3.0768, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(17.2915, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(17.2536, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(17.2915, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.59343326 0.0 0.022944938 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(17.2913, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -77.08276 -1.4143322 75.22685 262.436\n",
      "activ tensor(17.2913, grad_fn=<CopyBackwards>) tensor(0.0659, grad_fn=<DivBackward0>) tensor(3.5755, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(17.3778, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(17.2913, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(17.3778, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55316734 0.0 0.021190109 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(17.3781, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -77.982056 -1.382904 73.09613 262.79852\n",
      "activ tensor(17.3781, grad_fn=<CopyBackwards>) tensor(0.0661, grad_fn=<DivBackward0>) tensor(3.6236, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(17.4014, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(17.3781, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(17.4014, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55189496 0.0 0.020968111 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(17.4016, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -76.47527 -1.3835073 74.07699 263.06158\n",
      "activ tensor(17.4016, grad_fn=<CopyBackwards>) tensor(0.0662, grad_fn=<DivBackward0>) tensor(4.0815, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(17.4314, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(17.4016, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(17.4314, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55969554 0.0 0.021486104 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(17.4324, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -78.37827 -1.4227264 77.62887 263.31146\n",
      "activ tensor(17.4324, grad_fn=<CopyBackwards>) tensor(0.0662, grad_fn=<DivBackward0>) tensor(3.6792, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(17.4367, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(17.4324, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(17.4367, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55359083 0.0 0.021137254 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(17.4366, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -78.70215 -1.4215508 74.8349 263.56985\n",
      "activ tensor(17.4366, grad_fn=<CopyBackwards>) tensor(0.0662, grad_fn=<DivBackward0>) tensor(3.3907, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(17.4908, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(17.4366, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(17.4908, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5747762 0.0 0.021792673 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(17.4904, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -78.419304 -1.4492705 74.680084 263.88196\n",
      "activ tensor(17.4904, grad_fn=<CopyBackwards>) tensor(0.0663, grad_fn=<DivBackward0>) tensor(3.3091, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(17.5222, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(17.4904, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(17.5222, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5673893 0.0 0.021739816 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(17.5215, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -79.1019 -1.4505849 73.74057 264.22443\n",
      "activ tensor(17.5215, grad_fn=<CopyBackwards>) tensor(0.0663, grad_fn=<DivBackward0>) tensor(3.6044, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(17.5389, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(17.5215, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(17.5389, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55337894 0.0 0.020978684 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(17.5397, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -76.56739 -1.3436863 74.62232 264.6364\n",
      "activ tensor(17.5397, grad_fn=<CopyBackwards>) tensor(0.0663, grad_fn=<DivBackward0>) tensor(3.3970, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(17.5688, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(17.5397, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(17.5688, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5660451 0.0 0.021750387 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(17.5680, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -78.7784 -1.4681891 77.597244 265.08374\n",
      "activ tensor(17.5680, grad_fn=<CopyBackwards>) tensor(0.0663, grad_fn=<DivBackward0>) tensor(3.6299, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(17.6194, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(17.5680, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(17.6194, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5572828 0.0 0.02116897 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(17.6202, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -78.26596 -1.5683397 77.938675 265.63367\n",
      "activ tensor(17.6202, grad_fn=<CopyBackwards>) tensor(0.0663, grad_fn=<DivBackward0>) tensor(3.1666, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(17.6201, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(17.6202, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(17.6201, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5706843 0.0 0.021686962 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(17.6208, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -79.66452 -1.6089482 76.08487 266.20993\n",
      "activ tensor(17.6208, grad_fn=<CopyBackwards>) tensor(0.0662, grad_fn=<DivBackward0>) tensor(3.4265, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(17.6956, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(17.6208, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(17.6956, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57447034 0.0 0.022088666 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(17.6953, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -80.499176 -1.5171463 74.52706 266.86578\n",
      "activ tensor(17.6953, grad_fn=<CopyBackwards>) tensor(0.0663, grad_fn=<DivBackward0>) tensor(3.4196, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(17.7405, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(17.6953, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(17.7405, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5712999 0.0 0.022056954 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(17.7408, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -79.27344 -1.5892318 78.52204 267.58252\n",
      "activ tensor(17.7408, grad_fn=<CopyBackwards>) tensor(0.0663, grad_fn=<DivBackward0>) tensor(4.2124, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(17.7917, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(17.7408, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(17.7917, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5657347 0.0 0.021570675 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(17.7918, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -80.07678 -1.5569086 78.27788 268.33652\n",
      "activ tensor(17.7918, grad_fn=<CopyBackwards>) tensor(0.0663, grad_fn=<DivBackward0>) tensor(3.4802, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(17.8271, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(17.7918, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(17.8271, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5650094 0.0 0.021708101 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(17.8270, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -79.68961 -1.477607 78.569305 269.09882\n",
      "activ tensor(17.8270, grad_fn=<CopyBackwards>) tensor(0.0662, grad_fn=<DivBackward0>) tensor(3.2231, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(17.8566, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(17.8270, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(17.8566, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56873024 0.0 0.021739816 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(17.8572, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -79.859535 -1.3752452 80.65905 269.87982\n",
      "activ tensor(17.8572, grad_fn=<CopyBackwards>) tensor(0.0662, grad_fn=<DivBackward0>) tensor(3.4636, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(17.8644, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(17.8572, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(17.8644, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5714025 0.0 0.021834956 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(17.8641, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -83.37576 -1.5018604 75.68732 270.55188\n",
      "activ tensor(17.8641, grad_fn=<CopyBackwards>) tensor(0.0660, grad_fn=<DivBackward0>) tensor(3.7873, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(17.8904, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(17.8641, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(17.8904, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5697594 0.0 0.021887815 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(17.8913, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -83.69351 -1.3787107 77.203926 271.17902\n",
      "activ tensor(17.8913, grad_fn=<CopyBackwards>) tensor(0.0660, grad_fn=<DivBackward0>) tensor(3.5497, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(17.9111, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(17.8913, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(17.9111, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5342005 0.0 0.020037841 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(17.9113, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -79.13801 -1.3874742 80.49736 271.75232\n",
      "activ tensor(17.9113, grad_fn=<CopyBackwards>) tensor(0.0659, grad_fn=<DivBackward0>) tensor(3.6836, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(17.9242, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(17.9113, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(17.9242, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5708893 0.0 0.021834956 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(17.9250, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -79.5777 -1.5193346 79.8977 272.30682\n",
      "activ tensor(17.9250, grad_fn=<CopyBackwards>) tensor(0.0658, grad_fn=<DivBackward0>) tensor(3.4411, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(17.9505, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(17.9250, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(17.9505, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5596957 0.0 0.021665817 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(17.9511, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -80.65935 -1.4897624 78.61783 272.82242\n",
      "activ tensor(17.9511, grad_fn=<CopyBackwards>) tensor(0.0658, grad_fn=<DivBackward0>) tensor(3.4829, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(17.9722, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(17.9511, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(17.9722, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57252926 0.0 0.021792674 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(17.9720, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -80.96894 -1.4545696 77.09995 273.3516\n",
      "activ tensor(17.9720, grad_fn=<CopyBackwards>) tensor(0.0657, grad_fn=<DivBackward0>) tensor(3.4681, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(17.9963, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(17.9720, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(17.9963, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5680085 0.0 0.021993529 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(17.9958, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -81.56317 -1.420715 77.586105 273.84607\n",
      "activ tensor(17.9958, grad_fn=<CopyBackwards>) tensor(0.0657, grad_fn=<DivBackward0>) tensor(3.8393, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(18.0253, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(17.9958, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(18.0253, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55274355 0.0 0.0208624 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(18.0260, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -79.39743 -1.4170184 77.654594 274.28165\n",
      "activ tensor(18.0260, grad_fn=<CopyBackwards>) tensor(0.0657, grad_fn=<DivBackward0>) tensor(3.2338, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(18.0768, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(18.0260, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(18.0768, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5759981 0.0 0.022141524 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(18.0763, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -82.15088 -1.460799 80.51934 274.71948\n",
      "activ tensor(18.0763, grad_fn=<CopyBackwards>) tensor(0.0658, grad_fn=<DivBackward0>) tensor(3.6310, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(18.1142, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(18.0763, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(18.1142, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5588576 0.0 0.021316966 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(18.1149, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -80.79607 -1.50636 79.12527 275.15335\n",
      "activ tensor(18.1149, grad_fn=<CopyBackwards>) tensor(0.0658, grad_fn=<DivBackward0>) tensor(3.5983, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(18.1538, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(18.1149, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(18.1538, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5481666 0.0 0.02084126 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(18.1542, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -81.644035 -1.6798567 76.44411 275.4866\n",
      "activ tensor(18.1542, grad_fn=<CopyBackwards>) tensor(0.0659, grad_fn=<DivBackward0>) tensor(4.0989, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(18.1612, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(18.1542, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(18.1612, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55728275 0.0 0.021158395 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(18.1614, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -81.10303 -1.5069832 78.511055 275.89804\n",
      "activ tensor(18.1614, grad_fn=<CopyBackwards>) tensor(0.0658, grad_fn=<DivBackward0>) tensor(3.7119, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(18.2008, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(18.1614, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(18.2008, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5694509 0.0 0.021898385 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(18.2013, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -79.645706 -1.630105 77.40753 276.35043\n",
      "activ tensor(18.2013, grad_fn=<CopyBackwards>) tensor(0.0659, grad_fn=<DivBackward0>) tensor(3.8208, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(18.2701, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(18.2013, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(18.2701, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5667692 0.0 0.02177153 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(18.2699, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -82.64574 -1.6219484 81.03444 276.83353\n",
      "activ tensor(18.2699, grad_fn=<CopyBackwards>) tensor(0.0660, grad_fn=<DivBackward0>) tensor(3.6527, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(18.2814, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(18.2699, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(18.2814, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.54998046 0.0 0.020968115 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(18.2810, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -80.79893 -1.5595443 81.90081 277.3341\n",
      "activ tensor(18.2810, grad_fn=<CopyBackwards>) tensor(0.0659, grad_fn=<DivBackward0>) tensor(3.3991, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(18.3208, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(18.2810, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(18.3208, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56147236 0.0 0.021338107 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(18.3208, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -83.15831 -1.4723558 77.86383 277.88437\n",
      "activ tensor(18.3208, grad_fn=<CopyBackwards>) tensor(0.0659, grad_fn=<DivBackward0>) tensor(3.3180, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(18.3416, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(18.3208, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(18.3416, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56417924 0.0 0.021486104 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(18.3409, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -83.87803 -1.5651389 79.16417 278.39212\n",
      "activ tensor(18.3409, grad_fn=<CopyBackwards>) tensor(0.0659, grad_fn=<DivBackward0>) tensor(3.9669, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(18.3451, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(18.3409, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(18.3451, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5876791 0.0 0.022691231 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(18.3456, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -82.09225 -1.6142207 81.05063 278.93954\n",
      "activ tensor(18.3456, grad_fn=<CopyBackwards>) tensor(0.0658, grad_fn=<DivBackward0>) tensor(3.4566, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(18.3363, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(18.3456, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(18.3363, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55348474 0.0 0.021063255 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(18.3365, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -83.02313 -1.4688911 82.312416 279.53018\n",
      "activ tensor(18.3365, grad_fn=<CopyBackwards>) tensor(0.0656, grad_fn=<DivBackward0>) tensor(3.6136, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(18.3592, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(18.3365, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(18.3592, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5622025 0.0 0.021475535 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(18.3597, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -83.31449 -1.5959458 82.044914 280.12372\n",
      "activ tensor(18.3597, grad_fn=<CopyBackwards>) tensor(0.0655, grad_fn=<DivBackward0>) tensor(3.6400, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(18.3860, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(18.3597, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(18.3860, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56718266 0.0 0.021803243 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(18.3859, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -83.49519 -1.6591821 82.73779 280.73926\n",
      "activ tensor(18.3859, grad_fn=<CopyBackwards>) tensor(0.0655, grad_fn=<DivBackward0>) tensor(3.9477, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(18.4314, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(18.3859, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(18.4314, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5686272 0.0 0.02168696 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(18.4319, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -86.79421 -1.4953294 80.49855 281.45358\n",
      "activ tensor(18.4319, grad_fn=<CopyBackwards>) tensor(0.0655, grad_fn=<DivBackward0>) tensor(3.6818, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(18.4357, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(18.4319, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(18.4357, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5579132 0.0 0.021623531 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(18.4360, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -86.85751 -1.5131508 79.433205 282.14627\n",
      "activ tensor(18.4360, grad_fn=<CopyBackwards>) tensor(0.0653, grad_fn=<DivBackward0>) tensor(3.3382, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(18.4644, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(18.4360, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(18.4644, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5639713 0.0 0.021538962 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(18.4651, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -82.607475 -1.4621654 85.05777 282.8248\n",
      "activ tensor(18.4651, grad_fn=<CopyBackwards>) tensor(0.0653, grad_fn=<DivBackward0>) tensor(3.6126, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(18.5008, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(18.4651, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(18.5008, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5721196 0.0 0.022014668 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(18.5009, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -82.8009 -1.5063324 83.06871 283.49274\n",
      "activ tensor(18.5009, grad_fn=<CopyBackwards>) tensor(0.0653, grad_fn=<DivBackward0>) tensor(3.8779, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(18.5499, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(18.5009, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(18.5499, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5665628 0.0 0.021750387 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(18.5507, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -84.42776 -1.4767207 81.347496 284.1032\n",
      "activ tensor(18.5507, grad_fn=<CopyBackwards>) tensor(0.0653, grad_fn=<DivBackward0>) tensor(3.9223, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(18.5519, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(18.5507, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(18.5519, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5838783 0.0 0.022596087 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(18.5521, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -84.85554 -1.6686213 81.4811 284.62888\n",
      "activ tensor(18.5521, grad_fn=<CopyBackwards>) tensor(0.0652, grad_fn=<DivBackward0>) tensor(3.8234, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(18.6158, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(18.5521, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(18.6158, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5637636 0.0 0.021401534 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(18.6160, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -83.34354 -1.5571108 79.789604 285.1989\n",
      "activ tensor(18.6160, grad_fn=<CopyBackwards>) tensor(0.0653, grad_fn=<DivBackward0>) tensor(4.0200, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(18.6280, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(18.6160, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(18.6280, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56053257 0.0 0.021486105 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(18.6286, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -82.46591 -1.5600414 82.67516 285.72488\n",
      "activ tensor(18.6286, grad_fn=<CopyBackwards>) tensor(0.0652, grad_fn=<DivBackward0>) tensor(3.6419, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(18.6577, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(18.6286, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(18.6577, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5783331 0.0 0.022204952 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(18.6576, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -85.97342 -1.6296061 83.24662 286.20636\n",
      "activ tensor(18.6576, grad_fn=<CopyBackwards>) tensor(0.0652, grad_fn=<DivBackward0>) tensor(3.7808, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(18.6793, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(18.6576, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(18.6793, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5653202 0.0 0.021253536 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(18.6786, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -83.7094 -1.5209289 82.16275 286.6768\n",
      "activ tensor(18.6786, grad_fn=<CopyBackwards>) tensor(0.0652, grad_fn=<DivBackward0>) tensor(4.1255, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(18.7375, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(18.6786, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(18.7375, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5757945 0.0 0.022088667 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(18.7379, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -84.0983 -1.4206321 80.47978 287.14328\n",
      "activ tensor(18.7379, grad_fn=<CopyBackwards>) tensor(0.0653, grad_fn=<DivBackward0>) tensor(3.5137, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(18.7850, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(18.7379, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(18.7850, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5718123 0.0 0.021771528 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(18.7844, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -85.65475 -1.5545353 80.73091 287.51178\n",
      "activ tensor(18.7844, grad_fn=<CopyBackwards>) tensor(0.0653, grad_fn=<DivBackward0>) tensor(3.3910, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(18.7832, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(18.7844, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(18.7832, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56759584 0.0 0.021908956 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(18.7831, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -84.01907 -1.6383303 82.40726 287.90094\n",
      "activ tensor(18.7831, grad_fn=<CopyBackwards>) tensor(0.0652, grad_fn=<DivBackward0>) tensor(3.7828, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(18.8254, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(18.7831, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(18.8254, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57344943 0.0 0.022046383 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(18.8258, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -87.04612 -1.6007949 83.26827 288.2758\n",
      "activ tensor(18.8258, grad_fn=<CopyBackwards>) tensor(0.0653, grad_fn=<DivBackward0>) tensor(4.0178, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(18.8899, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(18.8258, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(18.8899, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5507259 0.0 0.020968113 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(18.8898, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -83.48003 -1.6894921 85.609146 288.69113\n",
      "activ tensor(18.8898, grad_fn=<CopyBackwards>) tensor(0.0654, grad_fn=<DivBackward0>) tensor(3.9016, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(18.9090, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(18.8898, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(18.9090, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56986207 0.0 0.022067526 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(18.9089, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -86.61707 -1.7703674 81.19524 289.19394\n",
      "activ tensor(18.9089, grad_fn=<CopyBackwards>) tensor(0.0654, grad_fn=<DivBackward0>) tensor(3.5989, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(18.9285, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(18.9089, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(18.9285, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.559591 0.0 0.02111611 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(18.9283, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -87.16783 -1.6381412 81.49562 289.7794\n",
      "activ tensor(18.9283, grad_fn=<CopyBackwards>) tensor(0.0653, grad_fn=<DivBackward0>) tensor(3.5206, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(18.9561, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(18.9283, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(18.9561, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5805577 0.0 0.022469234 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(18.9567, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -84.96362 -1.6614603 86.02724 290.36475\n",
      "activ tensor(18.9567, grad_fn=<CopyBackwards>) tensor(0.0653, grad_fn=<DivBackward0>) tensor(3.5770, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(19.0128, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(18.9567, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(19.0128, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.53922236 0.0 0.020259837 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(19.0138, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -85.954605 -1.6102946 84.511475 290.94623\n",
      "activ tensor(19.0138, grad_fn=<CopyBackwards>) tensor(0.0654, grad_fn=<DivBackward0>) tensor(3.7917, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(19.0517, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(19.0138, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(19.0517, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.58377784 0.0 0.02226838 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(19.0515, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -86.92802 -1.685265 86.0912 291.56018\n",
      "activ tensor(19.0515, grad_fn=<CopyBackwards>) tensor(0.0653, grad_fn=<DivBackward0>) tensor(4.4097, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(19.1085, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(19.0515, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(19.1085, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5590672 0.0 0.021390963 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(19.1095, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -87.2172 -1.581508 85.597946 292.20972\n",
      "activ tensor(19.1095, grad_fn=<CopyBackwards>) tensor(0.0654, grad_fn=<DivBackward0>) tensor(3.7394, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(19.1428, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(19.1095, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(19.1428, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57436836 0.0 0.022152094 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(19.1427, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -90.01923 -1.6712447 83.37746 292.79544\n",
      "activ tensor(19.1427, grad_fn=<CopyBackwards>) tensor(0.0654, grad_fn=<DivBackward0>) tensor(3.5358, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(19.1567, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(19.1427, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(19.1567, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55843794 0.0 0.021242967 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(19.1569, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -90.11321 -1.6323721 84.26515 293.38925\n",
      "activ tensor(19.1569, grad_fn=<CopyBackwards>) tensor(0.0653, grad_fn=<DivBackward0>) tensor(3.5294, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(19.1658, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(19.1569, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(19.1658, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5674926 0.0 0.021528391 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(19.1666, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -85.952126 -1.6216186 87.16203 293.97018\n",
      "activ tensor(19.1666, grad_fn=<CopyBackwards>) tensor(0.0652, grad_fn=<DivBackward0>) tensor(4.0910, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(19.2046, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(19.1666, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(19.2046, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.58287394 0.0 0.022363521 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(19.2046, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -85.549065 -1.6201545 87.74428 294.57535\n",
      "activ tensor(19.2046, grad_fn=<CopyBackwards>) tensor(0.0652, grad_fn=<DivBackward0>) tensor(3.9744, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(19.2350, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(19.2046, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(19.2350, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56831795 0.0 0.022067526 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(19.2346, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -88.11571 -1.4786638 83.83107 295.1733\n",
      "activ tensor(19.2346, grad_fn=<CopyBackwards>) tensor(0.0652, grad_fn=<DivBackward0>) tensor(3.7631, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(19.2137, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(19.2346, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(19.2137, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57058156 0.0 0.02195124 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(19.2138, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -88.68641 -1.4479519 84.79019 295.79965\n",
      "activ tensor(19.2138, grad_fn=<CopyBackwards>) tensor(0.0650, grad_fn=<DivBackward0>) tensor(3.3234, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(19.2458, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(19.2138, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(19.2458, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56313974 0.0 0.02144382 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(19.2458, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -86.76162 -1.544044 84.152565 296.34042\n",
      "activ tensor(19.2458, grad_fn=<CopyBackwards>) tensor(0.0649, grad_fn=<DivBackward0>) tensor(3.8721, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(19.2792, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(19.2458, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(19.2792, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5562306 0.0 0.021020971 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(19.2794, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -87.147446 -1.6632104 85.1788 296.8457\n",
      "activ tensor(19.2794, grad_fn=<CopyBackwards>) tensor(0.0649, grad_fn=<DivBackward0>) tensor(4.0704, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(19.2888, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(19.2794, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(19.2888, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57263136 0.0 0.021961812 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(19.2887, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -88.2632 -1.709863 88.04634 297.3751\n",
      "activ tensor(19.2887, grad_fn=<CopyBackwards>) tensor(0.0649, grad_fn=<DivBackward0>) tensor(3.9256, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(19.3534, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(19.2887, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(19.3534, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5710948 0.0 0.021708101 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(19.3537, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -86.93825 -1.6051497 84.135925 297.95517\n",
      "activ tensor(19.3537, grad_fn=<CopyBackwards>) tensor(0.0650, grad_fn=<DivBackward0>) tensor(3.7422, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(19.3623, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(19.3537, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(19.3623, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5712997 0.0 0.022088666 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(19.3626, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -87.678024 -1.7351179 84.21672 298.52115\n",
      "activ tensor(19.3626, grad_fn=<CopyBackwards>) tensor(0.0649, grad_fn=<DivBackward0>) tensor(3.5038, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(19.4052, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(19.3626, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(19.4052, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5720173 0.0 0.021993523 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(19.4048, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -88.53195 -1.6989346 84.625275 299.09833\n",
      "activ tensor(19.4048, grad_fn=<CopyBackwards>) tensor(0.0649, grad_fn=<DivBackward0>) tensor(3.9224, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(19.4392, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(19.4048, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(19.4392, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57579464 0.0 0.022141524 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(19.4396, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -87.67703 -1.711396 84.96602 299.63754\n",
      "activ tensor(19.4396, grad_fn=<CopyBackwards>) tensor(0.0649, grad_fn=<DivBackward0>) tensor(4.7472, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(19.4412, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(19.4396, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(19.4412, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5626191 0.0 0.021253537 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(19.4422, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -89.810524 -1.6149915 88.008804 300.12662\n",
      "activ tensor(19.4422, grad_fn=<CopyBackwards>) tensor(0.0648, grad_fn=<DivBackward0>) tensor(3.9396, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(19.4524, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(19.4422, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(19.4524, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.54998064 0.0 0.021084398 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(19.4523, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -87.21454 -1.6875851 87.37494 300.60104\n",
      "activ tensor(19.4523, grad_fn=<CopyBackwards>) tensor(0.0647, grad_fn=<DivBackward0>) tensor(3.6211, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(19.4644, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(19.4523, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(19.4644, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.54870063 0.0 0.02077783 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(19.4652, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -90.10384 -1.6479753 85.28481 301.05396\n",
      "activ tensor(19.4652, grad_fn=<CopyBackwards>) tensor(0.0647, grad_fn=<DivBackward0>) tensor(3.4696, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(19.4932, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(19.4652, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(19.4932, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5618898 0.0 0.021634102 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(19.4930, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -90.38997 -1.6930497 84.54547 301.4038\n",
      "activ tensor(19.4930, grad_fn=<CopyBackwards>) tensor(0.0647, grad_fn=<DivBackward0>) tensor(3.7500, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(19.5541, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(19.4930, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(19.5541, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5839786 0.0 0.022659516 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(19.5539, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -87.82169 -1.8503386 89.13722 301.78958\n",
      "activ tensor(19.5539, grad_fn=<CopyBackwards>) tensor(0.0648, grad_fn=<DivBackward0>) tensor(3.8415, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(19.5543, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(19.5539, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(19.5543, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5649056 0.0 0.021866672 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(19.5547, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -88.955414 -1.7747908 89.1396 302.2207\n",
      "activ tensor(19.5547, grad_fn=<CopyBackwards>) tensor(0.0647, grad_fn=<DivBackward0>) tensor(4.2360, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(19.6045, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(19.5547, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(19.6045, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56376344 0.0 0.021443821 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(19.6042, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -90.50299 -1.6346017 87.792305 302.7113\n",
      "activ tensor(19.6042, grad_fn=<CopyBackwards>) tensor(0.0648, grad_fn=<DivBackward0>) tensor(4.0392, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(19.6542, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(19.6042, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(19.6542, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5648019 0.0 0.021602388 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(19.6540, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -90.941505 -1.6228855 89.91493 303.22644\n",
      "activ tensor(19.6540, grad_fn=<CopyBackwards>) tensor(0.0648, grad_fn=<DivBackward0>) tensor(3.6366, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(19.6765, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(19.6540, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(19.6765, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5535909 0.0 0.020968113 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(19.6766, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -93.23284 -1.5780032 86.14145 303.7959\n",
      "activ tensor(19.6766, grad_fn=<CopyBackwards>) tensor(0.0648, grad_fn=<DivBackward0>) tensor(3.8381, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(19.7191, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(19.6766, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(19.7191, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5593814 0.0 0.021221824 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(19.7187, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -93.20678 -1.7288315 87.90036 304.35956\n",
      "activ tensor(19.7187, grad_fn=<CopyBackwards>) tensor(0.0648, grad_fn=<DivBackward0>) tensor(4.4050, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(19.7614, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(19.7187, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(19.7614, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56105506 0.0 0.021464963 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(19.7621, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -89.469376 -1.7810557 91.470024 305.00723\n",
      "activ tensor(19.7621, grad_fn=<CopyBackwards>) tensor(0.0648, grad_fn=<DivBackward0>) tensor(4.0845, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(19.7934, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(19.7621, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(19.7934, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5848809 0.0 0.022648945 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(19.7942, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -88.98371 -1.7676923 89.75644 305.70624\n",
      "activ tensor(19.7942, grad_fn=<CopyBackwards>) tensor(0.0647, grad_fn=<DivBackward0>) tensor(3.9497, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(19.8146, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(19.7942, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(19.8146, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.54398215 0.0 0.020386694 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(19.8152, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -91.92269 -1.6230431 88.53222 306.42957\n",
      "activ tensor(19.8152, grad_fn=<CopyBackwards>) tensor(0.0647, grad_fn=<DivBackward0>) tensor(3.6470, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(19.8756, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(19.8152, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(19.8756, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57467407 0.0 0.02227895 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(19.8759, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -92.34962 -1.7379867 86.986725 307.1394\n",
      "activ tensor(19.8759, grad_fn=<CopyBackwards>) tensor(0.0647, grad_fn=<DivBackward0>) tensor(4.5487, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(19.9371, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(19.8759, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(19.9371, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5661486 0.0 0.021898387 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(19.9375, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -91.030594 -1.7816523 88.3188 307.86386\n",
      "activ tensor(19.9375, grad_fn=<CopyBackwards>) tensor(0.0648, grad_fn=<DivBackward0>) tensor(4.2070, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(19.9556, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(19.9375, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(19.9556, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5616811 0.0 0.021242967 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(19.9557, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -90.72743 -1.6960851 89.16773 308.55054\n",
      "activ tensor(19.9557, grad_fn=<CopyBackwards>) tensor(0.0647, grad_fn=<DivBackward0>) tensor(3.9019, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(19.9934, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(19.9557, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(19.9934, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5600098 0.0 0.021380391 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(19.9948, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -91.9916 -1.598186 90.481125 309.1748\n",
      "activ tensor(19.9948, grad_fn=<CopyBackwards>) tensor(0.0647, grad_fn=<DivBackward0>) tensor(4.1667, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(20.0357, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(19.9948, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(20.0357, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5587527 0.0 0.02128525 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(20.0353, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -89.29384 -1.6309035 88.55737 309.7329\n",
      "activ tensor(20.0353, grad_fn=<CopyBackwards>) tensor(0.0647, grad_fn=<DivBackward0>) tensor(4.0753, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(20.1013, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(20.0353, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(20.1013, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57006794 0.0 0.021708101 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(20.1013, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -92.15335 -1.5995635 86.169754 310.18924\n",
      "activ tensor(20.1013, grad_fn=<CopyBackwards>) tensor(0.0648, grad_fn=<DivBackward0>) tensor(3.7536, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(20.1198, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(20.1013, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(20.1198, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5619942 0.0 0.021517819 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(20.1196, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -91.030716 -1.6943514 89.18387 310.52185\n",
      "activ tensor(20.1196, grad_fn=<CopyBackwards>) tensor(0.0648, grad_fn=<DivBackward0>) tensor(4.4541, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(20.1585, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(20.1196, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(20.1585, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55136377 0.0 0.021084396 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(20.1581, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -91.95554 -1.7434697 88.30742 310.8798\n",
      "activ tensor(20.1581, grad_fn=<CopyBackwards>) tensor(0.0648, grad_fn=<DivBackward0>) tensor(3.5495, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(20.2050, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(20.1581, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(20.2050, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5832759 0.0 0.02253266 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(20.2058, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -93.60611 -1.7493767 90.73839 311.2427\n",
      "activ tensor(20.2058, grad_fn=<CopyBackwards>) tensor(0.0649, grad_fn=<DivBackward0>) tensor(3.9233, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(20.2286, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(20.2058, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(20.2286, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56780225 0.0 0.021803243 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(20.2278, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -90.62144 -1.6810143 91.68932 311.64655\n",
      "activ tensor(20.2278, grad_fn=<CopyBackwards>) tensor(0.0649, grad_fn=<DivBackward0>) tensor(4.1487, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(20.2446, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(20.2278, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(20.2446, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5582282 0.0 0.021232395 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(20.2439, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -93.3806 -1.6546519 86.99431 312.0621\n",
      "activ tensor(20.2439, grad_fn=<CopyBackwards>) tensor(0.0649, grad_fn=<DivBackward0>) tensor(4.4947, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(20.3350, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(20.2439, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(20.3350, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.58488095 0.0 0.022426948 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(20.3351, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -93.6831 -1.6878407 89.20598 312.52826\n",
      "activ tensor(20.3351, grad_fn=<CopyBackwards>) tensor(0.0651, grad_fn=<DivBackward0>) tensor(4.0203, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(20.3687, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(20.3351, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(20.3687, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5723244 0.0 0.021961812 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(20.3681, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -90.64103 -1.8679712 91.96854 312.9995\n",
      "activ tensor(20.3681, grad_fn=<CopyBackwards>) tensor(0.0651, grad_fn=<DivBackward0>) tensor(3.7499, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(20.4040, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(20.3681, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(20.4040, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57252914 0.0 0.02202524 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(20.4049, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -91.98728 -1.8827987 92.61954 313.575\n",
      "activ tensor(20.4049, grad_fn=<CopyBackwards>) tensor(0.0651, grad_fn=<DivBackward0>) tensor(4.1418, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(20.4611, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(20.4049, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(20.4611, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.59224737 0.0 0.022944942 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(20.4620, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -94.11398 -1.9259903 91.6035 314.19714\n",
      "activ tensor(20.4620, grad_fn=<CopyBackwards>) tensor(0.0651, grad_fn=<DivBackward0>) tensor(4.2176, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(20.4731, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(20.4620, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(20.4731, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5731429 0.0 0.022067524 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(20.4738, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -94.595894 -1.8704113 91.736626 314.8647\n",
      "activ tensor(20.4738, grad_fn=<CopyBackwards>) tensor(0.0650, grad_fn=<DivBackward0>) tensor(4.2892, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(20.4807, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(20.4738, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(20.4807, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55200106 0.0 0.020883545 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(20.4811, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -96.43133 -1.9112014 91.041916 315.55994\n",
      "activ tensor(20.4811, grad_fn=<CopyBackwards>) tensor(0.0649, grad_fn=<DivBackward0>) tensor(4.6330, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(20.5333, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(20.4811, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(20.5333, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5726315 0.0 0.021644674 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(20.5338, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -96.25537 -1.8490993 90.49054 316.23218\n",
      "activ tensor(20.5338, grad_fn=<CopyBackwards>) tensor(0.0649, grad_fn=<DivBackward0>) tensor(3.9313, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(20.5262, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(20.5338, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(20.5262, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5542255 0.0 0.020915257 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(20.5266, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -92.94982 -1.8234193 95.42294 316.88953\n",
      "activ tensor(20.5266, grad_fn=<CopyBackwards>) tensor(0.0648, grad_fn=<DivBackward0>) tensor(4.0792, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(20.5136, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(20.5266, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(20.5136, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5688332 0.0 0.02177153 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(20.5143, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -92.724434 -1.6878626 93.32823 317.53497\n",
      "activ tensor(20.5143, grad_fn=<CopyBackwards>) tensor(0.0646, grad_fn=<DivBackward0>) tensor(4.1666, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(20.5478, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(20.5143, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(20.5478, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5711971 0.0 0.022078097 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(20.5476, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -95.547554 -1.6317909 90.80278 318.10333\n",
      "activ tensor(20.5476, grad_fn=<CopyBackwards>) tensor(0.0646, grad_fn=<DivBackward0>) tensor(4.5680, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(20.5709, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(20.5476, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(20.5709, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56126374 0.0 0.02134868 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(20.5712, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -96.09702 -1.7155814 91.79709 318.60535\n",
      "activ tensor(20.5712, grad_fn=<CopyBackwards>) tensor(0.0646, grad_fn=<DivBackward0>) tensor(4.0314, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(20.6295, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(20.5712, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(20.6295, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5670795 0.0 0.021581246 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(20.6292, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -93.79549 -1.6835241 90.57123 319.01923\n",
      "activ tensor(20.6292, grad_fn=<CopyBackwards>) tensor(0.0647, grad_fn=<DivBackward0>) tensor(3.7714, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(20.6705, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(20.6292, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(20.6705, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5650092 0.0 0.021718672 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(20.6710, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -93.85851 -1.6415402 93.85339 319.44034\n",
      "activ tensor(20.6710, grad_fn=<CopyBackwards>) tensor(0.0647, grad_fn=<DivBackward0>) tensor(3.5631, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(20.6720, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(20.6710, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(20.6720, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5542253 0.0 0.021020968 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(20.6721, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -95.09636 -1.7679302 93.55914 319.8809\n",
      "activ tensor(20.6721, grad_fn=<CopyBackwards>) tensor(0.0646, grad_fn=<DivBackward0>) tensor(4.3943, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(20.6716, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(20.6721, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(20.6716, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5678023 0.0 0.021940669 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(20.6708, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -94.00514 -1.6631417 91.13486 320.4161\n",
      "activ tensor(20.6708, grad_fn=<CopyBackwards>) tensor(0.0645, grad_fn=<DivBackward0>) tensor(4.5514, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(20.6916, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(20.6708, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(20.6916, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5794462 0.0 0.022416376 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(20.6912, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -95.97237 -1.6617073 90.61731 320.9444\n",
      "activ tensor(20.6912, grad_fn=<CopyBackwards>) tensor(0.0645, grad_fn=<DivBackward0>) tensor(3.8840, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(20.6732, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(20.6912, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(20.6732, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5563356 0.0 0.02120068 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(20.6738, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -95.238525 -1.6858785 91.46189 321.48257\n",
      "activ tensor(20.6738, grad_fn=<CopyBackwards>) tensor(0.0643, grad_fn=<DivBackward0>) tensor(3.7426, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(20.7034, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(20.6738, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(20.7034, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57365364 0.0 0.021961812 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(20.7035, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -95.15833 -1.8758954 93.11476 322.03812\n",
      "activ tensor(20.7035, grad_fn=<CopyBackwards>) tensor(0.0643, grad_fn=<DivBackward0>) tensor(3.9225, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(20.7693, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(20.7035, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(20.7693, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5645944 0.0 0.021496676 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(20.7697, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -97.473145 -1.7919976 93.25827 322.65753\n",
      "activ tensor(20.7697, grad_fn=<CopyBackwards>) tensor(0.0644, grad_fn=<DivBackward0>) tensor(3.9631, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(20.8425, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(20.7697, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(20.8425, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5622024 0.0 0.021380391 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(20.8429, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -94.3195 -1.8526577 94.77692 323.27554\n",
      "activ tensor(20.8429, grad_fn=<CopyBackwards>) tensor(0.0645, grad_fn=<DivBackward0>) tensor(5.0512, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(20.8885, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(20.8429, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(20.8885, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5652167 0.0 0.021739816 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(20.8887, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -96.74116 -1.9826014 90.98962 323.8959\n",
      "activ tensor(20.8887, grad_fn=<CopyBackwards>) tensor(0.0645, grad_fn=<DivBackward0>) tensor(3.8227, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(20.9316, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(20.8887, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(20.9316, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56903917 0.0 0.02176096 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(20.9323, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -96.770805 -1.9062269 91.29785 324.4884\n",
      "activ tensor(20.9323, grad_fn=<CopyBackwards>) tensor(0.0645, grad_fn=<DivBackward0>) tensor(3.6490, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(20.9576, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(20.9323, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(20.9576, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55401427 0.0 0.02094697 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(20.9581, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -94.558876 -1.9771842 96.99106 325.067\n",
      "activ tensor(20.9581, grad_fn=<CopyBackwards>) tensor(0.0645, grad_fn=<DivBackward0>) tensor(4.3678, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(20.9828, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(20.9581, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(20.9828, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5789405 0.0 0.022247236 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(20.9821, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -96.03915 -1.9326639 94.882324 325.6319\n",
      "activ tensor(20.9821, grad_fn=<CopyBackwards>) tensor(0.0644, grad_fn=<DivBackward0>) tensor(4.2712, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(21.0061, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(20.9821, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(21.0061, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.572222 0.0 0.021972384 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(21.0062, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -97.75778 -1.7060488 95.19855 326.17236\n",
      "activ tensor(21.0062, grad_fn=<CopyBackwards>) tensor(0.0644, grad_fn=<DivBackward0>) tensor(4.1299, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(21.0125, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(21.0062, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(21.0125, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5628273 0.0 0.0217821 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(21.0118, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -98.235176 -1.8278322 95.07262 326.65088\n",
      "activ tensor(21.0118, grad_fn=<CopyBackwards>) tensor(0.0643, grad_fn=<DivBackward0>) tensor(4.0763, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(21.0173, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(21.0118, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(21.0173, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5776233 0.0 0.022078097 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(21.0178, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -99.53806 -1.8796251 93.63539 327.04645\n",
      "activ tensor(21.0178, grad_fn=<CopyBackwards>) tensor(0.0643, grad_fn=<DivBackward0>) tensor(4.1623, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(21.0517, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(21.0178, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(21.0517, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55770326 0.0 0.02117954 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(21.0525, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -99.312355 -1.9316642 95.54434 327.4041\n",
      "activ tensor(21.0525, grad_fn=<CopyBackwards>) tensor(0.0643, grad_fn=<DivBackward0>) tensor(3.9813, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(21.0370, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(21.0525, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(21.0370, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57068413 0.0 0.022078095 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(21.0372, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -96.43181 -1.7462255 97.51132 327.8686\n",
      "activ tensor(21.0372, grad_fn=<CopyBackwards>) tensor(0.0642, grad_fn=<DivBackward0>) tensor(4.5536, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(21.0734, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(21.0372, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(21.0734, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.54795283 0.0 0.020809544 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(21.0739, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -96.49216 -1.8459643 97.47988 328.35458\n",
      "activ tensor(21.0739, grad_fn=<CopyBackwards>) tensor(0.0642, grad_fn=<DivBackward0>) tensor(4.4328, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(21.1310, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(21.0739, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(21.1310, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5600098 0.0 0.02135925 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(21.1308, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -99.22251 -1.8254092 93.86014 328.89954\n",
      "activ tensor(21.1308, grad_fn=<CopyBackwards>) tensor(0.0642, grad_fn=<DivBackward0>) tensor(4.2468, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(21.1298, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(21.1308, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(21.1298, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5821698 0.0 0.022331806 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(21.1302, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -99.63431 -1.695726 94.70975 329.45285\n",
      "activ tensor(21.1302, grad_fn=<CopyBackwards>) tensor(0.0641, grad_fn=<DivBackward0>) tensor(4.0750, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(21.1627, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(21.1302, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(21.1627, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56811184 0.0 0.021538964 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(21.1641, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -97.0292 -1.8314354 95.39725 330.05423\n",
      "activ tensor(21.1641, grad_fn=<CopyBackwards>) tensor(0.0641, grad_fn=<DivBackward0>) tensor(3.8544, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(21.1872, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(21.1641, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(21.1872, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5701706 0.0 0.021930097 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(21.1873, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -98.34508 -1.8349546 96.06894 330.67804\n",
      "activ tensor(21.1873, grad_fn=<CopyBackwards>) tensor(0.0641, grad_fn=<DivBackward0>) tensor(5.1968, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(21.2231, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(21.1873, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(21.2231, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55822825 0.0 0.02134868 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(21.2244, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -97.185715 -1.7349929 98.19475 331.36432\n",
      "activ tensor(21.2244, grad_fn=<CopyBackwards>) tensor(0.0641, grad_fn=<DivBackward0>) tensor(4.6039, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(21.2490, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(21.2244, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(21.2490, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5783328 0.0 0.022564374 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(21.2490, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -97.301155 -1.8414955 93.45325 332.00903\n",
      "activ tensor(21.2490, grad_fn=<CopyBackwards>) tensor(0.0640, grad_fn=<DivBackward0>) tensor(4.0903, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(21.2768, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(21.2490, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(21.2768, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5552816 0.0 0.02126411 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(21.2769, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -98.023834 -1.9198643 94.00105 332.5849\n",
      "activ tensor(21.2769, grad_fn=<CopyBackwards>) tensor(0.0640, grad_fn=<DivBackward0>) tensor(3.9120, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(21.3224, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(21.2769, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(21.3224, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.59175247 0.0 0.022913227 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(21.3225, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -97.5738 -1.9878459 95.898186 333.18613\n",
      "activ tensor(21.3225, grad_fn=<CopyBackwards>) tensor(0.0640, grad_fn=<DivBackward0>) tensor(3.9656, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(21.3565, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(21.3225, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(21.3565, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5845803 0.0 0.022606658 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(21.3556, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -99.94342 -1.8031626 95.41342 333.8071\n",
      "activ tensor(21.3556, grad_fn=<CopyBackwards>) tensor(0.0640, grad_fn=<DivBackward0>) tensor(4.4503, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(21.3844, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(21.3556, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(21.3844, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5825722 0.0 0.02260666 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(21.3840, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -99.80866 -1.8507081 97.84383 334.39154\n",
      "activ tensor(21.3840, grad_fn=<CopyBackwards>) tensor(0.0639, grad_fn=<DivBackward0>) tensor(4.6787, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(21.4264, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(21.3840, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(21.4264, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55083215 0.0 0.020598117 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(21.4267, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -97.65824 -1.7439847 96.78112 334.93457\n",
      "activ tensor(21.4267, grad_fn=<CopyBackwards>) tensor(0.0640, grad_fn=<DivBackward0>) tensor(4.1707, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(21.4253, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(21.4267, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(21.4253, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5789402 0.0 0.022469234 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(21.4247, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -99.97613 -1.7910666 94.73534 335.38855\n",
      "activ tensor(21.4247, grad_fn=<CopyBackwards>) tensor(0.0639, grad_fn=<DivBackward0>) tensor(4.0377, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(21.4972, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(21.4247, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(21.4972, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5774203 0.0 0.022257809 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(21.4974, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -99.93249 -1.953746 95.03076 335.81467\n",
      "activ tensor(21.4974, grad_fn=<CopyBackwards>) tensor(0.0640, grad_fn=<DivBackward0>) tensor(4.2523, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(21.5404, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(21.4974, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(21.5404, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.54019916 0.0 0.020270409 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(21.5404, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -97.53308 -1.8992805 99.54349 336.23206\n",
      "activ tensor(21.5404, grad_fn=<CopyBackwards>) tensor(0.0641, grad_fn=<DivBackward0>) tensor(4.9724, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(21.5938, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(21.5404, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(21.5938, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56800854 0.0 0.021898385 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(21.5941, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -98.647316 -1.8578291 99.66231 336.73862\n",
      "activ tensor(21.5941, grad_fn=<CopyBackwards>) tensor(0.0641, grad_fn=<DivBackward0>) tensor(4.1864, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(21.5988, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(21.5941, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(21.5988, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.54106617 0.0 0.020460691 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(21.5991, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -101.25146 -1.975158 96.85322 337.23337\n",
      "activ tensor(21.5991, grad_fn=<CopyBackwards>) tensor(0.0640, grad_fn=<DivBackward0>) tensor(4.2360, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(21.6288, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(21.5991, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(21.6288, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5728361 0.0 0.021961812 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(21.6292, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -101.92106 -2.0412424 99.170525 337.7598\n",
      "activ tensor(21.6292, grad_fn=<CopyBackwards>) tensor(0.0640, grad_fn=<DivBackward0>) tensor(4.4802, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(21.6286, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(21.6292, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(21.6286, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55717766 0.0 0.021306396 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(21.6291, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -102.58195 -1.9888861 96.90926 338.32437\n",
      "activ tensor(21.6291, grad_fn=<CopyBackwards>) tensor(0.0639, grad_fn=<DivBackward0>) tensor(4.8247, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(21.6574, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(21.6291, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(21.6574, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56645924 0.0 0.02168696 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(21.6581, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -102.31133 -1.7951778 98.757576 338.91217\n",
      "activ tensor(21.6581, grad_fn=<CopyBackwards>) tensor(0.0639, grad_fn=<DivBackward0>) tensor(4.9622, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(21.6809, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(21.6581, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(21.6809, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55822814 0.0 0.021348678 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(21.6812, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -99.989044 -1.9728125 102.01654 339.49316\n",
      "activ tensor(21.6812, grad_fn=<CopyBackwards>) tensor(0.0639, grad_fn=<DivBackward0>) tensor(4.5146, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(21.6639, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(21.6812, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(21.6639, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57304054 0.0 0.021940669 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(21.6649, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -100.25909 -1.9480278 99.26298 340.08694\n",
      "activ tensor(21.6649, grad_fn=<CopyBackwards>) tensor(0.0637, grad_fn=<DivBackward0>) tensor(4.1192, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(21.7131, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(21.6649, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(21.7131, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5753873 0.0 0.02219438 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(21.7136, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -102.8712 -1.9765007 98.41694 340.60608\n",
      "activ tensor(21.7136, grad_fn=<CopyBackwards>) tensor(0.0637, grad_fn=<DivBackward0>) tensor(4.1077, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(21.7456, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(21.7136, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(21.7456, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5510448 0.0 0.021168968 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(21.7458, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -103.13636 -2.0419958 97.50401 341.1252\n",
      "activ tensor(21.7458, grad_fn=<CopyBackwards>) tensor(0.0637, grad_fn=<DivBackward0>) tensor(4.3594, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(21.7790, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(21.7458, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(21.7790, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56480175 0.0 0.021538962 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(21.7797, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -100.41823 -1.9293559 99.023605 341.68597\n",
      "activ tensor(21.7797, grad_fn=<CopyBackwards>) tensor(0.0637, grad_fn=<DivBackward0>) tensor(5.0257, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(21.8163, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(21.7797, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(21.8163, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5599049 0.0 0.021507248 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(21.8157, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -101.41824 -1.8243103 100.46791 342.24933\n",
      "activ tensor(21.8157, grad_fn=<CopyBackwards>) tensor(0.0637, grad_fn=<DivBackward0>) tensor(4.5671, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(21.8715, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(21.8157, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(21.8715, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5448429 0.0 0.020746116 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(21.8707, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -100.73396 -1.8220083 100.20195 342.78882\n",
      "activ tensor(21.8707, grad_fn=<CopyBackwards>) tensor(0.0638, grad_fn=<DivBackward0>) tensor(3.9379, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(21.8914, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(21.8707, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(21.8914, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5513637 0.0 0.0207884 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(21.8925, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -100.70108 -1.7983137 98.01628 343.32175\n",
      "activ tensor(21.8925, grad_fn=<CopyBackwards>) tensor(0.0638, grad_fn=<DivBackward0>) tensor(4.0878, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(21.9020, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(21.8925, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(21.9020, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5553871 0.0 0.021221824 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(21.9014, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -100.9397 -1.8896856 96.23562 343.83218\n",
      "activ tensor(21.9014, grad_fn=<CopyBackwards>) tensor(0.0637, grad_fn=<DivBackward0>) tensor(5.0225, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(21.9068, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(21.9014, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(21.9068, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5563356 0.0 0.02120068 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(21.9070, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -100.87544 -1.7902374 100.16838 344.35092\n",
      "activ tensor(21.9070, grad_fn=<CopyBackwards>) tensor(0.0636, grad_fn=<DivBackward0>) tensor(5.0360, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(21.9489, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(21.9070, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(21.9489, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56996506 0.0 0.021887813 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(21.9491, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -103.60817 -1.8829019 99.1915 344.8665\n",
      "activ tensor(21.9491, grad_fn=<CopyBackwards>) tensor(0.0636, grad_fn=<DivBackward0>) tensor(4.3719, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(21.9733, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(21.9491, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(21.9733, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.579345 0.0 0.022310665 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(21.9731, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -103.589035 -1.9136057 100.0576 345.43964\n",
      "activ tensor(21.9731, grad_fn=<CopyBackwards>) tensor(0.0636, grad_fn=<DivBackward0>) tensor(4.1948, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(21.9593, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(21.9731, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(21.9593, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.58377796 0.0 0.02241638 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(21.9593, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -100.24045 -1.9927055 101.23717 346.0053\n",
      "activ tensor(21.9593, grad_fn=<CopyBackwards>) tensor(0.0635, grad_fn=<DivBackward0>) tensor(4.1646, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(22.0335, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(21.9593, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(22.0335, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5725292 0.0 0.021898385 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(22.0331, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -103.12511 -2.013678 96.70543 346.63126\n",
      "activ tensor(22.0331, grad_fn=<CopyBackwards>) tensor(0.0636, grad_fn=<DivBackward0>) tensor(4.8720, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(22.0595, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(22.0331, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(22.0595, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5707868 0.0 0.021898385 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(22.0598, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -103.028175 -1.8821418 99.43889 347.23758\n",
      "activ tensor(22.0598, grad_fn=<CopyBackwards>) tensor(0.0635, grad_fn=<DivBackward0>) tensor(5.2508, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(22.0558, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(22.0598, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(22.0558, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5655275 0.0 0.021560105 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(22.0565, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -100.85171 -1.8775154 102.81822 347.84042\n",
      "activ tensor(22.0565, grad_fn=<CopyBackwards>) tensor(0.0634, grad_fn=<DivBackward0>) tensor(4.5012, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(22.0781, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(22.0565, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(22.0781, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5721197 0.0 0.021930099 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(22.0784, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -102.288315 -1.9511185 102.451126 348.41034\n",
      "activ tensor(22.0784, grad_fn=<CopyBackwards>) tensor(0.0634, grad_fn=<DivBackward0>) tensor(4.1256, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(22.1119, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(22.0784, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(22.1119, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5651128 0.0 0.021739816 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(22.1126, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -104.87706 -2.0256538 101.073845 348.9452\n",
      "activ tensor(22.1126, grad_fn=<CopyBackwards>) tensor(0.0634, grad_fn=<DivBackward0>) tensor(4.0600, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(22.1483, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(22.1126, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(22.1483, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5584382 0.0 0.02128525 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(22.1480, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -105.410164 -2.1587737 100.861694 349.45685\n",
      "activ tensor(22.1480, grad_fn=<CopyBackwards>) tensor(0.0634, grad_fn=<DivBackward0>) tensor(4.7916, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(22.1728, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(22.1480, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(22.1728, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55051297 0.0 0.020968115 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(22.1729, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -105.71176 -1.9666858 101.79174 349.95523\n",
      "activ tensor(22.1729, grad_fn=<CopyBackwards>) tensor(0.0634, grad_fn=<DivBackward0>) tensor(4.4005, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(22.2193, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(22.1729, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(22.2193, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5694507 0.0 0.022183808 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(22.2202, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -105.22547 -2.1348732 101.6779 350.44516\n",
      "activ tensor(22.2202, grad_fn=<CopyBackwards>) tensor(0.0634, grad_fn=<DivBackward0>) tensor(4.4735, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(22.3222, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(22.2202, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(22.3222, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56480175 0.0 0.021560103 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(22.3223, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -103.57304 -2.0610337 105.3916 350.97354\n",
      "activ tensor(22.3223, grad_fn=<CopyBackwards>) tensor(0.0636, grad_fn=<DivBackward0>) tensor(4.7393, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(22.3198, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(22.3223, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(22.3198, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5759981 0.0 0.022321235 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(22.3195, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -104.09921 -1.9587905 103.12881 351.50372\n",
      "activ tensor(22.3195, grad_fn=<CopyBackwards>) tensor(0.0635, grad_fn=<DivBackward0>) tensor(4.4486, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(22.3539, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(22.3195, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(22.3539, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56687284 0.0 0.021591818 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(22.3546, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -106.42773 -1.9310594 100.49698 352.02362\n",
      "activ tensor(22.3546, grad_fn=<CopyBackwards>) tensor(0.0635, grad_fn=<DivBackward0>) tensor(5.3358, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(22.3915, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(22.3546, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(22.3915, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5594862 0.0 0.021422677 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(22.3908, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -106.59642 -1.9310929 102.467865 352.51596\n",
      "activ tensor(22.3908, grad_fn=<CopyBackwards>) tensor(0.0635, grad_fn=<DivBackward0>) tensor(4.2270, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(22.3729, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(22.3908, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(22.3729, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55780834 0.0 0.021158397 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(22.3736, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -103.82072 -2.0252898 101.56186 353.03336\n",
      "activ tensor(22.3736, grad_fn=<CopyBackwards>) tensor(0.0634, grad_fn=<DivBackward0>) tensor(4.2625, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(22.4099, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(22.3736, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(22.4099, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55040646 0.0 0.020904686 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(22.4108, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -104.48471 -1.9487433 104.52461 353.52194\n",
      "activ tensor(22.4108, grad_fn=<CopyBackwards>) tensor(0.0634, grad_fn=<DivBackward0>) tensor(4.6362, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(22.4306, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(22.4108, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(22.4306, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5805575 0.0 0.022310665 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(22.4309, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -103.28311 -2.051318 103.78572 354.07697\n",
      "activ tensor(22.4309, grad_fn=<CopyBackwards>) tensor(0.0634, grad_fn=<DivBackward0>) tensor(4.2828, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(22.4822, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(22.4309, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(22.4822, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5669762 0.0 0.02169753 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(22.4825, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -104.53917 -2.025651 100.17534 354.6731\n",
      "activ tensor(22.4825, grad_fn=<CopyBackwards>) tensor(0.0634, grad_fn=<DivBackward0>) tensor(5.6079, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(22.4917, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(22.4825, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(22.4917, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5763031 0.0 0.022088667 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(22.4916, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -106.03363 -1.8316636 101.070885 355.259\n",
      "activ tensor(22.4916, grad_fn=<CopyBackwards>) tensor(0.0633, grad_fn=<DivBackward0>) tensor(4.5933, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(22.5616, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(22.4916, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(22.5616, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57201725 0.0 0.02177153 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(22.5619, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -104.503334 -2.0194602 102.511856 355.8243\n",
      "activ tensor(22.5619, grad_fn=<CopyBackwards>) tensor(0.0634, grad_fn=<DivBackward0>) tensor(4.1739, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(22.5664, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(22.5619, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(22.5664, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5885757 0.0 0.022892084 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(22.5670, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -106.78032 -1.8199818 103.705765 356.41916\n",
      "activ tensor(22.5670, grad_fn=<CopyBackwards>) tensor(0.0633, grad_fn=<DivBackward0>) tensor(4.7377, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(22.6266, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(22.5670, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(22.6266, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5693479 0.0 0.021771532 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(22.6266, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -106.77338 -1.9522883 102.88151 356.96948\n",
      "activ tensor(22.6266, grad_fn=<CopyBackwards>) tensor(0.0634, grad_fn=<DivBackward0>) tensor(4.5574, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(22.5992, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(22.6266, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(22.5992, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57436806 0.0 0.021972384 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(22.5992, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -105.09552 -1.9646276 103.86099 357.51846\n",
      "activ tensor(22.5992, grad_fn=<CopyBackwards>) tensor(0.0632, grad_fn=<DivBackward0>) tensor(4.9331, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(22.6678, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(22.5992, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(22.6678, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56188977 0.0 0.021295823 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(22.6672, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -106.26673 -1.9812407 101.18793 358.0323\n",
      "activ tensor(22.6672, grad_fn=<CopyBackwards>) tensor(0.0633, grad_fn=<DivBackward0>) tensor(4.3091, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(22.7090, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(22.6672, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(22.7090, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56365967 0.0 0.021655245 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(22.7092, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -106.04046 -2.0131857 101.549515 358.53586\n",
      "activ tensor(22.7092, grad_fn=<CopyBackwards>) tensor(0.0633, grad_fn=<DivBackward0>) tensor(4.4416, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(22.7534, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(22.7092, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(22.7534, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56438696 0.0 0.021390963 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(22.7538, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -104.06229 -2.0320022 107.540306 359.05746\n",
      "activ tensor(22.7538, grad_fn=<CopyBackwards>) tensor(0.0634, grad_fn=<DivBackward0>) tensor(4.6944, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(22.7907, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(22.7538, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(22.7907, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56738913 0.0 0.021644674 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(22.7902, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -106.03195 -2.140229 105.008896 359.62387\n",
      "activ tensor(22.7902, grad_fn=<CopyBackwards>) tensor(0.0634, grad_fn=<DivBackward0>) tensor(5.2586, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(22.8466, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(22.7902, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(22.8466, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5682148 0.0 0.02176096 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(22.8461, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -108.36817 -1.9208043 104.11272 360.20108\n",
      "activ tensor(22.8461, grad_fn=<CopyBackwards>) tensor(0.0634, grad_fn=<DivBackward0>) tensor(4.9984, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(22.8827, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(22.8461, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(22.8827, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5726315 0.0 0.021760957 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(22.8828, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -108.99148 -1.9630277 104.80981 360.76416\n",
      "activ tensor(22.8828, grad_fn=<CopyBackwards>) tensor(0.0634, grad_fn=<DivBackward0>) tensor(4.2302, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(22.9102, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(22.8828, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(22.9102, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5791428 0.0 0.022236666 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(22.9098, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -108.65795 -2.186475 104.07944 361.32422\n",
      "activ tensor(22.9098, grad_fn=<CopyBackwards>) tensor(0.0634, grad_fn=<DivBackward0>) tensor(4.8336, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(22.9378, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(22.9098, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(22.9378, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5405245 0.0 0.020502977 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(22.9387, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -108.27314 -2.0883174 106.80978 361.90887\n",
      "activ tensor(22.9387, grad_fn=<CopyBackwards>) tensor(0.0634, grad_fn=<DivBackward0>) tensor(4.4763, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(22.9234, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(22.9387, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(22.9234, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5629314 0.0 0.021496676 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(22.9245, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -107.07289 -2.2256625 107.53052 362.46506\n",
      "activ tensor(22.9245, grad_fn=<CopyBackwards>) tensor(0.0632, grad_fn=<DivBackward0>) tensor(5.5164, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(22.9812, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(22.9245, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(22.9812, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.54623914 0.0 0.020703834 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(22.9815, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -108.00936 -2.1737256 106.78664 363.07227\n",
      "activ tensor(22.9815, grad_fn=<CopyBackwards>) tensor(0.0633, grad_fn=<DivBackward0>) tensor(5.1626, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(23.0624, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(22.9815, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(23.0624, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56063676 0.0 0.021496676 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(23.0628, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -109.95374 -2.1747828 104.08589 363.65546\n",
      "activ tensor(23.0628, grad_fn=<CopyBackwards>) tensor(0.0634, grad_fn=<DivBackward0>) tensor(4.5809, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(23.1156, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(23.0628, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(23.1156, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5857818 0.0 0.022585519 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(23.1159, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -109.93415 -2.0766332 105.153824 364.2423\n",
      "activ tensor(23.1159, grad_fn=<CopyBackwards>) tensor(0.0635, grad_fn=<DivBackward0>) tensor(4.2970, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(23.1378, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(23.1159, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(23.1378, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56573474 0.0 0.02161296 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(23.1388, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -107.28401 -2.0691311 106.548645 364.79944\n",
      "activ tensor(23.1388, grad_fn=<CopyBackwards>) tensor(0.0634, grad_fn=<DivBackward0>) tensor(4.6406, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(23.1669, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(23.1388, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(23.1669, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.554437 0.0 0.020883543 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(23.1680, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -108.68935 -1.971962 106.68826 365.32687\n",
      "activ tensor(23.1680, grad_fn=<CopyBackwards>) tensor(0.0634, grad_fn=<DivBackward0>) tensor(5.0560, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(23.1612, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(23.1680, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(23.1612, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5615768 0.0 0.02152839 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(23.1610, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -105.63261 -1.9436163 108.0069 365.79382\n",
      "activ tensor(23.1610, grad_fn=<CopyBackwards>) tensor(0.0633, grad_fn=<DivBackward0>) tensor(4.7296, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(23.1980, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(23.1610, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(23.1980, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56718284 0.0 0.021708101 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(23.1984, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -108.4208 -1.9763827 103.12014 366.22137\n",
      "activ tensor(23.1984, grad_fn=<CopyBackwards>) tensor(0.0633, grad_fn=<DivBackward0>) tensor(4.4835, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(23.2271, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(23.1984, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(23.2271, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5662523 0.0 0.021570675 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(23.2268, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -109.18089 -2.015129 104.00514 366.6243\n",
      "activ tensor(23.2268, grad_fn=<CopyBackwards>) tensor(0.0634, grad_fn=<DivBackward0>) tensor(4.3784, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(23.2783, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(23.2268, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(23.2783, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56324375 0.0 0.02160239 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(23.2788, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -106.363075 -1.9615173 107.42371 367.04968\n",
      "activ tensor(23.2788, grad_fn=<CopyBackwards>) tensor(0.0634, grad_fn=<DivBackward0>) tensor(4.5615, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(23.3122, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(23.2788, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(23.3122, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56063706 0.0 0.021454388 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(23.3128, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -111.28555 -2.0251796 105.76157 367.4966\n",
      "activ tensor(23.3128, grad_fn=<CopyBackwards>) tensor(0.0634, grad_fn=<DivBackward0>) tensor(5.3043, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(23.3734, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(23.3128, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(23.3734, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56188977 0.0 0.02135925 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(23.3724, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -108.94519 -2.0602577 107.276306 368.0051\n",
      "activ tensor(23.3724, grad_fn=<CopyBackwards>) tensor(0.0635, grad_fn=<DivBackward0>) tensor(4.8276, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(23.4161, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(23.3724, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(23.4161, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5725292 0.0 0.022067526 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(23.4154, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -108.55879 -1.9746726 106.19537 368.54315\n",
      "activ tensor(23.4154, grad_fn=<CopyBackwards>) tensor(0.0635, grad_fn=<DivBackward0>) tensor(4.6346, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(23.4445, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(23.4154, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(23.4445, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5687301 0.0 0.021813814 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(23.4451, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -109.360435 -1.9518769 104.64674 369.1277\n",
      "activ tensor(23.4451, grad_fn=<CopyBackwards>) tensor(0.0635, grad_fn=<DivBackward0>) tensor(4.5054, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(23.4862, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(23.4451, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(23.4862, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57160753 0.0 0.021919526 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(23.4860, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -109.056885 -2.0971236 105.80131 369.7472\n",
      "activ tensor(23.4860, grad_fn=<CopyBackwards>) tensor(0.0635, grad_fn=<DivBackward0>) tensor(4.7523, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(23.5363, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(23.4860, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(23.5363, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55707246 0.0 0.021316966 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(23.5366, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -108.85457 -2.1841137 109.717735 370.40952\n",
      "activ tensor(23.5366, grad_fn=<CopyBackwards>) tensor(0.0635, grad_fn=<DivBackward0>) tensor(5.3495, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(23.5493, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(23.5366, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(23.5493, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5589625 0.0 0.021042112 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(23.5495, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -108.32064 -2.2062392 109.64442 371.06232\n",
      "activ tensor(23.5495, grad_fn=<CopyBackwards>) tensor(0.0635, grad_fn=<DivBackward0>) tensor(4.9323, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(23.5542, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(23.5495, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(23.5542, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5444128 0.0 0.020587547 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(23.5541, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -111.82475 -2.1435935 106.07297 371.74472\n",
      "activ tensor(23.5541, grad_fn=<CopyBackwards>) tensor(0.0634, grad_fn=<DivBackward0>) tensor(4.3321, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(23.5459, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(23.5541, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(23.5459, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.54452026 0.0 0.020756686 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(23.5463, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -112.45161 -2.2725532 108.51683 372.42688\n",
      "activ tensor(23.5463, grad_fn=<CopyBackwards>) tensor(0.0632, grad_fn=<DivBackward0>) tensor(4.4976, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(23.5814, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(23.5463, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(23.5814, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56511307 0.0 0.021538962 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(23.5814, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -111.70582 -2.073308 107.95891 373.10434\n",
      "activ tensor(23.5814, grad_fn=<CopyBackwards>) tensor(0.0632, grad_fn=<DivBackward0>) tensor(4.7084, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(23.6115, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(23.5814, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(23.6115, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5749799 0.0 0.022067526 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(23.6116, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -111.168816 -2.147338 109.48613 373.70657\n",
      "activ tensor(23.6116, grad_fn=<CopyBackwards>) tensor(0.0632, grad_fn=<DivBackward0>) tensor(5.0755, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(23.6637, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(23.6116, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(23.6637, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5710946 0.0 0.021824386 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(23.6642, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -110.7433 -2.1458302 112.19806 374.2836\n",
      "activ tensor(23.6642, grad_fn=<CopyBackwards>) tensor(0.0632, grad_fn=<DivBackward0>) tensor(5.1403, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(23.6665, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(23.6642, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(23.6665, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.556757 0.0 0.02128525 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(23.6665, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -111.802216 -2.11913 108.47337 374.7909\n",
      "activ tensor(23.6665, grad_fn=<CopyBackwards>) tensor(0.0631, grad_fn=<DivBackward0>) tensor(4.3013, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(23.6597, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(23.6665, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(23.6597, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5553871 0.0 0.021063255 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(23.6607, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -113.52095 -2.0870905 108.46687 375.24426\n",
      "activ tensor(23.6607, grad_fn=<CopyBackwards>) tensor(0.0631, grad_fn=<DivBackward0>) tensor(4.5022, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(23.6806, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(23.6607, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(23.6806, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5694508 0.0 0.021961812 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(23.6812, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -113.174774 -2.0514646 108.5177 375.64523\n",
      "activ tensor(23.6812, grad_fn=<CopyBackwards>) tensor(0.0630, grad_fn=<DivBackward0>) tensor(5.2500, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(23.7001, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(23.6812, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(23.7001, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56147254 0.0 0.021486107 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(23.7003, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -110.73404 -2.1815484 109.69462 376.03467\n",
      "activ tensor(23.7003, grad_fn=<CopyBackwards>) tensor(0.0630, grad_fn=<DivBackward0>) tensor(5.4933, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(23.7038, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(23.7003, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(23.7038, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5513637 0.0 0.020872971 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(23.7038, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -111.15325 -2.151703 111.25026 376.436\n",
      "activ tensor(23.7038, grad_fn=<CopyBackwards>) tensor(0.0630, grad_fn=<DivBackward0>) tensor(4.8124, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(23.7188, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(23.7038, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(23.7188, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5501936 0.0 0.021126682 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(23.7188, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -109.70643 -2.1708305 109.884636 376.89456\n",
      "activ tensor(23.7188, grad_fn=<CopyBackwards>) tensor(0.0629, grad_fn=<DivBackward0>) tensor(4.7872, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(23.7503, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(23.7188, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(23.7503, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5581231 0.0 0.021338107 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(23.7506, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -112.26476 -2.104366 107.60763 377.40707\n",
      "activ tensor(23.7506, grad_fn=<CopyBackwards>) tensor(0.0629, grad_fn=<DivBackward0>) tensor(4.4139, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(23.7587, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(23.7506, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(23.7587, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57047886 0.0 0.02161296 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(23.7597, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -110.832306 -1.8999268 106.777794 377.92572\n",
      "activ tensor(23.7597, grad_fn=<CopyBackwards>) tensor(0.0629, grad_fn=<DivBackward0>) tensor(5.2139, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(23.8108, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(23.7597, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(23.8108, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57813036 0.0 0.022416376 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(23.8106, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -109.61354 -2.1700816 111.152115 378.43533\n",
      "activ tensor(23.8106, grad_fn=<CopyBackwards>) tensor(0.0629, grad_fn=<DivBackward0>) tensor(5.6109, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(23.8100, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(23.8106, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(23.8100, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.58065826 0.0 0.022257809 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(23.8103, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -114.3598 -2.1858926 110.01159 379.01108\n",
      "activ tensor(23.8103, grad_fn=<CopyBackwards>) tensor(0.0628, grad_fn=<DivBackward0>) tensor(4.7632, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(23.8427, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(23.8103, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(23.8427, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57088935 0.0 0.021771528 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(23.8427, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -112.5916 -2.2371044 109.04258 379.57797\n",
      "activ tensor(23.8427, grad_fn=<CopyBackwards>) tensor(0.0628, grad_fn=<DivBackward0>) tensor(4.9057, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(23.8924, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(23.8427, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(23.8924, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56386745 0.0 0.021517819 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(23.8927, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -111.13376 -2.130701 110.7771 380.21243\n",
      "activ tensor(23.8927, grad_fn=<CopyBackwards>) tensor(0.0628, grad_fn=<DivBackward0>) tensor(4.4345, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(23.9508, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(23.8927, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(23.9508, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5678021 0.0 0.02161296 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(23.9509, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -112.30427 -2.147953 106.95103 380.8516\n",
      "activ tensor(23.9509, grad_fn=<CopyBackwards>) tensor(0.0629, grad_fn=<DivBackward0>) tensor(5.0132, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(23.9722, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(23.9509, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(23.9722, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5534849 0.0 0.021073824 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(23.9713, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -112.09091 -2.1513817 109.9091 381.45364\n",
      "activ tensor(23.9713, grad_fn=<CopyBackwards>) tensor(0.0628, grad_fn=<DivBackward0>) tensor(4.9671, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(23.9615, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(23.9713, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(23.9615, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55200106 0.0 0.020841256 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(23.9612, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -112.53964 -2.1104784 113.32216 382.04282\n",
      "activ tensor(23.9612, grad_fn=<CopyBackwards>) tensor(0.0627, grad_fn=<DivBackward0>) tensor(4.7280, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(24.0247, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(23.9612, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(24.0247, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5429039 0.0 0.020428978 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(24.0250, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -111.94793 -2.1065602 111.91399 382.64835\n",
      "activ tensor(24.0250, grad_fn=<CopyBackwards>) tensor(0.0628, grad_fn=<DivBackward0>) tensor(4.7819, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(24.0381, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(24.0250, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(24.0381, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56178546 0.0 0.02117954 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(24.0379, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -115.28689 -2.07721 110.515945 383.21088\n",
      "activ tensor(24.0379, grad_fn=<CopyBackwards>) tensor(0.0627, grad_fn=<DivBackward0>) tensor(4.7000, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(24.1102, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(24.0379, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(24.1102, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56313956 0.0 0.021697529 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(24.1106, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -115.82253 -2.0737157 110.41963 383.75198\n",
      "activ tensor(24.1106, grad_fn=<CopyBackwards>) tensor(0.0628, grad_fn=<DivBackward0>) tensor(5.9091, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(24.1050, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(24.1106, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(24.1050, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5712998 0.0 0.022014668 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(24.1052, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -114.72261 -2.1611147 112.50205 384.2455\n",
      "activ tensor(24.1052, grad_fn=<CopyBackwards>) tensor(0.0627, grad_fn=<DivBackward0>) tensor(4.7901, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(24.1104, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(24.1052, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(24.1104, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5662524 0.0 0.021718673 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(24.1103, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -114.157845 -2.2161756 112.86601 384.7361\n",
      "activ tensor(24.1103, grad_fn=<CopyBackwards>) tensor(0.0627, grad_fn=<DivBackward0>) tensor(4.6786, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(24.1272, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(24.1103, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(24.1272, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56583804 0.0 0.021686958 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(24.1272, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -114.28936 -2.2864134 114.90215 385.2658\n",
      "activ tensor(24.1272, grad_fn=<CopyBackwards>) tensor(0.0626, grad_fn=<DivBackward0>) tensor(4.9085, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(24.1433, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(24.1272, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(24.1433, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55019367 0.0 0.0209364 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(24.1434, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -115.75648 -2.225024 112.70762 385.87622\n",
      "activ tensor(24.1434, grad_fn=<CopyBackwards>) tensor(0.0626, grad_fn=<DivBackward0>) tensor(4.5760, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(24.1477, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(24.1434, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(24.1477, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55485934 0.0 0.020978685 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(24.1480, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -116.893074 -2.216814 110.434326 386.4699\n",
      "activ tensor(24.1480, grad_fn=<CopyBackwards>) tensor(0.0625, grad_fn=<DivBackward0>) tensor(5.9703, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(24.1851, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(24.1480, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(24.1851, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5618897 0.0 0.021380393 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(24.1861, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -116.46754 -2.1932814 113.52789 387.04733\n",
      "activ tensor(24.1861, grad_fn=<CopyBackwards>) tensor(0.0625, grad_fn=<DivBackward0>) tensor(5.0639, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(24.2146, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(24.1861, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(24.2146, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5701705 0.0 0.021792673 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(24.2147, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -114.133064 -2.311324 112.51204 387.62753\n",
      "activ tensor(24.2147, grad_fn=<CopyBackwards>) tensor(0.0625, grad_fn=<DivBackward0>) tensor(4.9485, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(24.1832, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(24.2147, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(24.1832, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5676991 0.0 0.021644674 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(24.1846, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -114.27043 -2.2485127 114.68794 388.19354\n",
      "activ tensor(24.1846, grad_fn=<CopyBackwards>) tensor(0.0623, grad_fn=<DivBackward0>) tensor(4.9586, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(24.1884, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(24.1846, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(24.1884, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56893617 0.0 0.02177153 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(24.1889, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -113.11147 -2.2958608 113.86387 388.72617\n",
      "activ tensor(24.1889, grad_fn=<CopyBackwards>) tensor(0.0622, grad_fn=<DivBackward0>) tensor(4.7644, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(24.2306, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(24.1889, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(24.2306, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.569965 0.0 0.021887813 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(24.2310, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -115.99841 -2.196011 109.64533 389.27435\n",
      "activ tensor(24.2310, grad_fn=<CopyBackwards>) tensor(0.0622, grad_fn=<DivBackward0>) tensor(5.5166, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(24.2575, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(24.2310, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(24.2575, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5827734 0.0 0.022405807 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(24.2582, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -114.33377 -2.2772021 111.7124 389.7704\n",
      "activ tensor(24.2582, grad_fn=<CopyBackwards>) tensor(0.0622, grad_fn=<DivBackward0>) tensor(4.6112, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(24.2937, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(24.2582, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(24.2937, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5702731 0.0 0.02177153 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(24.2928, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -114.319756 -2.2792318 113.76274 390.25922\n",
      "activ tensor(24.2928, grad_fn=<CopyBackwards>) tensor(0.0622, grad_fn=<DivBackward0>) tensor(5.2522, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(24.3047, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(24.2928, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(24.3047, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5723244 0.0 0.021930097 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(24.3048, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -117.39001 -2.0675466 113.96835 390.73917\n",
      "activ tensor(24.3048, grad_fn=<CopyBackwards>) tensor(0.0622, grad_fn=<DivBackward0>) tensor(4.5462, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(24.3380, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(24.3048, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(24.3380, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56355554 0.0 0.021517819 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(24.3387, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -115.26686 -2.0051935 112.37172 391.2237\n",
      "activ tensor(24.3387, grad_fn=<CopyBackwards>) tensor(0.0622, grad_fn=<DivBackward0>) tensor(5.5137, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(24.3364, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(24.3387, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(24.3364, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55822814 0.0 0.021158397 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(24.3355, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -115.34601 -2.0921233 112.95137 391.65622\n",
      "activ tensor(24.3355, grad_fn=<CopyBackwards>) tensor(0.0621, grad_fn=<DivBackward0>) tensor(5.5234, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(24.3339, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(24.3355, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(24.3339, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5678021 0.0 0.021898383 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(24.3332, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -116.42148 -2.1630712 111.871826 392.09555\n",
      "activ tensor(24.3332, grad_fn=<CopyBackwards>) tensor(0.0621, grad_fn=<DivBackward0>) tensor(4.5527, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(24.3822, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(24.3332, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(24.3822, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5738581 0.0 0.02220495 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(24.3826, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -114.97228 -2.1803627 112.07668 392.55026\n",
      "activ tensor(24.3826, grad_fn=<CopyBackwards>) tensor(0.0621, grad_fn=<DivBackward0>) tensor(4.9716, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(24.4382, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(24.3826, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(24.4382, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5703761 0.0 0.021845529 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(24.4386, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -115.72137 -2.2047904 117.6548 393.05106\n",
      "activ tensor(24.4386, grad_fn=<CopyBackwards>) tensor(0.0622, grad_fn=<DivBackward0>) tensor(4.9084, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(24.4674, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(24.4386, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(24.4674, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56469804 0.0 0.021644674 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(24.4672, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -115.086136 -2.29365 114.76593 393.62027\n",
      "activ tensor(24.4672, grad_fn=<CopyBackwards>) tensor(0.0622, grad_fn=<DivBackward0>) tensor(5.5532, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(24.5146, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(24.4672, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(24.5146, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5785353 0.0 0.022405805 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(24.5144, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -118.67428 -2.12097 113.16917 394.26053\n",
      "activ tensor(24.5144, grad_fn=<CopyBackwards>) tensor(0.0622, grad_fn=<DivBackward0>) tensor(5.6786, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(24.5107, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(24.5144, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(24.5107, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5853816 0.0 0.022712376 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(24.5108, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -119.17711 -2.3090217 114.81976 394.90384\n",
      "activ tensor(24.5108, grad_fn=<CopyBackwards>) tensor(0.0621, grad_fn=<DivBackward0>) tensor(4.7230, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(24.5181, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(24.5108, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(24.5181, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5492344 0.0 0.021042112 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(24.5185, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -117.693825 -2.2645073 114.702324 395.57068\n",
      "activ tensor(24.5185, grad_fn=<CopyBackwards>) tensor(0.0620, grad_fn=<DivBackward0>) tensor(4.9080, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(24.5557, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(24.5185, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(24.5557, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5709919 0.0 0.02202524 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(24.5563, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -117.180214 -2.2191734 117.71347 396.24942\n",
      "activ tensor(24.5563, grad_fn=<CopyBackwards>) tensor(0.0620, grad_fn=<DivBackward0>) tensor(5.2179, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(24.5442, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(24.5563, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(24.5442, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.578434 0.0 0.02244809 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(24.5456, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -117.87967 -2.3105788 117.36792 396.926\n",
      "activ tensor(24.5456, grad_fn=<CopyBackwards>) tensor(0.0618, grad_fn=<DivBackward0>) tensor(5.4873, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(24.5694, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(24.5456, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(24.5694, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56945086 0.0 0.021750387 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(24.5691, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -119.60204 -2.3592904 115.76839 397.542\n",
      "activ tensor(24.5691, grad_fn=<CopyBackwards>) tensor(0.0618, grad_fn=<DivBackward0>) tensor(5.2763, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(24.5782, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(24.5691, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(24.5782, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5591721 0.0 0.021242965 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(24.5778, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -120.367256 -2.3653343 114.641815 398.15784\n",
      "activ tensor(24.5778, grad_fn=<CopyBackwards>) tensor(0.0617, grad_fn=<DivBackward0>) tensor(4.8850, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(24.6184, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(24.5778, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(24.6184, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5799515 0.0 0.022437518 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(24.6195, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -119.54202 -2.3517606 115.96097 398.7421\n",
      "activ tensor(24.6195, grad_fn=<CopyBackwards>) tensor(0.0617, grad_fn=<DivBackward0>) tensor(4.8197, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(24.6120, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(24.6195, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(24.6120, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.558543 0.0 0.021380395 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(24.6118, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -117.690956 -2.3828943 117.5451 399.29797\n",
      "activ tensor(24.6118, grad_fn=<CopyBackwards>) tensor(0.0616, grad_fn=<DivBackward0>) tensor(5.0287, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(24.6963, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(24.6118, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(24.6963, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5336519 0.0 0.01977356 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(24.6960, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -117.965225 -2.2585618 116.85297 399.83563\n",
      "activ tensor(24.6960, grad_fn=<CopyBackwards>) tensor(0.0618, grad_fn=<DivBackward0>) tensor(5.6176, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(24.7422, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(24.6960, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(24.7422, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57222205 0.0 0.021908956 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(24.7430, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -116.79084 -2.0380943 117.686806 400.35123\n",
      "activ tensor(24.7430, grad_fn=<CopyBackwards>) tensor(0.0618, grad_fn=<DivBackward0>) tensor(5.6117, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(24.7683, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(24.7430, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(24.7683, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.54976755 0.0 0.02105268 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(24.7679, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -119.67778 -2.3173025 113.2017 400.78235\n",
      "activ tensor(24.7679, grad_fn=<CopyBackwards>) tensor(0.0618, grad_fn=<DivBackward0>) tensor(5.1873, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(24.7837, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(24.7679, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(24.7837, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5708894 0.0 0.021982955 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(24.7839, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -118.59462 -2.2420115 114.35855 401.20322\n",
      "activ tensor(24.7839, grad_fn=<CopyBackwards>) tensor(0.0618, grad_fn=<DivBackward0>) tensor(5.0134, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(24.8476, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(24.7839, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(24.8476, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55864817 0.0 0.021422677 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(24.8487, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -117.33856 -2.2015119 118.81413 401.60535\n",
      "activ tensor(24.8487, grad_fn=<CopyBackwards>) tensor(0.0619, grad_fn=<DivBackward0>) tensor(4.8961, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(24.8855, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(24.8487, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(24.8855, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55633575 0.0 0.021200681 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(24.8862, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -121.54895 -2.281141 116.04367 402.03833\n",
      "activ tensor(24.8862, grad_fn=<CopyBackwards>) tensor(0.0619, grad_fn=<DivBackward0>) tensor(5.7995, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(24.8938, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(24.8862, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(24.8938, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5635556 0.0 0.021549534 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(24.8937, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -117.379715 -2.3148117 116.371284 402.50262\n",
      "activ tensor(24.8937, grad_fn=<CopyBackwards>) tensor(0.0618, grad_fn=<DivBackward0>) tensor(5.5318, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(24.9171, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(24.8937, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(24.9171, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55147004 0.0 0.020724973 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(24.9170, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -119.19414 -2.2768524 115.87482 403.03064\n",
      "activ tensor(24.9170, grad_fn=<CopyBackwards>) tensor(0.0618, grad_fn=<DivBackward0>) tensor(5.3355, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(24.9495, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(24.9170, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(24.9495, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55854315 0.0 0.021496676 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(24.9503, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -119.70273 -2.187578 114.914856 403.5604\n",
      "activ tensor(24.9503, grad_fn=<CopyBackwards>) tensor(0.0618, grad_fn=<DivBackward0>) tensor(5.0476, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(24.9637, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(24.9503, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(24.9637, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56376356 0.0 0.021581246 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(24.9644, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -117.99929 -2.2519739 116.81364 404.0745\n",
      "activ tensor(24.9644, grad_fn=<CopyBackwards>) tensor(0.0618, grad_fn=<DivBackward0>) tensor(5.1372, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(25.0062, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(24.9644, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(25.0062, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5687301 0.0 0.021803245 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(25.0054, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -120.24704 -2.2881107 119.49422 404.63452\n",
      "activ tensor(25.0054, grad_fn=<CopyBackwards>) tensor(0.0618, grad_fn=<DivBackward0>) tensor(5.4482, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(25.0497, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(25.0054, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(25.0497, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56105506 0.0 0.02127468 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(25.0501, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -119.12793 -2.2034996 119.19984 405.21933\n",
      "activ tensor(25.0501, grad_fn=<CopyBackwards>) tensor(0.0618, grad_fn=<DivBackward0>) tensor(4.9682, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(25.0839, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(25.0501, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(25.0839, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56780213 0.0 0.021898387 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(25.0837, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -121.943726 -2.4307125 115.53522 405.76746\n",
      "activ tensor(25.0837, grad_fn=<CopyBackwards>) tensor(0.0618, grad_fn=<DivBackward0>) tensor(4.9689, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(25.0980, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(25.0837, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(25.0980, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5818678 0.0 0.022500947 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(25.0975, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -122.46944 -2.3496592 118.20926 406.3628\n",
      "activ tensor(25.0975, grad_fn=<CopyBackwards>) tensor(0.0618, grad_fn=<DivBackward0>) tensor(5.0018, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(25.0871, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(25.0975, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(25.0871, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.54095805 0.0 0.02059812 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(25.0879, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -120.70961 -2.3480449 118.992226 406.96518\n",
      "activ tensor(25.0879, grad_fn=<CopyBackwards>) tensor(0.0616, grad_fn=<DivBackward0>) tensor(5.4585, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(25.1705, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(25.0879, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(25.1705, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55125767 0.0 0.020968113 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(25.1697, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -120.1809 -2.3312669 120.01549 407.577\n",
      "activ tensor(25.1697, grad_fn=<CopyBackwards>) tensor(0.0618, grad_fn=<DivBackward0>) tensor(6.2376, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(25.1932, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(25.1697, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(25.1932, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56188977 0.0 0.021496676 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(25.1924, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -121.503624 -2.2787342 121.9435 408.21112\n",
      "activ tensor(25.1924, grad_fn=<CopyBackwards>) tensor(0.0617, grad_fn=<DivBackward0>) tensor(4.6592, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(25.1982, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(25.1924, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(25.1982, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.565838 0.0 0.021813814 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(25.1986, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -123.431435 -2.1910424 117.718414 408.8862\n",
      "activ tensor(25.1986, grad_fn=<CopyBackwards>) tensor(0.0616, grad_fn=<DivBackward0>) tensor(5.0415, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(25.2523, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(25.1986, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(25.2523, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5860817 0.0 0.022596087 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(25.2522, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -123.739716 -2.2835782 118.62143 409.47458\n",
      "activ tensor(25.2522, grad_fn=<CopyBackwards>) tensor(0.0617, grad_fn=<DivBackward0>) tensor(5.0789, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(25.2981, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(25.2522, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(25.2981, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5700679 0.0 0.02191953 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(25.2990, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -122.68391 -2.467222 119.98232 410.05933\n",
      "activ tensor(25.2990, grad_fn=<CopyBackwards>) tensor(0.0617, grad_fn=<DivBackward0>) tensor(5.4632, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(25.3082, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(25.2990, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(25.3082, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57650626 0.0 0.021993527 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(25.3092, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -121.11163 -2.3981926 120.12785 410.6502\n",
      "activ tensor(25.3092, grad_fn=<CopyBackwards>) tensor(0.0616, grad_fn=<DivBackward0>) tensor(6.0369, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(25.3321, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(25.3092, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(25.3321, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5714023 0.0 0.021845527 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(25.3326, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -120.034256 -2.3077426 121.5352 411.23114\n",
      "activ tensor(25.3326, grad_fn=<CopyBackwards>) tensor(0.0616, grad_fn=<DivBackward0>) tensor(4.7952, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(25.3680, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(25.3326, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(25.3680, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56021905 0.0 0.021221824 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(25.3679, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -120.42698 -2.4802747 119.54765 411.81476\n",
      "activ tensor(25.3679, grad_fn=<CopyBackwards>) tensor(0.0616, grad_fn=<DivBackward0>) tensor(5.1630, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(25.4274, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(25.3679, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(25.4274, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5753876 0.0 0.022099238 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(25.4280, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -123.37233 -2.3704586 117.60243 412.39365\n",
      "activ tensor(25.4280, grad_fn=<CopyBackwards>) tensor(0.0617, grad_fn=<DivBackward0>) tensor(5.4698, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(25.4768, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(25.4280, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(25.4768, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56573474 0.0 0.021412106 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(25.4777, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -121.67621 -2.3477185 117.64203 412.90073\n",
      "activ tensor(25.4777, grad_fn=<CopyBackwards>) tensor(0.0617, grad_fn=<DivBackward0>) tensor(5.2771, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(25.5043, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(25.4777, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(25.5043, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56053245 0.0 0.021464964 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(25.5052, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -121.51958 -2.3149867 122.02587 413.36423\n",
      "activ tensor(25.5052, grad_fn=<CopyBackwards>) tensor(0.0617, grad_fn=<DivBackward0>) tensor(5.4827, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(25.5385, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(25.5052, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(25.5385, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56365955 0.0 0.021338109 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(25.5384, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -123.95436 -2.2887528 120.54108 413.82947\n",
      "activ tensor(25.5384, grad_fn=<CopyBackwards>) tensor(0.0617, grad_fn=<DivBackward0>) tensor(4.8235, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(25.5444, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(25.5384, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(25.5444, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56386745 0.0 0.02144382 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(25.5444, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -121.136314 -2.239357 118.03467 414.26025\n",
      "activ tensor(25.5444, grad_fn=<CopyBackwards>) tensor(0.0617, grad_fn=<DivBackward0>) tensor(5.0459, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(25.5658, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(25.5444, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(25.5658, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5671827 0.0 0.02168696 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(25.5657, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -122.917145 -2.3345447 120.344086 414.623\n",
      "activ tensor(25.5657, grad_fn=<CopyBackwards>) tensor(0.0617, grad_fn=<DivBackward0>) tensor(5.0360, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(25.5658, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(25.5657, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(25.5658, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56749266 0.0 0.02169753 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(25.5658, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -121.49377 -2.4341183 117.77553 415.02704\n",
      "activ tensor(25.5658, grad_fn=<CopyBackwards>) tensor(0.0616, grad_fn=<DivBackward0>) tensor(6.1845, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(25.5701, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(25.5658, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(25.5701, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56365955 0.0 0.021644674 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(25.5703, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -122.00527 -2.4022331 120.396805 415.4457\n",
      "activ tensor(25.5703, grad_fn=<CopyBackwards>) tensor(0.0615, grad_fn=<DivBackward0>) tensor(5.6283, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(25.5880, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(25.5703, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(25.5880, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5802544 0.0 0.022183808 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(25.5872, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -123.35755 -2.2667687 123.493416 415.94168\n",
      "activ tensor(25.5872, grad_fn=<CopyBackwards>) tensor(0.0615, grad_fn=<DivBackward0>) tensor(5.2039, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(25.6364, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(25.5872, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(25.6364, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5723245 0.0 0.021834956 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(25.6359, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -122.41444 -2.289225 120.995674 416.49\n",
      "activ tensor(25.6359, grad_fn=<CopyBackwards>) tensor(0.0616, grad_fn=<DivBackward0>) tensor(5.2870, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(25.7010, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(25.6359, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(25.7010, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57701445 0.0 0.022046382 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(25.7016, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -125.302895 -2.1985397 120.18451 417.0695\n",
      "activ tensor(25.7016, grad_fn=<CopyBackwards>) tensor(0.0616, grad_fn=<DivBackward0>) tensor(4.8257, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(25.6720, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(25.7016, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(25.6720, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56500924 0.0 0.021644676 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(25.6716, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -125.58944 -2.4270253 120.417694 417.6395\n",
      "activ tensor(25.6716, grad_fn=<CopyBackwards>) tensor(0.0615, grad_fn=<DivBackward0>) tensor(6.2235, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(25.7242, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(25.6716, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(25.7242, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5615767 0.0 0.021486105 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(25.7242, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -123.78609 -2.4976573 123.12166 418.27368\n",
      "activ tensor(25.7242, grad_fn=<CopyBackwards>) tensor(0.0615, grad_fn=<DivBackward0>) tensor(5.5137, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(25.7823, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(25.7242, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(25.7823, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5649056 0.0 0.021750385 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(25.7830, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -123.23066 -2.4089718 123.72189 418.94315\n",
      "activ tensor(25.7830, grad_fn=<CopyBackwards>) tensor(0.0615, grad_fn=<DivBackward0>) tensor(5.2851, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(25.8268, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(25.7830, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(25.8268, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56903917 0.0 0.0219301 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(25.8272, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -125.07243 -2.3987756 124.18486 419.63687\n",
      "activ tensor(25.8272, grad_fn=<CopyBackwards>) tensor(0.0615, grad_fn=<DivBackward0>) tensor(4.8169, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(25.8634, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(25.8272, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(25.8634, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.574572 0.0 0.022141524 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(25.8636, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -127.27933 -2.4277434 122.14594 420.3324\n",
      "activ tensor(25.8636, grad_fn=<CopyBackwards>) tensor(0.0615, grad_fn=<DivBackward0>) tensor(5.2545, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(25.9044, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(25.8636, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(25.9044, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5494478 0.0 0.020999826 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(25.9044, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -127.048225 -2.5167525 120.76544 421.0086\n",
      "activ tensor(25.9044, grad_fn=<CopyBackwards>) tensor(0.0615, grad_fn=<DivBackward0>) tensor(5.9687, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(25.9407, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(25.9044, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(25.9407, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.58277357 0.0 0.022458663 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(25.9406, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -125.764366 -2.4277177 124.70273 421.64862\n",
      "activ tensor(25.9406, grad_fn=<CopyBackwards>) tensor(0.0615, grad_fn=<DivBackward0>) tensor(5.1351, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(25.9777, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(25.9406, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(25.9777, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5591719 0.0 0.021190109 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(25.9781, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -124.62845 -2.2904792 123.39264 422.2667\n",
      "activ tensor(25.9781, grad_fn=<CopyBackwards>) tensor(0.0615, grad_fn=<DivBackward0>) tensor(5.1336, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(25.9986, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(25.9781, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(25.9986, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.598741 0.0 0.023124654 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(25.9988, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -123.13614 -2.4124117 124.25535 422.83902\n",
      "activ tensor(25.9988, grad_fn=<CopyBackwards>) tensor(0.0615, grad_fn=<DivBackward0>) tensor(4.6660, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(26.0545, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(25.9988, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(26.0545, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5808601 0.0 0.022511518 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(26.0556, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -124.21734 -2.3320606 123.97179 423.3507\n",
      "activ tensor(26.0556, grad_fn=<CopyBackwards>) tensor(0.0615, grad_fn=<DivBackward0>) tensor(5.8585, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(26.0678, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(26.0556, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(26.0678, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56532025 0.0 0.02159182 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(26.0677, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -126.84188 -2.4898157 119.61153 423.7865\n",
      "activ tensor(26.0677, grad_fn=<CopyBackwards>) tensor(0.0615, grad_fn=<DivBackward0>) tensor(6.0299, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(26.1267, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(26.0677, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(26.1267, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.558438 0.0 0.021211252 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(26.1267, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -124.28742 -2.4202933 122.574585 424.2337\n",
      "activ tensor(26.1267, grad_fn=<CopyBackwards>) tensor(0.0616, grad_fn=<DivBackward0>) tensor(5.3224, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(26.1526, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(26.1267, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(26.1526, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5650093 0.0 0.02160239 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(26.1526, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -125.74446 -2.4430244 124.86854 424.68533\n",
      "activ tensor(26.1526, grad_fn=<CopyBackwards>) tensor(0.0616, grad_fn=<DivBackward0>) tensor(5.4096, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(26.2186, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(26.1526, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(26.2186, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.54955447 0.0 0.020830687 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(26.2186, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -126.98782 -2.3559055 123.96394 425.16605\n",
      "activ tensor(26.2186, grad_fn=<CopyBackwards>) tensor(0.0617, grad_fn=<DivBackward0>) tensor(5.5833, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(26.2542, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(26.2186, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(26.2542, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57721734 0.0 0.022130953 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(26.2546, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -124.90657 -2.2991593 121.80722 425.63074\n",
      "activ tensor(26.2546, grad_fn=<CopyBackwards>) tensor(0.0617, grad_fn=<DivBackward0>) tensor(5.7824, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(26.3161, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(26.2546, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(26.3161, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56303585 0.0 0.021591818 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(26.3165, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -126.264725 -2.4376454 122.34742 426.1194\n",
      "activ tensor(26.3165, grad_fn=<CopyBackwards>) tensor(0.0618, grad_fn=<DivBackward0>) tensor(6.0631, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(26.3329, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(26.3165, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(26.3329, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5877785 0.0 0.022733515 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(26.3328, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -125.14714 -2.3807895 122.80821 426.5964\n",
      "activ tensor(26.3328, grad_fn=<CopyBackwards>) tensor(0.0617, grad_fn=<DivBackward0>) tensor(5.4298, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(26.3531, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(26.3328, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(26.3531, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55485946 0.0 0.02095754 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(26.3536, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -126.8385 -2.4353223 122.852615 427.11383\n",
      "activ tensor(26.3536, grad_fn=<CopyBackwards>) tensor(0.0617, grad_fn=<DivBackward0>) tensor(5.5209, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(26.3473, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(26.3536, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(26.3473, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55570346 0.0 0.021063253 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(26.3478, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -126.41932 -2.522377 127.20613 427.64316\n",
      "activ tensor(26.3478, grad_fn=<CopyBackwards>) tensor(0.0616, grad_fn=<DivBackward0>) tensor(5.3513, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(26.3655, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(26.3478, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(26.3655, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56842124 0.0 0.021708101 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(26.3659, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -124.75429 -2.4630532 124.32349 428.18555\n",
      "activ tensor(26.3659, grad_fn=<CopyBackwards>) tensor(0.0616, grad_fn=<DivBackward0>) tensor(5.8114, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(26.4218, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(26.3659, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(26.4218, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.58176726 0.0 0.022384664 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(26.4216, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -128.45465 -2.5469122 122.456314 428.74338\n",
      "activ tensor(26.4216, grad_fn=<CopyBackwards>) tensor(0.0616, grad_fn=<DivBackward0>) tensor(5.5613, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(26.4253, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(26.4216, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(26.4253, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5717098 0.0 0.021813815 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(26.4247, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -128.79918 -2.4751344 125.243515 429.31943\n",
      "activ tensor(26.4247, grad_fn=<CopyBackwards>) tensor(0.0616, grad_fn=<DivBackward0>) tensor(5.4584, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(26.4419, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(26.4247, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(26.4419, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.561368 0.0 0.021570677 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(26.4418, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -126.75094 -2.468746 125.25546 429.91235\n",
      "activ tensor(26.4418, grad_fn=<CopyBackwards>) tensor(0.0615, grad_fn=<DivBackward0>) tensor(5.0527, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(26.4859, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(26.4418, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(26.4859, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5720173 0.0 0.021982953 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(26.4863, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -126.411606 -2.250534 128.17258 430.49054\n",
      "activ tensor(26.4863, grad_fn=<CopyBackwards>) tensor(0.0615, grad_fn=<DivBackward0>) tensor(5.1986, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(26.5227, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(26.4863, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(26.5227, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55136365 0.0 0.020830685 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(26.5221, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -128.59923 -2.4652143 127.01123 431.04636\n",
      "activ tensor(26.5221, grad_fn=<CopyBackwards>) tensor(0.0615, grad_fn=<DivBackward0>) tensor(6.1071, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(26.5806, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(26.5221, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(26.5806, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5800526 0.0 0.022300094 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(26.5810, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -131.05884 -2.478949 124.79254 431.6033\n",
      "activ tensor(26.5810, grad_fn=<CopyBackwards>) tensor(0.0616, grad_fn=<DivBackward0>) tensor(6.0071, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(26.6073, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(26.5810, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(26.6073, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.560846 0.0 0.021401536 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(26.6073, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -130.37273 -2.3417416 125.38134 432.12012\n",
      "activ tensor(26.6073, grad_fn=<CopyBackwards>) tensor(0.0616, grad_fn=<DivBackward0>) tensor(5.2091, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(26.6160, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(26.6073, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(26.6160, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57037616 0.0 0.02186667 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(26.6161, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -128.77069 -2.4539857 127.08197 432.69315\n",
      "activ tensor(26.6161, grad_fn=<CopyBackwards>) tensor(0.0615, grad_fn=<DivBackward0>) tensor(5.2411, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(26.6357, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(26.6161, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(26.6357, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5578083 0.0 0.021211252 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(26.6363, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -128.1725 -2.50402 128.1494 433.2805\n",
      "activ tensor(26.6363, grad_fn=<CopyBackwards>) tensor(0.0615, grad_fn=<DivBackward0>) tensor(5.4150, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(26.6450, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(26.6363, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(26.6450, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5698622 0.0 0.021782098 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(26.6448, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -126.97154 -2.4952745 126.70028 433.90738\n",
      "activ tensor(26.6448, grad_fn=<CopyBackwards>) tensor(0.0614, grad_fn=<DivBackward0>) tensor(6.0509, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(26.6869, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(26.6448, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(26.6869, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56842107 0.0 0.021718673 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(26.6872, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -127.9007 -2.441324 127.259384 434.537\n",
      "activ tensor(26.6872, grad_fn=<CopyBackwards>) tensor(0.0614, grad_fn=<DivBackward0>) tensor(5.9995, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(26.7044, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(26.6872, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(26.7044, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5824717 0.0 0.022469232 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(26.7052, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -130.38182 -2.4586377 123.8777 435.17374\n",
      "activ tensor(26.7052, grad_fn=<CopyBackwards>) tensor(0.0614, grad_fn=<DivBackward0>) tensor(5.0672, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(26.7212, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(26.7052, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(26.7212, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5592767 0.0 0.021232395 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(26.7211, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -127.61985 -2.5348933 124.917175 435.79117\n",
      "activ tensor(26.7211, grad_fn=<CopyBackwards>) tensor(0.0613, grad_fn=<DivBackward0>) tensor(5.3579, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(26.7913, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(26.7211, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(26.7913, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56759566 0.0 0.02187724 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(26.7923, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -128.54854 -2.4901502 129.91997 436.40277\n",
      "activ tensor(26.7923, grad_fn=<CopyBackwards>) tensor(0.0614, grad_fn=<DivBackward0>) tensor(5.4690, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(26.8132, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(26.7923, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(26.8132, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.54376656 0.0 0.020872971 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(26.8141, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -130.60408 -2.6798193 126.121506 436.98514\n",
      "activ tensor(26.8141, grad_fn=<CopyBackwards>) tensor(0.0614, grad_fn=<DivBackward0>) tensor(5.8198, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(26.8429, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(26.8141, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(26.8429, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.53922224 0.0 0.020481834 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(26.8435, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -127.82473 -2.539987 125.47136 437.5293\n",
      "activ tensor(26.8435, grad_fn=<CopyBackwards>) tensor(0.0614, grad_fn=<DivBackward0>) tensor(5.5185, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(26.8857, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(26.8435, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(26.8857, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5582282 0.0 0.021327537 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(26.8856, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -129.54999 -2.5109994 125.833664 438.03558\n",
      "activ tensor(26.8856, grad_fn=<CopyBackwards>) tensor(0.0614, grad_fn=<DivBackward0>) tensor(5.1868, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(26.9098, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(26.8856, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(26.9098, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5668726 0.0 0.021940669 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(26.9094, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -129.56577 -2.4796772 125.55442 438.52206\n",
      "activ tensor(26.9094, grad_fn=<CopyBackwards>) tensor(0.0614, grad_fn=<DivBackward0>) tensor(5.8673, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(26.9050, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(26.9094, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(26.9050, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.54677516 0.0 0.020915257 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(26.9048, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -129.95485 -2.4815629 127.76681 438.95193\n",
      "activ tensor(26.9048, grad_fn=<CopyBackwards>) tensor(0.0613, grad_fn=<DivBackward0>) tensor(5.8203, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(26.8913, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(26.9048, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(26.8913, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57863665 0.0 0.022173237 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(26.8913, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -130.63153 -2.3814123 129.01071 439.37274\n",
      "activ tensor(26.8913, grad_fn=<CopyBackwards>) tensor(0.0612, grad_fn=<DivBackward0>) tensor(6.5759, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(26.9072, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(26.8913, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(26.9072, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5806583 0.0 0.022479804 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(26.9065, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -128.54672 -2.3848643 128.33046 439.7733\n",
      "activ tensor(26.9065, grad_fn=<CopyBackwards>) tensor(0.0612, grad_fn=<DivBackward0>) tensor(5.7502, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(26.9482, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(26.9065, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(26.9482, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5599051 0.0 0.021327537 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(26.9483, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -131.6325 -2.4656758 125.472275 440.14267\n",
      "activ tensor(26.9483, grad_fn=<CopyBackwards>) tensor(0.0612, grad_fn=<DivBackward0>) tensor(5.6328, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(26.9541, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(26.9483, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(26.9541, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5610547 0.0 0.021369822 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(26.9541, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -131.81265 -2.4202523 128.20703 440.50772\n",
      "activ tensor(26.9541, grad_fn=<CopyBackwards>) tensor(0.0612, grad_fn=<DivBackward0>) tensor(5.4519, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(26.9887, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(26.9541, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(26.9887, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57293826 0.0 0.02210981 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(26.9889, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -129.90076 -2.4789722 129.91504 440.95786\n",
      "activ tensor(26.9889, grad_fn=<CopyBackwards>) tensor(0.0612, grad_fn=<DivBackward0>) tensor(5.5377, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(26.9894, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(26.9889, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(26.9894, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57027346 0.0 0.02186667 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(26.9898, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -129.73004 -2.6441965 130.11287 441.46503\n",
      "activ tensor(26.9898, grad_fn=<CopyBackwards>) tensor(0.0611, grad_fn=<DivBackward0>) tensor(6.6141, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(27.0614, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(26.9898, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(27.0614, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5731428 0.0 0.02219438 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(27.0610, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -132.22397 -2.5648947 131.4647 442.07025\n",
      "activ tensor(27.0610, grad_fn=<CopyBackwards>) tensor(0.0612, grad_fn=<DivBackward0>) tensor(5.5112, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(27.0719, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(27.0610, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(27.0719, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.575591 0.0 0.02244809 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(27.0719, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -134.71768 -2.4876888 127.12383 442.73526\n",
      "activ tensor(27.0719, grad_fn=<CopyBackwards>) tensor(0.0611, grad_fn=<DivBackward0>) tensor(5.3761, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(27.0941, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(27.0719, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(27.0941, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56862694 0.0 0.021676388 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(27.0948, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -133.67378 -2.478897 128.98227 443.43347\n",
      "activ tensor(27.0948, grad_fn=<CopyBackwards>) tensor(0.0611, grad_fn=<DivBackward0>) tensor(5.1862, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(27.1330, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(27.0948, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(27.1330, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5665626 0.0 0.021665817 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(27.1331, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -131.79707 -2.6482172 131.54488 444.13092\n",
      "activ tensor(27.1331, grad_fn=<CopyBackwards>) tensor(0.0611, grad_fn=<DivBackward0>) tensor(5.7359, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(27.1691, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(27.1331, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(27.1691, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55623055 0.0 0.021126682 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(27.1694, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -131.6777 -2.56214 130.37146 444.88275\n",
      "activ tensor(27.1694, grad_fn=<CopyBackwards>) tensor(0.0611, grad_fn=<DivBackward0>) tensor(5.9553, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(27.1965, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(27.1694, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(27.1965, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56438667 0.0 0.021792674 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(27.1970, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -130.9477 -2.634852 131.25546 445.60007\n",
      "activ tensor(27.1970, grad_fn=<CopyBackwards>) tensor(0.0610, grad_fn=<DivBackward0>) tensor(5.3907, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(27.2374, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(27.1970, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(27.2374, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.565113 0.0 0.021665815 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(27.2377, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -131.57578 -2.6463263 129.42618 446.28296\n",
      "activ tensor(27.2377, grad_fn=<CopyBackwards>) tensor(0.0610, grad_fn=<DivBackward0>) tensor(5.5190, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(27.2872, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(27.2377, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(27.2872, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.581465 0.0 0.022543233 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(27.2878, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -133.78986 -2.588461 127.96809 446.94284\n",
      "activ tensor(27.2878, grad_fn=<CopyBackwards>) tensor(0.0611, grad_fn=<DivBackward0>) tensor(5.3788, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(27.3393, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(27.2878, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(27.3393, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5629315 0.0 0.021264108 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(27.3396, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -131.06105 -2.5367064 128.819 447.54184\n",
      "activ tensor(27.3396, grad_fn=<CopyBackwards>) tensor(0.0611, grad_fn=<DivBackward0>) tensor(6.5286, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(27.3986, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(27.3396, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(27.3986, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56583816 0.0 0.02169753 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(27.3983, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -132.68597 -2.551958 132.5101 448.0903\n",
      "activ tensor(27.3983, grad_fn=<CopyBackwards>) tensor(0.0611, grad_fn=<DivBackward0>) tensor(6.2292, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(27.4034, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(27.3983, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(27.4034, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5667696 0.0 0.021760957 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(27.4038, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -132.58803 -2.3819935 130.82726 448.59305\n",
      "activ tensor(27.4038, grad_fn=<CopyBackwards>) tensor(0.0611, grad_fn=<DivBackward0>) tensor(5.3010, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(27.4220, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(27.4038, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(27.4220, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5653205 0.0 0.02161296 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(27.4213, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -131.52632 -2.409901 127.18624 449.02045\n",
      "activ tensor(27.4213, grad_fn=<CopyBackwards>) tensor(0.0611, grad_fn=<DivBackward0>) tensor(5.6970, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(27.4404, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(27.4213, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(27.4404, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55686206 0.0 0.02134868 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(27.4409, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -132.9081 -2.5655909 130.16998 449.40887\n",
      "activ tensor(27.4409, grad_fn=<CopyBackwards>) tensor(0.0611, grad_fn=<DivBackward0>) tensor(5.7694, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(27.4541, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(27.4409, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(27.4541, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55284953 0.0 0.021042112 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(27.4547, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -132.80032 -2.5205672 128.90419 449.75845\n",
      "activ tensor(27.4547, grad_fn=<CopyBackwards>) tensor(0.0610, grad_fn=<DivBackward0>) tensor(6.6263, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(27.5101, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(27.4547, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(27.5101, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5503 0.0 0.020999826 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(27.5106, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -134.19728 -2.4838583 130.87286 450.1563\n",
      "activ tensor(27.5106, grad_fn=<CopyBackwards>) tensor(0.0611, grad_fn=<DivBackward0>) tensor(5.8757, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(27.5256, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(27.5106, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(27.5256, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5779276 0.0 0.022300093 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(27.5259, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -133.10701 -2.6325302 133.2276 450.57553\n",
      "activ tensor(27.5259, grad_fn=<CopyBackwards>) tensor(0.0611, grad_fn=<DivBackward0>) tensor(5.4347, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(27.5728, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(27.5259, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(27.5728, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5580182 0.0 0.021433251 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(27.5722, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -133.10123 -2.633201 129.97787 451.05783\n",
      "activ tensor(27.5722, grad_fn=<CopyBackwards>) tensor(0.0611, grad_fn=<DivBackward0>) tensor(5.2890, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(27.6112, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(27.5722, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(27.6112, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5459175 0.0 0.020661546 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(27.6117, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -134.77135 -2.5625303 130.06628 451.5685\n",
      "activ tensor(27.6117, grad_fn=<CopyBackwards>) tensor(0.0611, grad_fn=<DivBackward0>) tensor(5.7833, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(27.6543, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(27.6117, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(27.6543, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5758963 0.0 0.02218381 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(27.6545, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -134.80617 -2.5569258 130.96652 452.12198\n",
      "activ tensor(27.6545, grad_fn=<CopyBackwards>) tensor(0.0612, grad_fn=<DivBackward0>) tensor(6.2267, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(27.6727, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(27.6545, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(27.6727, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57731885 0.0 0.02218381 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(27.6722, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -132.98405 -2.707558 133.40512 452.6943\n",
      "activ tensor(27.6722, grad_fn=<CopyBackwards>) tensor(0.0611, grad_fn=<DivBackward0>) tensor(5.6137, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(27.6722, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(27.6722, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(27.6722, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.569142 0.0 0.022025239 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(27.6727, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -132.76985 -2.5230763 134.1885 453.28958\n",
      "activ tensor(27.6727, grad_fn=<CopyBackwards>) tensor(0.0610, grad_fn=<DivBackward0>) tensor(6.0698, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(27.7390, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(27.6727, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(27.7390, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.568627 0.0 0.021676388 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(27.7391, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -135.6622 -2.71417 133.308 453.8732\n",
      "activ tensor(27.7391, grad_fn=<CopyBackwards>) tensor(0.0611, grad_fn=<DivBackward0>) tensor(5.2485, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(27.7866, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(27.7391, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(27.7866, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5718122 0.0 0.021972384 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(27.7868, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -138.45728 -2.730063 131.74768 454.46353\n",
      "activ tensor(27.7868, grad_fn=<CopyBackwards>) tensor(0.0611, grad_fn=<DivBackward0>) tensor(5.8580, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(27.7762, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(27.7868, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(27.7762, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56614906 0.0 0.021486105 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(27.7766, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -136.84406 -2.6431735 131.35226 455.0714\n",
      "activ tensor(27.7766, grad_fn=<CopyBackwards>) tensor(0.0610, grad_fn=<DivBackward0>) tensor(6.6264, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(27.8147, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(27.7766, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(27.8147, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55770314 0.0 0.021464963 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(27.8149, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -134.86575 -2.6166089 135.87375 455.7012\n",
      "activ tensor(27.8149, grad_fn=<CopyBackwards>) tensor(0.0610, grad_fn=<DivBackward0>) tensor(6.4256, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(27.8390, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(27.8149, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(27.8390, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55990505 0.0 0.02117954 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(27.8386, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -135.20448 -2.512038 133.99362 456.312\n",
      "activ tensor(27.8386, grad_fn=<CopyBackwards>) tensor(0.0610, grad_fn=<DivBackward0>) tensor(5.8667, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(27.8815, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(27.8386, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(27.8815, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55927694 0.0 0.021253537 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(27.8819, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -134.90918 -2.495192 133.47577 456.9043\n",
      "activ tensor(27.8819, grad_fn=<CopyBackwards>) tensor(0.0610, grad_fn=<DivBackward0>) tensor(5.4459, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(27.9270, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(27.8819, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(27.9270, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5688333 0.0 0.021982953 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(27.9266, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -135.28635 -2.4876075 134.07994 457.47803\n",
      "activ tensor(27.9266, grad_fn=<CopyBackwards>) tensor(0.0610, grad_fn=<DivBackward0>) tensor(5.9869, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(27.9400, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(27.9266, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(27.9400, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57630324 0.0 0.022173237 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(27.9402, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -137.10307 -2.6192322 130.23428 458.0438\n",
      "activ tensor(27.9402, grad_fn=<CopyBackwards>) tensor(0.0610, grad_fn=<DivBackward0>) tensor(6.2492, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(27.9611, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(27.9402, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(27.9611, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5623068 0.0 0.02150725 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(27.9615, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -134.4761 -2.6302013 133.4357 458.59348\n",
      "activ tensor(27.9615, grad_fn=<CopyBackwards>) tensor(0.0610, grad_fn=<DivBackward0>) tensor(5.8367, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(27.9839, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(27.9615, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(27.9839, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56645894 0.0 0.021496676 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(27.9846, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -136.36919 -2.5556307 135.7393 459.18915\n",
      "activ tensor(27.9846, grad_fn=<CopyBackwards>) tensor(0.0609, grad_fn=<DivBackward0>) tensor(5.9149, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(28.0158, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(27.9846, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(28.0158, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5769127 0.0 0.02228952 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(28.0160, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -135.59438 -2.771521 133.6124 459.77536\n",
      "activ tensor(28.0160, grad_fn=<CopyBackwards>) tensor(0.0609, grad_fn=<DivBackward0>) tensor(5.2118, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(28.0281, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(28.0160, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(28.0281, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5652166 0.0 0.021560105 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(28.0279, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -135.51343 -2.6070895 131.49078 460.37204\n",
      "activ tensor(28.0279, grad_fn=<CopyBackwards>) tensor(0.0609, grad_fn=<DivBackward0>) tensor(6.2466, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(28.0905, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(28.0279, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(28.0905, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55791336 0.0 0.021316964 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(28.0908, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -136.83604 -2.6276627 132.09068 460.94916\n",
      "activ tensor(28.0908, grad_fn=<CopyBackwards>) tensor(0.0609, grad_fn=<DivBackward0>) tensor(6.1266, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(28.1589, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(28.0908, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(28.1589, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57447016 0.0 0.0218561 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(28.1598, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -135.09975 -2.503954 133.906 461.521\n",
      "activ tensor(28.1598, grad_fn=<CopyBackwards>) tensor(0.0610, grad_fn=<DivBackward0>) tensor(5.3832, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(28.1804, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(28.1598, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(28.1804, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5747761 0.0 0.021951241 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(28.1803, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -138.47389 -2.6425529 133.60345 462.02838\n",
      "activ tensor(28.1803, grad_fn=<CopyBackwards>) tensor(0.0610, grad_fn=<DivBackward0>) tensor(5.7531, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(28.2015, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(28.1803, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(28.2015, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55812335 0.0 0.02126411 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(28.2018, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -136.21243 -2.7065034 136.39178 462.51236\n",
      "activ tensor(28.2018, grad_fn=<CopyBackwards>) tensor(0.0610, grad_fn=<DivBackward0>) tensor(5.9497, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(28.2267, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(28.2018, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(28.2267, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55801815 0.0 0.021158395 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(28.2271, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -136.91269 -2.6269057 133.70544 462.9708\n",
      "activ tensor(28.2271, grad_fn=<CopyBackwards>) tensor(0.0610, grad_fn=<DivBackward0>) tensor(6.7071, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(28.2507, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(28.2271, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(28.2507, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5536965 0.0 0.021126682 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(28.2510, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -137.80936 -2.7030861 132.20401 463.39432\n",
      "activ tensor(28.2510, grad_fn=<CopyBackwards>) tensor(0.0610, grad_fn=<DivBackward0>) tensor(6.5378, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(28.2654, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(28.2510, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(28.2654, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5763032 0.0 0.02227895 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(28.2655, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -137.7836 -2.6676848 135.9136 463.82248\n",
      "activ tensor(28.2655, grad_fn=<CopyBackwards>) tensor(0.0609, grad_fn=<DivBackward0>) tensor(5.6648, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(28.3225, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(28.2655, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(28.3225, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.59066236 0.0 0.022860369 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(28.3219, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -136.14604 -2.7076714 135.71312 464.28763\n",
      "activ tensor(28.3219, grad_fn=<CopyBackwards>) tensor(0.0610, grad_fn=<DivBackward0>) tensor(5.7928, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(28.3231, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(28.3219, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(28.3231, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56645936 0.0 0.021623531 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(28.3228, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -136.04623 -2.652353 137.987 464.76035\n",
      "activ tensor(28.3228, grad_fn=<CopyBackwards>) tensor(0.0609, grad_fn=<DivBackward0>) tensor(5.6402, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(28.3116, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(28.3228, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(28.3116, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5497673 0.0 0.021105539 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(28.3119, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -139.18428 -2.580793 136.69125 465.2526\n",
      "activ tensor(28.3119, grad_fn=<CopyBackwards>) tensor(0.0609, grad_fn=<DivBackward0>) tensor(6.3408, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(28.3465, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(28.3119, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(28.3465, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5666659 0.0 0.021824388 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(28.3459, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -141.97267 -2.6833284 134.00287 465.77377\n",
      "activ tensor(28.3459, grad_fn=<CopyBackwards>) tensor(0.0609, grad_fn=<DivBackward0>) tensor(6.5103, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(28.3849, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(28.3459, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(28.3849, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5633476 0.0 0.021581247 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(28.3854, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -140.14935 -2.5771437 136.311 466.3269\n",
      "activ tensor(28.3854, grad_fn=<CopyBackwards>) tensor(0.0609, grad_fn=<DivBackward0>) tensor(5.9856, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(28.4355, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(28.3854, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(28.4355, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5744703 0.0 0.022109812 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(28.4359, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -137.80829 -2.797812 138.19115 466.86646\n",
      "activ tensor(28.4359, grad_fn=<CopyBackwards>) tensor(0.0609, grad_fn=<DivBackward0>) tensor(5.8411, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(28.4947, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(28.4359, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(28.4947, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5785355 0.0 0.022215523 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(28.4947, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -138.82784 -2.7597866 138.40088 467.49533\n",
      "activ tensor(28.4947, grad_fn=<CopyBackwards>) tensor(0.0610, grad_fn=<DivBackward0>) tensor(5.9482, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(28.5124, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(28.4947, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(28.5124, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5676989 0.0 0.021877244 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(28.5128, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -138.86102 -2.7174165 136.25163 468.16315\n",
      "activ tensor(28.5128, grad_fn=<CopyBackwards>) tensor(0.0609, grad_fn=<DivBackward0>) tensor(6.3215, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(28.5447, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(28.5128, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(28.5447, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5859819 0.0 0.022395235 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(28.5448, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -138.95374 -2.6573772 136.9694 468.84848\n",
      "activ tensor(28.5448, grad_fn=<CopyBackwards>) tensor(0.0609, grad_fn=<DivBackward0>) tensor(6.4856, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(28.5188, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(28.5448, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(28.5188, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5545424 0.0 0.021147825 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(28.5189, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -140.40437 -2.6253014 134.99188 469.5206\n",
      "activ tensor(28.5189, grad_fn=<CopyBackwards>) tensor(0.0607, grad_fn=<DivBackward0>) tensor(5.9877, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(28.5847, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(28.5189, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(28.5847, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5790415 0.0 0.022321235 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(28.5855, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -137.8425 -2.5780816 135.70062 470.17642\n",
      "activ tensor(28.5855, grad_fn=<CopyBackwards>) tensor(0.0608, grad_fn=<DivBackward0>) tensor(5.4686, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(28.6265, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(28.5855, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(28.6265, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57487804 0.0 0.021982955 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(28.6266, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -138.88219 -2.651466 140.45906 470.78625\n",
      "activ tensor(28.6266, grad_fn=<CopyBackwards>) tensor(0.0608, grad_fn=<DivBackward0>) tensor(6.3343, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(28.6857, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(28.6266, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(28.6857, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56324375 0.0 0.02152839 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(28.6864, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -138.7787 -2.7349465 136.12213 471.38104\n",
      "activ tensor(28.6864, grad_fn=<CopyBackwards>) tensor(0.0609, grad_fn=<DivBackward0>) tensor(6.9401, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(28.7272, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(28.6864, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(28.7272, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5729383 0.0 0.022025239 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(28.7274, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -139.08849 -2.660857 134.71356 471.942\n",
      "activ tensor(28.7274, grad_fn=<CopyBackwards>) tensor(0.0609, grad_fn=<DivBackward0>) tensor(5.8093, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(28.7822, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(28.7274, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(28.7822, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5511513 0.0 0.020841256 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(28.7827, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -139.45721 -2.643576 136.24107 472.47565\n",
      "activ tensor(28.7827, grad_fn=<CopyBackwards>) tensor(0.0609, grad_fn=<DivBackward0>) tensor(6.2150, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(28.8145, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(28.7827, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(28.8145, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56821465 0.0 0.021792673 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(28.8149, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -138.02739 -2.8027167 136.29948 472.97623\n",
      "activ tensor(28.8149, grad_fn=<CopyBackwards>) tensor(0.0609, grad_fn=<DivBackward0>) tensor(5.9423, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(28.8890, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(28.8149, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(28.8890, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55517614 0.0 0.02126411 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(28.8883, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -141.30035 -2.7379193 138.57242 473.47446\n",
      "activ tensor(28.8883, grad_fn=<CopyBackwards>) tensor(0.0610, grad_fn=<DivBackward0>) tensor(5.9597, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(28.9295, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(28.8883, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(28.9295, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5508323 0.0 0.020968113 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(28.9291, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -139.9303 -2.61324 138.27686 473.98196\n",
      "activ tensor(28.9291, grad_fn=<CopyBackwards>) tensor(0.0610, grad_fn=<DivBackward0>) tensor(6.7350, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(28.9524, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(28.9291, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(28.9524, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5535908 0.0 0.020946968 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(28.9530, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -139.2687 -2.5765166 137.33095 474.4771\n",
      "activ tensor(28.9530, grad_fn=<CopyBackwards>) tensor(0.0610, grad_fn=<DivBackward0>) tensor(5.7920, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(28.9771, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(28.9530, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(28.9771, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5528493 0.0 0.021242967 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(28.9767, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -140.813 -2.7522902 135.80992 474.9398\n",
      "activ tensor(28.9767, grad_fn=<CopyBackwards>) tensor(0.0610, grad_fn=<DivBackward0>) tensor(6.1286, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(28.9752, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(28.9767, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(28.9752, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5748781 0.0 0.022004098 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(28.9759, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -140.66934 -2.7339582 138.58879 475.4162\n",
      "activ tensor(28.9759, grad_fn=<CopyBackwards>) tensor(0.0609, grad_fn=<DivBackward0>) tensor(5.9893, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(28.9629, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(28.9759, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(28.9629, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5510449 0.0 0.0209364 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(28.9636, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -139.3184 -2.8244708 140.45291 475.86075\n",
      "activ tensor(28.9636, grad_fn=<CopyBackwards>) tensor(0.0609, grad_fn=<DivBackward0>) tensor(6.1232, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(28.9621, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(28.9636, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(28.9621, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55507046 0.0 0.020989256 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(28.9621, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -139.49496 -2.8119857 139.84596 476.34976\n",
      "activ tensor(28.9621, grad_fn=<CopyBackwards>) tensor(0.0608, grad_fn=<DivBackward0>) tensor(6.3416, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(28.9602, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(28.9621, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(28.9602, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55980027 0.0 0.021211252 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(28.9600, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -142.64958 -2.7632074 140.80193 476.87204\n",
      "activ tensor(28.9600, grad_fn=<CopyBackwards>) tensor(0.0607, grad_fn=<DivBackward0>) tensor(5.7827, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(29.0286, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(28.9600, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(29.0286, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5892721 0.0 0.022807514 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(29.0284, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -145.48853 -2.7126932 136.98169 477.4159\n",
      "activ tensor(29.0284, grad_fn=<CopyBackwards>) tensor(0.0608, grad_fn=<DivBackward0>) tensor(6.8218, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(29.0720, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(29.0284, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(29.0720, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56053245 0.0 0.021422679 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(29.0724, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -143.3058 -2.8054743 139.38919 477.93472\n",
      "activ tensor(29.0724, grad_fn=<CopyBackwards>) tensor(0.0608, grad_fn=<DivBackward0>) tensor(5.8633, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(29.0990, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(29.0724, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(29.0990, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5675958 0.0 0.021665817 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(29.0996, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -140.92743 -2.7254064 143.02519 478.48038\n",
      "activ tensor(29.0996, grad_fn=<CopyBackwards>) tensor(0.0608, grad_fn=<DivBackward0>) tensor(6.7012, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(29.1118, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(29.0996, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(29.1118, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.573756 0.0 0.022078097 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(29.1116, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -142.30646 -2.6658711 140.3118 479.03442\n",
      "activ tensor(29.1116, grad_fn=<CopyBackwards>) tensor(0.0608, grad_fn=<DivBackward0>) tensor(6.4772, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(29.1408, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(29.1116, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(29.1408, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5613683 0.0 0.021496678 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(29.1410, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -142.93738 -2.725105 140.66898 479.5842\n",
      "activ tensor(29.1410, grad_fn=<CopyBackwards>) tensor(0.0608, grad_fn=<DivBackward0>) tensor(5.7937, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(29.1630, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(29.1410, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(29.1630, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57721734 0.0 0.022331806 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(29.1632, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -142.5315 -2.7552109 139.51575 480.15726\n",
      "activ tensor(29.1632, grad_fn=<CopyBackwards>) tensor(0.0607, grad_fn=<DivBackward0>) tensor(6.0176, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(29.1760, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(29.1632, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(29.1760, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5632438 0.0 0.02136982 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(29.1764, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -143.6589 -2.7276535 138.77716 480.74658\n",
      "activ tensor(29.1764, grad_fn=<CopyBackwards>) tensor(0.0607, grad_fn=<DivBackward0>) tensor(6.0762, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(29.1697, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(29.1764, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(29.1697, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55189484 0.0 0.020820115 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(29.1698, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -141.24538 -2.8397408 140.03773 481.33478\n",
      "activ tensor(29.1698, grad_fn=<CopyBackwards>) tensor(0.0606, grad_fn=<DivBackward0>) tensor(6.4419, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(29.2648, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(29.1698, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(29.2648, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5507258 0.0 0.02094697 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(29.2646, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -142.94313 -2.7985902 142.62613 481.99216\n",
      "activ tensor(29.2646, grad_fn=<CopyBackwards>) tensor(0.0607, grad_fn=<DivBackward0>) tensor(6.1478, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(29.2811, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(29.2646, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(29.2811, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5760997 0.0 0.021887815 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(29.2810, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -142.00035 -2.6569524 140.77411 482.66382\n",
      "activ tensor(29.2810, grad_fn=<CopyBackwards>) tensor(0.0607, grad_fn=<DivBackward0>) tensor(5.8319, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(29.2963, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(29.2810, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(29.2963, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56635576 0.0 0.021623531 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(29.2968, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -142.62381 -2.5923572 136.82968 483.3176\n",
      "activ tensor(29.2968, grad_fn=<CopyBackwards>) tensor(0.0606, grad_fn=<DivBackward0>) tensor(5.6182, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(29.2985, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(29.2968, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(29.2985, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5589624 0.0 0.02128525 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(29.2977, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -142.24359 -2.83603 140.22626 483.94113\n",
      "activ tensor(29.2977, grad_fn=<CopyBackwards>) tensor(0.0605, grad_fn=<DivBackward0>) tensor(6.1334, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(29.2631, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(29.2977, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(29.2631, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5625149 0.0 0.021560103 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(29.2638, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -142.55632 -2.8049488 140.211 484.56003\n",
      "activ tensor(29.2638, grad_fn=<CopyBackwards>) tensor(0.0604, grad_fn=<DivBackward0>) tensor(6.6173, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(29.2817, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(29.2638, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(29.2817, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5565464 0.0 0.021316964 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(29.2824, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -145.43553 -2.8050845 141.10728 485.1143\n",
      "activ tensor(29.2824, grad_fn=<CopyBackwards>) tensor(0.0604, grad_fn=<DivBackward0>) tensor(5.9178, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(29.3114, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(29.2824, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(29.3114, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57477605 0.0 0.022035811 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(29.3121, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -142.03082 -2.8129556 142.72607 485.66626\n",
      "activ tensor(29.3121, grad_fn=<CopyBackwards>) tensor(0.0604, grad_fn=<DivBackward0>) tensor(6.2449, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(29.3527, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(29.3121, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(29.3527, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56220245 0.0 0.021486105 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(29.3534, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -142.26855 -2.7742047 138.97478 486.21124\n",
      "activ tensor(29.3534, grad_fn=<CopyBackwards>) tensor(0.0604, grad_fn=<DivBackward0>) tensor(5.7244, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(29.3906, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(29.3534, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(29.3906, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5458101 0.0 0.020418406 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(29.3902, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -144.43877 -2.7126348 140.2873 486.6898\n",
      "activ tensor(29.3902, grad_fn=<CopyBackwards>) tensor(0.0604, grad_fn=<DivBackward0>) tensor(6.0076, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(29.4641, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(29.3902, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(29.4641, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5719148 0.0 0.021824386 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(29.4635, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -143.48965 -2.7677002 141.8518 487.11682\n",
      "activ tensor(29.4635, grad_fn=<CopyBackwards>) tensor(0.0605, grad_fn=<DivBackward0>) tensor(6.9007, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(29.5041, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(29.4635, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(29.5041, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56220233 0.0 0.02134868 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(29.5045, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -143.4244 -2.749273 143.37505 487.55844\n",
      "activ tensor(29.5045, grad_fn=<CopyBackwards>) tensor(0.0605, grad_fn=<DivBackward0>) tensor(6.0276, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(29.5209, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(29.5045, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(29.5209, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5714023 0.0 0.021655245 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(29.5207, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -142.76926 -2.748535 144.0943 487.96335\n",
      "activ tensor(29.5207, grad_fn=<CopyBackwards>) tensor(0.0605, grad_fn=<DivBackward0>) tensor(5.7931, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(29.5644, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(29.5207, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(29.5644, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5855819 0.0 0.022712372 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(29.5645, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -146.02892 -2.7683358 142.56897 488.37692\n",
      "activ tensor(29.5645, grad_fn=<CopyBackwards>) tensor(0.0605, grad_fn=<DivBackward0>) tensor(6.0990, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(29.5985, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(29.5645, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(29.5985, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.564283 0.0 0.021464963 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(29.5988, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -148.93445 -2.8413131 141.56102 488.7977\n",
      "activ tensor(29.5988, grad_fn=<CopyBackwards>) tensor(0.0606, grad_fn=<DivBackward0>) tensor(5.9703, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(29.6183, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(29.5988, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(29.6183, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.569348 0.0 0.022046383 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(29.6184, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -146.48772 -2.8164933 142.2042 489.25\n",
      "activ tensor(29.6184, grad_fn=<CopyBackwards>) tensor(0.0605, grad_fn=<DivBackward0>) tensor(6.6050, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(29.6010, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(29.6184, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(29.6010, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.549341 0.0 0.020925827 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(29.6009, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -143.97858 -2.9607778 146.70334 489.73993\n",
      "activ tensor(29.6009, grad_fn=<CopyBackwards>) tensor(0.0604, grad_fn=<DivBackward0>) tensor(5.9743, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(29.6415, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(29.6009, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(29.6415, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5412827 0.0 0.020545263 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(29.6414, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -145.9336 -2.9250512 144.3695 490.326\n",
      "activ tensor(29.6414, grad_fn=<CopyBackwards>) tensor(0.0605, grad_fn=<DivBackward0>) tensor(6.7266, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(29.6573, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(29.6414, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(29.6573, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57772475 0.0 0.022342378 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(29.6577, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -146.84468 -2.8205993 142.47601 490.97134\n",
      "activ tensor(29.6577, grad_fn=<CopyBackwards>) tensor(0.0604, grad_fn=<DivBackward0>) tensor(6.2295, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(29.6719, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(29.6577, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(29.6719, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5603235 0.0 0.021591816 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(29.6727, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -146.22879 -2.7424183 144.35686 491.61737\n",
      "activ tensor(29.6727, grad_fn=<CopyBackwards>) tensor(0.0604, grad_fn=<DivBackward0>) tensor(6.4047, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(29.6715, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(29.6727, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(29.6715, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5686271 0.0 0.021644674 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(29.6709, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -146.76215 -2.8454492 141.32848 492.2725\n",
      "activ tensor(29.6709, grad_fn=<CopyBackwards>) tensor(0.0603, grad_fn=<DivBackward0>) tensor(6.1509, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(29.7136, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(29.6709, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(29.7136, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5468825 0.0 0.02095754 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(29.7143, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -144.6994 -2.8884466 144.26971 492.9416\n",
      "activ tensor(29.7143, grad_fn=<CopyBackwards>) tensor(0.0603, grad_fn=<DivBackward0>) tensor(6.5329, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(29.7618, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(29.7143, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(29.7618, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56178516 0.0 0.021412106 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(29.7620, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -145.91327 -2.8856747 146.15735 493.5802\n",
      "activ tensor(29.7620, grad_fn=<CopyBackwards>) tensor(0.0603, grad_fn=<DivBackward0>) tensor(5.8048, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(29.7530, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(29.7620, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(29.7530, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55591434 0.0 0.021211252 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(29.7537, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -144.0262 -2.8283675 143.11731 494.2466\n",
      "activ tensor(29.7537, grad_fn=<CopyBackwards>) tensor(0.0602, grad_fn=<DivBackward0>) tensor(6.0223, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(29.7630, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(29.7537, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(29.7630, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5762014 0.0 0.022120383 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(29.7638, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -146.24707 -2.8097725 141.47185 494.88004\n",
      "activ tensor(29.7638, grad_fn=<CopyBackwards>) tensor(0.0601, grad_fn=<DivBackward0>) tensor(6.7453, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(29.7565, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(29.7638, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(29.7565, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5662523 0.0 0.021570675 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(29.7562, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -146.88002 -2.7731407 142.36137 495.47708\n",
      "activ tensor(29.7562, grad_fn=<CopyBackwards>) tensor(0.0601, grad_fn=<DivBackward0>) tensor(6.7268, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(29.7978, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(29.7562, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(29.7978, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55316734 0.0 0.021052683 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(29.7974, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -146.10776 -2.7718139 144.83542 496.03107\n",
      "activ tensor(29.7974, grad_fn=<CopyBackwards>) tensor(0.0601, grad_fn=<DivBackward0>) tensor(6.0130, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(29.8193, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(29.7974, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(29.8193, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5789405 0.0 0.022067524 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(29.8194, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -149.11519 -2.6838152 144.27621 496.5715\n",
      "activ tensor(29.8194, grad_fn=<CopyBackwards>) tensor(0.0601, grad_fn=<DivBackward0>) tensor(5.8841, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(29.8651, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(29.8194, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(29.8651, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57058144 0.0 0.021972382 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(29.8643, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -145.16121 -2.7876794 145.28903 497.07675\n",
      "activ tensor(29.8643, grad_fn=<CopyBackwards>) tensor(0.0601, grad_fn=<DivBackward0>) tensor(5.9013, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(29.9096, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(29.8643, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(29.9096, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55348486 0.0 0.020830687 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(29.9098, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -147.45667 -2.6996694 143.18619 497.55768\n",
      "activ tensor(29.9098, grad_fn=<CopyBackwards>) tensor(0.0601, grad_fn=<DivBackward0>) tensor(6.4581, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(29.9071, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(29.9098, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(29.9071, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5654239 0.0 0.021718672 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(29.9074, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -147.469 -2.8089724 142.3353 497.99652\n",
      "activ tensor(29.9074, grad_fn=<CopyBackwards>) tensor(0.0601, grad_fn=<DivBackward0>) tensor(6.3054, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(29.9709, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(29.9074, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(29.9709, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5686272 0.0 0.021623531 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(29.9713, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -146.40253 -2.9025598 146.79318 498.4384\n",
      "activ tensor(29.9713, grad_fn=<CopyBackwards>) tensor(0.0601, grad_fn=<DivBackward0>) tensor(5.8206, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(29.9926, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(29.9713, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(29.9926, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5696565 0.0 0.021813815 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(29.9929, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -147.6747 -2.9135892 145.87761 498.91058\n",
      "activ tensor(29.9929, grad_fn=<CopyBackwards>) tensor(0.0601, grad_fn=<DivBackward0>) tensor(6.2113, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(30.0271, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(29.9929, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(30.0271, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5765064 0.0 0.022310663 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(30.0272, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -146.50172 -2.924713 147.29202 499.43863\n",
      "activ tensor(30.0272, grad_fn=<CopyBackwards>) tensor(0.0601, grad_fn=<DivBackward0>) tensor(7.0292, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(30.0983, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(30.0272, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(30.0983, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56449056 0.0 0.02159182 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(30.0982, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -149.40189 -2.8855777 146.42403 499.96964\n",
      "activ tensor(30.0982, grad_fn=<CopyBackwards>) tensor(0.0602, grad_fn=<DivBackward0>) tensor(6.3915, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(30.1389, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(30.0982, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(30.1389, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55854315 0.0 0.021264108 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(30.1391, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -152.24118 -2.8797827 143.69089 500.5042\n",
      "activ tensor(30.1391, grad_fn=<CopyBackwards>) tensor(0.0602, grad_fn=<DivBackward0>) tensor(5.8974, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(30.1648, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(30.1391, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(30.1648, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56718284 0.0 0.021560103 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(30.1646, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -149.6987 -2.964141 147.18234 501.03\n",
      "activ tensor(30.1646, grad_fn=<CopyBackwards>) tensor(0.0602, grad_fn=<DivBackward0>) tensor(6.2105, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(30.2050, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(30.1646, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(30.2050, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5569674 0.0 0.021338109 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(30.2060, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -147.07123 -3.0351713 149.16727 501.57034\n",
      "activ tensor(30.2060, grad_fn=<CopyBackwards>) tensor(0.0602, grad_fn=<DivBackward0>) tensor(6.0428, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(30.1943, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(30.2060, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(30.1943, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.58096087 0.0 0.022659516 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(30.1945, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -149.50505 -3.0107536 148.2001 502.12088\n",
      "activ tensor(30.1945, grad_fn=<CopyBackwards>) tensor(0.0601, grad_fn=<DivBackward0>) tensor(6.2845, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(30.1999, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(30.1945, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(30.1999, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5707867 0.0 0.021845527 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(30.2002, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -150.84355 -2.8650992 145.806 502.6869\n",
      "activ tensor(30.2002, grad_fn=<CopyBackwards>) tensor(0.0601, grad_fn=<DivBackward0>) tensor(7.2201, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(30.2424, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(30.2002, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(30.2424, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57222205 0.0 0.021961812 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(30.2427, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -149.74634 -3.0519795 146.82474 503.2438\n",
      "activ tensor(30.2427, grad_fn=<CopyBackwards>) tensor(0.0601, grad_fn=<DivBackward0>) tensor(6.2163, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(30.2890, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(30.2427, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(30.2890, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57201743 0.0 0.021750389 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(30.2887, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -149.98639 -2.907539 146.48575 503.80362\n",
      "activ tensor(30.2887, grad_fn=<CopyBackwards>) tensor(0.0601, grad_fn=<DivBackward0>) tensor(6.4762, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(30.3314, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(30.2887, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(30.3314, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55770314 0.0 0.021253536 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(30.3312, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -148.74693 -2.8114738 146.48459 504.35812\n",
      "activ tensor(30.3312, grad_fn=<CopyBackwards>) tensor(0.0601, grad_fn=<DivBackward0>) tensor(6.2663, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(30.3809, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(30.3312, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(30.3809, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5686273 0.0 0.021739814 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(30.3815, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -148.33145 -2.717836 150.48175 504.95047\n",
      "activ tensor(30.3815, grad_fn=<CopyBackwards>) tensor(0.0602, grad_fn=<DivBackward0>) tensor(5.9951, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(30.4150, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(30.3815, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(30.4150, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5569672 0.0 0.021390963 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(30.4142, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -147.46782 -2.7726572 146.00673 505.515\n",
      "activ tensor(30.4142, grad_fn=<CopyBackwards>) tensor(0.0602, grad_fn=<DivBackward0>) tensor(6.6588, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(30.4528, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(30.4142, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(30.4528, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5812634 0.0 0.022374092 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(30.4529, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -149.79025 -2.787342 144.40111 506.0726\n",
      "activ tensor(30.4529, grad_fn=<CopyBackwards>) tensor(0.0602, grad_fn=<DivBackward0>) tensor(6.5057, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(30.4449, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(30.4529, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(30.4449, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57242656 0.0 0.021972384 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(30.4457, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -149.03778 -2.8286998 146.97517 506.60858\n",
      "activ tensor(30.4457, grad_fn=<CopyBackwards>) tensor(0.0601, grad_fn=<DivBackward0>) tensor(6.1527, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(30.5010, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(30.4457, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(30.5010, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5587527 0.0 0.021327537 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(30.5016, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -150.2887 -2.9552183 147.07414 507.17252\n",
      "activ tensor(30.5016, grad_fn=<CopyBackwards>) tensor(0.0601, grad_fn=<DivBackward0>) tensor(5.7976, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(30.5407, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(30.5016, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(30.5407, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5762016 0.0 0.021930097 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(30.5404, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -151.60141 -2.8572578 148.97136 507.77112\n",
      "activ tensor(30.5404, grad_fn=<CopyBackwards>) tensor(0.0601, grad_fn=<DivBackward0>) tensor(6.2645, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(30.5340, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(30.5404, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(30.5340, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55125755 0.0 0.021242965 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(30.5337, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -149.40448 -2.8981647 147.57585 508.40454\n",
      "activ tensor(30.5337, grad_fn=<CopyBackwards>) tensor(0.0601, grad_fn=<DivBackward0>) tensor(6.7520, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(30.5780, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(30.5337, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(30.5780, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5664593 0.0 0.021813815 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(30.5783, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -150.95158 -2.9754076 146.32117 509.03662\n",
      "activ tensor(30.5783, grad_fn=<CopyBackwards>) tensor(0.0601, grad_fn=<DivBackward0>) tensor(6.0862, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(30.6418, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(30.5783, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(30.6418, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55738777 0.0 0.021221824 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(30.6428, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -149.72781 -2.8558784 146.58852 509.62112\n",
      "activ tensor(30.6428, grad_fn=<CopyBackwards>) tensor(0.0601, grad_fn=<DivBackward0>) tensor(6.1772, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(30.6536, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(30.6428, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(30.6536, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56738925 0.0 0.021729244 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(30.6528, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -149.13885 -2.9662228 149.13596 510.1896\n",
      "activ tensor(30.6528, grad_fn=<CopyBackwards>) tensor(0.0601, grad_fn=<DivBackward0>) tensor(6.2669, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(30.6614, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(30.6528, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(30.6614, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5780289 0.0 0.022173239 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(30.6620, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -150.47295 -2.922574 150.61472 510.75488\n",
      "activ tensor(30.6620, grad_fn=<CopyBackwards>) tensor(0.0600, grad_fn=<DivBackward0>) tensor(6.8516, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(30.6642, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(30.6620, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(30.6642, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56313956 0.0 0.02144382 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(30.6641, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -149.75471 -3.0356042 149.16974 511.26178\n",
      "activ tensor(30.6641, grad_fn=<CopyBackwards>) tensor(0.0600, grad_fn=<DivBackward0>) tensor(6.5984, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(30.6963, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(30.6641, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(30.6963, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5473108 0.0 0.020576974 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(30.6960, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -152.77925 -2.9736063 150.21771 511.77335\n",
      "activ tensor(30.6960, grad_fn=<CopyBackwards>) tensor(0.0600, grad_fn=<DivBackward0>) tensor(5.9170, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(30.7373, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(30.6960, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(30.7373, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5527436 0.0 0.020841258 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(30.7383, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -155.46886 -3.0768592 147.2773 512.246\n",
      "activ tensor(30.7383, grad_fn=<CopyBackwards>) tensor(0.0600, grad_fn=<DivBackward0>) tensor(6.4275, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(30.7444, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(30.7383, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(30.7444, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55072594 0.0 0.020957544 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(30.7442, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -152.85965 -2.9972901 149.86205 512.7092\n",
      "activ tensor(30.7442, grad_fn=<CopyBackwards>) tensor(0.0600, grad_fn=<DivBackward0>) tensor(7.3843, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(30.7817, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(30.7442, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(30.7817, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.54258007 0.0 0.020598121 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(30.7810, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -150.26294 -2.834762 154.0521 513.1721\n",
      "activ tensor(30.7810, grad_fn=<CopyBackwards>) tensor(0.0600, grad_fn=<DivBackward0>) tensor(6.5111, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(30.8062, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(30.7810, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(30.8062, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57863677 0.0 0.021972384 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(30.8065, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -153.04892 -2.8345351 150.1033 513.65137\n",
      "activ tensor(30.8065, grad_fn=<CopyBackwards>) tensor(0.0600, grad_fn=<DivBackward0>) tensor(6.2858, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(30.8213, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(30.8065, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(30.8213, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57211983 0.0 0.021951241 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(30.8211, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -154.78625 -2.9367392 149.89136 514.1024\n",
      "activ tensor(30.8211, grad_fn=<CopyBackwards>) tensor(0.0600, grad_fn=<DivBackward0>) tensor(6.5364, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(30.8304, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(30.8211, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(30.8304, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.577826 0.0 0.022627803 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(30.8311, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -153.31172 -3.0246754 149.97537 514.5703\n",
      "activ tensor(30.8311, grad_fn=<CopyBackwards>) tensor(0.0599, grad_fn=<DivBackward0>) tensor(6.8367, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(30.8160, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(30.8311, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(30.8160, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56676936 0.0 0.021813814 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(30.8164, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -153.05453 -2.9559884 149.78824 515.0604\n",
      "activ tensor(30.8164, grad_fn=<CopyBackwards>) tensor(0.0598, grad_fn=<DivBackward0>) tensor(6.3737, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(30.8419, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(30.8164, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(30.8419, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5581233 0.0 0.021168968 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(30.8426, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -151.75336 -2.9635172 151.22437 515.59204\n",
      "activ tensor(30.8426, grad_fn=<CopyBackwards>) tensor(0.0598, grad_fn=<DivBackward0>) tensor(7.1422, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(30.8519, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(30.8426, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(30.8519, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5580182 0.0 0.02134868 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(30.8517, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -152.11641 -3.0622096 152.29088 516.18823\n",
      "activ tensor(30.8517, grad_fn=<CopyBackwards>) tensor(0.0598, grad_fn=<DivBackward0>) tensor(6.2062, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(30.9024, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(30.8517, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(30.9024, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5547539 0.0 0.02119011 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(30.9019, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -152.85728 -2.9802155 150.5654 516.83704\n",
      "activ tensor(30.9019, grad_fn=<CopyBackwards>) tensor(0.0598, grad_fn=<DivBackward0>) tensor(6.3035, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(30.9240, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(30.9019, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(30.9240, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57426614 0.0 0.022257809 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(30.9246, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -153.20718 -3.0138712 146.99774 517.4839\n",
      "activ tensor(30.9246, grad_fn=<CopyBackwards>) tensor(0.0598, grad_fn=<DivBackward0>) tensor(6.4125, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(30.9567, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(30.9246, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(30.9567, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55770326 0.0 0.021338105 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(30.9576, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -152.15144 -2.9241235 150.6331 518.144\n",
      "activ tensor(30.9576, grad_fn=<CopyBackwards>) tensor(0.0597, grad_fn=<DivBackward0>) tensor(6.1322, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(30.9712, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(30.9576, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(30.9712, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5816668 0.0 0.022490377 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(30.9709, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -153.74834 -2.9471674 151.34523 518.80927\n",
      "activ tensor(30.9709, grad_fn=<CopyBackwards>) tensor(0.0597, grad_fn=<DivBackward0>) tensor(7.0902, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(31.0421, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(30.9709, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(31.0421, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57934517 0.0 0.022268381 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(31.0425, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -155.62715 -2.8830295 151.13055 519.4557\n",
      "activ tensor(31.0425, grad_fn=<CopyBackwards>) tensor(0.0598, grad_fn=<DivBackward0>) tensor(6.4526, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(31.1191, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(31.0425, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(31.1191, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56199396 0.0 0.021316964 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(31.1189, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -151.74165 -3.0492043 152.04362 520.0459\n",
      "activ tensor(31.1189, grad_fn=<CopyBackwards>) tensor(0.0598, grad_fn=<DivBackward0>) tensor(6.4033, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(31.1092, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(31.1189, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(31.1092, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.54731095 0.0 0.020883543 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(31.1093, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -152.85123 -2.9993346 148.31372 520.6346\n",
      "activ tensor(31.1093, grad_fn=<CopyBackwards>) tensor(0.0598, grad_fn=<DivBackward0>) tensor(6.2054, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(31.1246, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(31.1093, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(31.1246, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5522133 0.0 0.020872973 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(31.1247, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -153.10597 -2.954615 150.68399 521.1821\n",
      "activ tensor(31.1247, grad_fn=<CopyBackwards>) tensor(0.0597, grad_fn=<DivBackward0>) tensor(6.7350, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(31.1389, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(31.1247, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(31.1389, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5767096 0.0 0.022215523 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(31.1384, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -151.98367 -2.9721632 152.98653 521.7177\n",
      "activ tensor(31.1384, grad_fn=<CopyBackwards>) tensor(0.0597, grad_fn=<DivBackward0>) tensor(7.8085, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(31.1083, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(31.1384, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(31.1083, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5680085 0.0 0.022004098 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(31.1076, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -154.58192 -2.853228 152.91742 522.2404\n",
      "activ tensor(31.1076, grad_fn=<CopyBackwards>) tensor(0.0596, grad_fn=<DivBackward0>) tensor(5.8463, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(31.1248, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(31.1076, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(31.1248, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56042796 0.0 0.021496676 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(31.1253, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -153.29498 -2.79068 153.59448 522.7396\n",
      "activ tensor(31.1253, grad_fn=<CopyBackwards>) tensor(0.0595, grad_fn=<DivBackward0>) tensor(6.2729, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(31.1554, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(31.1253, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(31.1554, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.58287394 0.0 0.02243752 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(31.1554, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -155.97787 -2.9619887 152.02744 523.1787\n",
      "activ tensor(31.1554, grad_fn=<CopyBackwards>) tensor(0.0596, grad_fn=<DivBackward0>) tensor(6.3915, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(31.1883, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(31.1554, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(31.1883, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5664592 0.0 0.021306396 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(31.1886, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -158.69817 -3.0713515 151.76627 523.6303\n",
      "activ tensor(31.1886, grad_fn=<CopyBackwards>) tensor(0.0596, grad_fn=<DivBackward0>) tensor(7.0008, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(31.2355, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(31.1886, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(31.2355, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5682147 0.0 0.021919526 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(31.2360, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -155.9908 -3.032014 153.07266 524.0958\n",
      "activ tensor(31.2360, grad_fn=<CopyBackwards>) tensor(0.0596, grad_fn=<DivBackward0>) tensor(6.6466, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(31.3057, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(31.2360, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(31.3057, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.549874 0.0 0.020925827 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(31.3059, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -153.48582 -3.0983615 157.11806 524.5865\n",
      "activ tensor(31.3059, grad_fn=<CopyBackwards>) tensor(0.0597, grad_fn=<DivBackward0>) tensor(6.0842, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(31.2980, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(31.3059, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(31.2980, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56168103 0.0 0.021316964 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(31.2982, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -156.64041 -3.2208652 154.41882 525.1122\n",
      "activ tensor(31.2982, grad_fn=<CopyBackwards>) tensor(0.0596, grad_fn=<DivBackward0>) tensor(6.1582, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(31.3623, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(31.2982, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(31.3623, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.573858 0.0 0.022152094 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(31.3627, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -158.63316 -3.0679014 151.63377 525.67236\n",
      "activ tensor(31.3627, grad_fn=<CopyBackwards>) tensor(0.0597, grad_fn=<DivBackward0>) tensor(6.6423, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(31.3838, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(31.3627, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(31.3838, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5776232 0.0 0.022321235 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(31.3843, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -156.85959 -3.1270008 154.72362 526.22327\n",
      "activ tensor(31.3843, grad_fn=<CopyBackwards>) tensor(0.0596, grad_fn=<DivBackward0>) tensor(7.0431, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(31.4380, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(31.3843, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(31.4380, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5479529 0.0 0.020756686 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(31.4383, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -156.14578 -3.0572605 152.80887 526.7859\n",
      "activ tensor(31.4383, grad_fn=<CopyBackwards>) tensor(0.0597, grad_fn=<DivBackward0>) tensor(7.1353, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(31.4415, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(31.4383, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(31.4415, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5364991 0.0 0.020365551 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(31.4409, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -155.26753 -3.0816135 154.84676 527.3623\n",
      "activ tensor(31.4409, grad_fn=<CopyBackwards>) tensor(0.0596, grad_fn=<DivBackward0>) tensor(6.8818, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(31.4696, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(31.4409, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(31.4696, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5704787 0.0 0.02179267 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(31.4707, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -154.5316 -3.005793 156.22598 527.9317\n",
      "activ tensor(31.4707, grad_fn=<CopyBackwards>) tensor(0.0596, grad_fn=<DivBackward0>) tensor(6.8489, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(31.5498, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(31.4707, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(31.5498, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5673893 0.0 0.021718673 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(31.5499, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -156.12688 -3.1076365 152.52045 528.46436\n",
      "activ tensor(31.5499, grad_fn=<CopyBackwards>) tensor(0.0597, grad_fn=<DivBackward0>) tensor(6.3438, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(31.5895, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(31.5499, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(31.5895, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56074154 0.0 0.021380393 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(31.5895, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -156.71185 -3.042499 151.93265 529.0138\n",
      "activ tensor(31.5895, grad_fn=<CopyBackwards>) tensor(0.0597, grad_fn=<DivBackward0>) tensor(6.9008, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(31.6063, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(31.5895, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(31.6063, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5583332 0.0 0.021486105 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(31.6065, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -156.52106 -2.9728327 153.03503 529.5672\n",
      "activ tensor(31.6065, grad_fn=<CopyBackwards>) tensor(0.0597, grad_fn=<DivBackward0>) tensor(6.8910, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(31.6445, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(31.6065, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(31.6445, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5587528 0.0 0.021369819 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(31.6439, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -157.26085 -3.0501938 155.50784 530.1472\n",
      "activ tensor(31.6439, grad_fn=<CopyBackwards>) tensor(0.0597, grad_fn=<DivBackward0>) tensor(6.5538, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(31.6741, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(31.6439, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(31.6741, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5371538 0.0 0.020175265 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(31.6740, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -158.57056 -2.881979 154.64938 530.7291\n",
      "activ tensor(31.6740, grad_fn=<CopyBackwards>) tensor(0.0597, grad_fn=<DivBackward0>) tensor(5.9740, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(31.7157, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(31.6740, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(31.7157, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5461319 0.0 0.02036555 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(31.7167, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -156.28128 -2.8534594 154.24216 531.33136\n",
      "activ tensor(31.7167, grad_fn=<CopyBackwards>) tensor(0.0597, grad_fn=<DivBackward0>) tensor(6.3657, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(31.7558, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(31.7167, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(31.7558, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5970752 0.0 0.023092937 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(31.7552, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -156.39737 -3.0124054 152.82257 531.9062\n",
      "activ tensor(31.7552, grad_fn=<CopyBackwards>) tensor(0.0597, grad_fn=<DivBackward0>) tensor(6.8751, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(31.8554, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(31.7552, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(31.8554, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5732448 0.0 0.022056954 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(31.8557, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -157.38423 -3.0557673 152.89343 532.4868\n",
      "activ tensor(31.8557, grad_fn=<CopyBackwards>) tensor(0.0598, grad_fn=<DivBackward0>) tensor(6.7080, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(31.8878, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(31.8557, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(31.8878, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5781303 0.0 0.022014668 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(31.8880, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -155.33958 -3.0779767 157.58142 533.0693\n",
      "activ tensor(31.8880, grad_fn=<CopyBackwards>) tensor(0.0598, grad_fn=<DivBackward0>) tensor(6.5574, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(31.9224, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(31.8880, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(31.9224, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57078665 0.0 0.022078095 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(31.9228, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -158.25972 -3.0082781 155.8277 533.6596\n",
      "activ tensor(31.9228, grad_fn=<CopyBackwards>) tensor(0.0598, grad_fn=<DivBackward0>) tensor(6.1487, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(31.9188, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(31.9228, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(31.9188, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55927676 0.0 0.021570677 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(31.9191, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -156.82516 -3.2063622 156.12875 534.24066\n",
      "activ tensor(31.9191, grad_fn=<CopyBackwards>) tensor(0.0597, grad_fn=<DivBackward0>) tensor(6.4060, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(31.9602, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(31.9191, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(31.9602, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5723243 0.0 0.021813814 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(31.9597, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -159.28983 -3.0546772 156.40775 534.81805\n",
      "activ tensor(31.9597, grad_fn=<CopyBackwards>) tensor(0.0598, grad_fn=<DivBackward0>) tensor(7.3558, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(32.0094, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(31.9597, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(32.0094, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5643868 0.0 0.021602388 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(32.0095, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -161.69191 -3.0793595 153.8278 535.368\n",
      "activ tensor(32.0095, grad_fn=<CopyBackwards>) tensor(0.0598, grad_fn=<DivBackward0>) tensor(6.5954, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(32.0750, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(32.0095, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(32.0750, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5799515 0.0 0.022162667 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(32.0756, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -159.24562 -3.0252807 157.94797 535.87286\n",
      "activ tensor(32.0756, grad_fn=<CopyBackwards>) tensor(0.0599, grad_fn=<DivBackward0>) tensor(7.0084, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(32.0691, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(32.0756, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(32.0691, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.569245 0.0 0.021708101 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(32.0685, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -156.6991 -3.0500398 159.72125 536.3545\n",
      "activ tensor(32.0685, grad_fn=<CopyBackwards>) tensor(0.0598, grad_fn=<DivBackward0>) tensor(6.0718, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(32.0850, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(32.0685, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(32.0850, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56449056 0.0 0.021401536 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(32.0853, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -160.23572 -3.1318603 157.73248 536.827\n",
      "activ tensor(32.0853, grad_fn=<CopyBackwards>) tensor(0.0598, grad_fn=<DivBackward0>) tensor(7.7282, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(32.1186, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(32.0853, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(32.1186, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57355154 0.0 0.021877242 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(32.1185, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -162.45502 -3.1335268 155.46255 537.2545\n",
      "activ tensor(32.1185, grad_fn=<CopyBackwards>) tensor(0.0598, grad_fn=<DivBackward0>) tensor(6.7532, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(32.1683, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(32.1185, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(32.1683, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5609505 0.0 0.021454392 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(32.1686, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -160.32166 -3.2554688 156.99916 537.7252\n",
      "activ tensor(32.1686, grad_fn=<CopyBackwards>) tensor(0.0598, grad_fn=<DivBackward0>) tensor(6.2619, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(32.1676, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(32.1686, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(32.1676, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5842798 0.0 0.0226278 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(32.1682, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -159.27246 -3.1536677 158.00443 538.21545\n",
      "activ tensor(32.1682, grad_fn=<CopyBackwards>) tensor(0.0598, grad_fn=<DivBackward0>) tensor(6.8959, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(32.1828, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(32.1682, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(32.1828, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5668726 0.0 0.02151782 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(32.1830, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -159.51387 -3.1034045 157.25455 538.73285\n",
      "activ tensor(32.1830, grad_fn=<CopyBackwards>) tensor(0.0597, grad_fn=<DivBackward0>) tensor(6.0155, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(32.2176, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(32.1830, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(32.2176, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.57518363 0.0 0.022130955 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(32.2177, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -157.18567 -3.1476626 159.95375 539.2874\n",
      "activ tensor(32.2177, grad_fn=<CopyBackwards>) tensor(0.0597, grad_fn=<DivBackward0>) tensor(6.7144, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(32.2078, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(32.2177, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(32.2078, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5645943 0.0 0.021866672 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(32.2082, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -158.41301 -3.0292358 155.981 539.88\n",
      "activ tensor(32.2082, grad_fn=<CopyBackwards>) tensor(0.0597, grad_fn=<DivBackward0>) tensor(7.1451, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(32.2841, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(32.2082, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(32.2841, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5731426 0.0 0.022035811 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(32.2836, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -160.00964 -3.195055 154.53258 540.4644\n",
      "activ tensor(32.2836, grad_fn=<CopyBackwards>) tensor(0.0597, grad_fn=<DivBackward0>) tensor(6.5630, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(32.3362, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(32.2836, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(32.3362, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56831795 0.0 0.021898385 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(32.3370, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -158.27939 -3.089919 158.04826 541.0823\n",
      "activ tensor(32.3370, grad_fn=<CopyBackwards>) tensor(0.0598, grad_fn=<DivBackward0>) tensor(7.0646, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(32.3791, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(32.3370, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(32.3791, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5646981 0.0 0.021697532 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(32.3800, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -161.86476 -3.1729145 157.61246 541.6934\n",
      "activ tensor(32.3800, grad_fn=<CopyBackwards>) tensor(0.0598, grad_fn=<DivBackward0>) tensor(6.5463, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(32.4523, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(32.3800, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(32.4523, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5544369 0.0 0.021126682 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(32.4524, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -160.94826 -3.0753508 158.99629 542.2982\n",
      "activ tensor(32.4524, grad_fn=<CopyBackwards>) tensor(0.0598, grad_fn=<DivBackward0>) tensor(6.9558, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(32.4495, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(32.4524, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(32.4495, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.55633575 0.0 0.021316964 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(32.4493, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -160.29782 -3.0919588 156.99718 542.89984\n",
      "activ tensor(32.4493, grad_fn=<CopyBackwards>) tensor(0.0598, grad_fn=<DivBackward0>) tensor(7.1764, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(32.5173, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(32.4493, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(32.5173, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5774203 0.0 0.022078097 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(32.5172, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -160.93045 -3.152824 155.62366 543.505\n",
      "activ tensor(32.5172, grad_fn=<CopyBackwards>) tensor(0.0598, grad_fn=<DivBackward0>) tensor(8.0160, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(32.4903, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(32.5172, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(32.4903, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5763031 0.0 0.02243752 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(32.4900, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -161.01073 -2.9867768 157.55173 544.0737\n",
      "activ tensor(32.4900, grad_fn=<CopyBackwards>) tensor(0.0597, grad_fn=<DivBackward0>) tensor(6.2417, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(32.5411, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(32.4900, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(32.5411, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56873006 0.0 0.02169753 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(32.5419, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -160.29141 -2.954134 159.79152 544.61694\n",
      "activ tensor(32.5419, grad_fn=<CopyBackwards>) tensor(0.0598, grad_fn=<DivBackward0>) tensor(6.3245, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(32.6015, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(32.5419, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(32.6015, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5746742 0.0 0.02219438 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(32.6010, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -160.74495 -3.0442028 160.28458 545.1436\n",
      "activ tensor(32.6010, grad_fn=<CopyBackwards>) tensor(0.0598, grad_fn=<DivBackward0>) tensor(7.0638, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(32.6317, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(32.6010, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(32.6317, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5569672 0.0 0.02125354 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(32.6315, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -160.41977 -3.1066372 158.34343 545.63666\n",
      "activ tensor(32.6315, grad_fn=<CopyBackwards>) tensor(0.0598, grad_fn=<DivBackward0>) tensor(6.7864, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(32.6386, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(32.6315, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(32.6386, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5460248 0.0 0.020661546 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(32.6382, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -162.46315 -3.0971093 159.7361 546.11414\n",
      "activ tensor(32.6382, grad_fn=<CopyBackwards>) tensor(0.0598, grad_fn=<DivBackward0>) tensor(7.0429, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(32.6905, grad_fn=<CopyBackwards>)\n",
      "input tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])\n",
      "qinput tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(32.6382, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(32.6905, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5811626 0.0 0.022490375 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(32.6905, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -161.97974 -3.1661458 155.32 541.2995\n",
      "activ tensor(32.6905, grad_fn=<CopyBackwards>) tensor(0.0604, grad_fn=<DivBackward0>) tensor(6.8428, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(32.7464, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(32.6905, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(32.7464, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.58025455 0.0 0.022352949 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(32.7472, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -159.57573 -3.241165 158.81133 541.7634\n",
      "activ tensor(32.7472, grad_fn=<CopyBackwards>) tensor(0.0604, grad_fn=<DivBackward0>) tensor(6.9449, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(32.8040, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(32.7472, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(32.8040, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.54752505 0.0 0.020946972 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(32.8037, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -158.01086 -3.1397452 163.16293 542.2873\n",
      "activ tensor(32.8037, grad_fn=<CopyBackwards>) tensor(0.0605, grad_fn=<DivBackward0>) tensor(7.3908, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(32.8872, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(32.8037, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(32.8872, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.56136817 0.0 0.02134868 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(32.8872, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -162.33032 -3.2548218 156.96042 542.82825\n",
      "activ tensor(32.8872, grad_fn=<CopyBackwards>) tensor(0.0606, grad_fn=<DivBackward0>) tensor(6.7690, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(32.9279, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(32.8872, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(32.9279, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.54805964 0.0 0.020841256 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(32.9283, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -163.48456 -3.1942978 157.8815 543.40466\n",
      "activ tensor(32.9283, grad_fn=<CopyBackwards>) tensor(0.0606, grad_fn=<DivBackward0>) tensor(6.8550, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(32.9379, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(32.9283, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(32.9379, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5533792 0.0 0.02085183 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(32.9373, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -162.37184 -3.0607164 160.68607 543.98596\n",
      "activ tensor(32.9373, grad_fn=<CopyBackwards>) tensor(0.0605, grad_fn=<DivBackward0>) tensor(6.5014, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(32.9514, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(32.9373, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(32.9514, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5724267 0.0 0.021824386 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(32.9524, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -161.66942 -3.3141508 157.44241 544.5847\n",
      "activ tensor(32.9524, grad_fn=<CopyBackwards>) tensor(0.0605, grad_fn=<DivBackward0>) tensor(7.7051, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(32.9350, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(32.9524, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(32.9350, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5652166 0.0 0.021750389 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(32.9359, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -160.81195 -3.476279 162.45586 545.1616\n",
      "activ tensor(32.9359, grad_fn=<CopyBackwards>) tensor(0.0604, grad_fn=<DivBackward0>) tensor(7.6264, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(32.9676, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(32.9359, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(32.9676, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.5623067 0.0 0.021528391 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(32.9679, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -159.88388 -3.3685675 160.87363 545.7477\n",
      "activ tensor(32.9679, grad_fn=<CopyBackwards>) tensor(0.0604, grad_fn=<DivBackward0>) tensor(6.8158, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(33.0214, grad_fn=<CopyBackwards>)\n",
      "input tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "qinput tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "diff U.x tensor(0., grad_fn=<CopyBackwards>)\n",
      "diff_hidden_scaled tensor(32.9679, grad_fn=<CopyBackwards>)\n",
      "diff_hidden_quant tensor(33.0214, grad_fn=<CopyBackwards>)\n",
      "quant diff hidden 0.58367765 0.0 0.022638373 0.043299917 torch.Size([1, 512])\n",
      "diff h_t tensor(33.0220, grad_fn=<CopyBackwards>)\n",
      "h_t, min,mean,max,norm -163.23312 -3.277036 157.87927 546.3574\n",
      "activ tensor(33.0220, grad_fn=<CopyBackwards>) tensor(0.0604, grad_fn=<DivBackward0>) tensor(6.7731, grad_fn=<MaxBackward1>)\n",
      "activ quantif tensor(33.0800, grad_fn=<CopyBackwards>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch.nn.functional as F\n",
    "hidden = []\n",
    "hidden_scaled = []\n",
    "hidden_scaled_input = []\n",
    "hidden_scaled_quant = []\n",
    "hidden_scaled_rec = []\n",
    "for ii in range(1000):\n",
    "    print(\"input\",x0[0:1,ii])\n",
    "    print(\"qinput\",model_scaled.quant_input(x0[0:1,ii])/input_pre_scaling_factor)\n",
    "    #print(model.input_layer(x0[0:1,ii]))\n",
    "    #print(model_scaled.input_layer(x0[0:1,ii])*saved_quant_input_max_absolute*saved_alpha_u)\n",
    "    print(\"diff U.x\",torch.norm(model.input_layer(x0[0:1,ii])-model_scaled.input_layer(model_scaled.quant_input(x0[0:1,ii]))*saved_alpha_u/input_pre_scaling_factor))\n",
    "    \n",
    "    if len(hidden)==0:\n",
    "        if 'recurrent_layer.bias' in new_weights:\n",
    "                hidden.append(model.input_layer(x0[0:1,ii])+model.recurrent_layer.bias)\n",
    "                hidden_scaled.append(model_scaled.input_layer(model_scaled.quant_input(x0[0:1,ii]))+model_scaled.recurrent_layer.bias)\n",
    "                hidden_scaled_rec.append(torch.zeros_like(hidden_scaled[0])+model_scaled.recurrent_layer.bias) # fake\n",
    "        else:\n",
    "            hidden.append(model.input_layer(x0[0:1,ii]))\n",
    "            hidden_scaled.append(model_scaled.input_layer(model_scaled.quant_input(x0[0:1,ii])))\n",
    "            hidden_scaled_rec.append(torch.zeros_like(hidden_scaled[0])) # fake\n",
    "        #print(hidden_scaled[0])\n",
    "        #print(hidden_scaled[0]*2**(config.model['num_bits']))\n",
    "        hidden_scaled_quant.append(torch.zeros_like(hidden_scaled[0])) # fake\n",
    "    else:\n",
    "        #print(\"hidden\",hidden[-1][0,0:10])\n",
    "        #print(\"hidden_rec\",model.recurrent_layer(hidden[-1])[0,0:10])\n",
    "        hidden.append(model.input_layer(x0[0:1,ii])+model.recurrent_layer(hidden[-1]))\n",
    "        #print(\"hidden_scaled\",hidden_scaled[-1][0,0:10])\n",
    "        #print(\"hidden_scaled\",hidden_scaled[-1][0,0:10]*saved_alpha_u/input_pre_scaling_factor)\n",
    "        hidden_scaled_quant.append(model_scaled.quant_feat(hidden_scaled[-1]))\n",
    "        #print(\"hidden_scaled_quant\",hidden_scaled_quant[-1][0,0:10])\n",
    "        #print(\"hidden_scaled_quant\",hidden_scaled_quant[-1][0,0:10]*saved_alpha_u/(input_pre_scaling_factor*model_scaled.quant_feat.pre_scaling_factor))\n",
    "        print(\"diff_hidden_scaled\",torch.norm(hidden[-2]-hidden_scaled[-1]*saved_alpha_u/input_pre_scaling_factor))\n",
    "        print(\"diff_hidden_quant\",torch.norm(hidden[-2]-hidden_scaled_quant[-1]*saved_alpha_u/(input_pre_scaling_factor*model_scaled.quant_feat.pre_scaling_factor)))\n",
    "        diff_hidden_scaled = (hidden_scaled_quant[-1]/model_scaled.quant_feat.pre_scaling_factor-hidden_scaled[-1])*saved_alpha_u/input_pre_scaling_factor\n",
    "        print(\"quant diff hidden\",torch.norm(diff_hidden_scaled).detach().numpy(),torch.min(torch.abs(diff_hidden_scaled)).detach().numpy(),torch.mean(torch.abs(diff_hidden_scaled)).detach().numpy(),torch.max(torch.abs(diff_hidden_scaled)).detach().numpy(),diff_hidden_scaled.shape)\n",
    "        #print(\"hidden_scaled_reccccc\",model.recurrent_layer(hidden_scaled_quant[-1]*saved_alpha_u/(input_pre_scaling_factor*model_scaled.quant_feat.pre_scaling_factor))[0,0:10])\n",
    "        #print(\"hidden_scaled_reccccc1\",model.recurrent_layer(hidden_scaled_quant[-1]/model_scaled.quant_feat.pre_scaling_factor)[0,0:10]*saved_alpha_u/input_pre_scaling_factor)\n",
    "        #print(\"hidden_scaled_rec\",model_scaled.recurrent_layer(hidden_scaled_quant[-1])[0,0:10]*saved_alpha_u/input_pre_scaling_factor)\n",
    "        #print(\"hidden_scaled_rec\",model_scaled.recurrent_layer(model_scaled.quant_feat(hidden_scaled[-1]))[0,0:10]*saved_alpha_u/input_pre_scaling_factor)\n",
    "        hidden_scaled.append(model_scaled.input_layer(model_scaled.quant_input(x0[0:1,ii]))+model_scaled.recurrent_layer(model_scaled.quant_feat(hidden_scaled[-1])))\n",
    "        hidden_scaled_rec.append(model_scaled.recurrent_layer(model_scaled.quant_feat(hidden_scaled[-1]))) \n",
    "    \n",
    "    hidden_scaled_input.append(model_scaled.input_layer(model_scaled.quant_input(x0[0:1,ii])))\n",
    "    #print(\"hidden_scaled_rec\",hidden_scaled_rec[-1]*saved_alpha_u/input_pre_scaling_factor)\n",
    "    print(\"diff h_t\",torch.norm(hidden[-1]-hidden_scaled[-1]*saved_alpha_u/input_pre_scaling_factor))\n",
    "    print(\"h_t, min,mean,max,norm\",hidden[-1].min().detach().numpy(),hidden[-1].mean().detach().numpy(),hidden[-1].max().detach().numpy(),torch.norm(hidden[-1]).detach().numpy())\n",
    "    #print(\"diff h_t\",hidden[-1][0,0:10])\n",
    "    #print(\"diff h_t\",hidden_scaled[-1][0,0:10])\n",
    "    #print(\"diff h_t\",hidden_scaled[-1][0,0:10]*saved_alpha_u/input_pre_scaling_factor)\n",
    "    #if ii>2:\n",
    "    #    aaa\n",
    "    if 'activation.b' in new_weights:\n",
    "        print(\"diff h_t+bias\",torch.norm(hidden[-1]+model.activation.b-(hidden_scaled[-1]+model_scaled.activation.b)*saved_alpha_u/input_pre_scaling_factor))\n",
    "    #NO ACTIV IN SSM hidden[-1] = model.activation(hidden[-1])\n",
    "    #NO ACTIV IN SSM hidden_scaled[-1] = model_scaled.activation(hidden_scaled[-1])\n",
    "    print(\"activ\",torch.norm(hidden[-1]-hidden_scaled[-1]*saved_alpha_u/input_pre_scaling_factor),torch.norm(hidden[-1]-hidden_scaled[-1]*saved_alpha_u/input_pre_scaling_factor)/torch.norm(hidden[-1]),torch.max(torch.abs(hidden[-1]-hidden_scaled[-1]*saved_alpha_u/input_pre_scaling_factor)))\n",
    "    #print(\"activ value\",hidden[-1],hidden_scaled[-1]*saved_alpha_u/input_pre_scaling_factor)\n",
    "    #print(\"activ valueq\",hidden_scaled[-1],model_scaled.quant_feat(hidden_scaled[-1])/model_scaled.quant_feat.pre_scaling_factor)\n",
    "    print(\"activ quantif\",torch.norm(hidden[-1]-model_scaled.quant_feat(hidden_scaled[-1])/model_scaled.quant_feat.pre_scaling_factor*saved_alpha_u/input_pre_scaling_factor))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QSSM(\n",
      "  (activation): ReLU()\n",
      "  (input_layer): QLinear(in_features=10, out_features=512, bias=False)\n",
      "  (recurrent_layer): QLinear(in_features=512, out_features=512, bias=True)\n",
      "  (output_layer): Linear(in_features=512, out_features=9, bias=True)\n",
      "  (quant_input): _Quantize_stats()\n",
      "  (quant_feat): _Quantize_stats()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 66.1710,  90.4001,  83.2699, 118.5345,  96.8048,  83.4471,  87.4061,\n",
      "          90.6219,  92.2442]], grad_fn=<SliceBackward0>)\n",
      "preds tensor([[ 60.6146, 121.7774,  83.6743,  91.0054,  93.0944,  87.9002,  88.1035,\n",
      "          91.6217,  88.7217],\n",
      "        [ 62.4902,  92.1335,  90.6568,  96.9405,  85.4834, 121.6068,  93.6237,\n",
      "          86.7393,  94.0797],\n",
      "        [ 77.5797,  81.6285, 102.6577,  82.3822,  82.1084,  82.4381,  80.7235,\n",
      "          82.3260,  80.8269],\n",
      "        [ 78.1954,  82.8082,  71.7577,  85.6764, 111.2355,  74.9758,  83.6806,\n",
      "          76.1679,  81.5955],\n",
      "        [ 64.0168,  96.0230,  94.8161,  87.6663,  99.3767,  88.2571, 125.3367,\n",
      "          97.8592,  90.0806],\n",
      "        [ 72.9442,  93.6363,  90.9010,  95.0775,  85.7783,  93.8768,  93.3043,\n",
      "         112.5697,  87.6182],\n",
      "        [ 58.2644, 100.9028, 117.8342,  92.7372,  94.5905,  94.7654,  98.1885,\n",
      "          93.7455,  91.8851],\n",
      "        [ 72.1435, 121.2415,  84.1608,  80.8647,  86.8823,  81.7435,  87.8093,\n",
      "          82.1826,  81.5950],\n",
      "        [ 53.0322,  94.7370,  95.4239, 119.4778,  91.7915,  94.4395,  91.0644,\n",
      "          98.2267,  95.4373],\n",
      "        [ 66.1710,  90.4001,  83.2699, 118.5345,  96.8048,  83.4471,  87.4061,\n",
      "          90.6219,  92.2442]], grad_fn=<SliceBackward0>)\n",
      "tensor([1, 5, 2, 4, 6, 7, 2, 1, 3, 3])\n",
      "preds tensor([1, 5, 2, 4, 6, 7, 2, 1, 3, 3])\n",
      "torch.Size([1000, 9]) torch.Size([1000, 9])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "preds = []\n",
    "if config.model['manytomany']:\n",
    "    for ii,hh in enumerate(hidden_scaled): #zip(hidden_scaled):\n",
    "        #print(hh.shape)\n",
    "        #print(hh[:,0:10])\n",
    "        preds.append(model_scaled.output_layer(model_scaled.activation(hh)))\n",
    "    nsmples = 10\n",
    "else:\n",
    "    preds.append(model_scaled.output_layer(model_scaled.activation(hidden_scaled[-1])))\n",
    "    nsmples = 1\n",
    "    '''if ii==2:\n",
    "        break'''\n",
    "    #preds.append(model_scaled.output_layer(hh[0]))\n",
    "ypred = model_scaled(x0.to(device))\n",
    "preds = torch.concat(preds,0)\n",
    "print(ypred[-1:])    \n",
    "print(\"preds\",preds[-nsmples:])\n",
    "print(torch.argmax(ypred[-nsmples:],dim=-1))    \n",
    "print(\"preds\",torch.argmax(preds[-nsmples:],dim=-1))\n",
    "print(ypred.shape,preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 max 2\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0.) tensor(0.) tensor(0.)\n",
      "1 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "1 max 1\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "8 max 11\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "11 max 12\n"
     ]
    }
   ],
   "source": [
    "vvin = verify_and_round(model_scaled.recurrent_layer.weight, config.model['num_bits'])\n",
    "\n",
    "for hh, hhin, hhq, hhrec in zip(hidden_scaled,hidden_scaled_input, hidden_scaled_quant,hidden_scaled_rec):\n",
    "    vvin = hhin*(2**(config.model['num_bits']-1+shift_input)) #+model_scaled.quant_input.num_bits-1)) \n",
    "    print(\"hhin \",torch.norm(vvin.round() - vvin), torch.min(vvin.round() - vvin), torch.max(vvin.round() - vvin))\n",
    "    vvin = verify_and_round(hhin, config.model['num_bits']-1+shift_input)\n",
    "    vvq = hhq*(2**(model_scaled.quant_input.num_bits-1)) \n",
    "    #vvq = hhq*(2**(shift_input+model_scaled.quant_input.num_bits-1)*model_scaled.quant_input.max_absolute) \n",
    "    print(\"hhq \",torch.norm(vvq.round() - vvq), torch.min(vvq.round() - vvq), torch.max(vvq.round() - vvq))\n",
    "    vvq = verify_and_round(hhq, model_scaled.quant_feat.num_bits-1)  # +1 for multiply by 2 in model_scaled.quant_input.max_absolute\n",
    "    vvrec = hhrec*(2**(config.model['num_bits']-1+model_scaled.quant_feat.num_bits-1)) \n",
    "    print(\"hhrec \",torch.norm(vvrec.round() - vvrec), torch.min(vvrec.round() - vvrec), torch.max(vvrec.round() - vvrec))\n",
    "    vvrec = verify_and_round(hhrec, config.model['num_bits']-1+model_scaled.quant_feat.num_bits-1)\n",
    "    vv = hh*(2**(config.model['num_bits']-1+model_scaled.quant_feat.num_bits-1)) \n",
    "    print(\"hh \",torch.norm(vv.round() - vv), torch.min(vv.round() - vv), torch.max(vv.round() - vv))\n",
    "    vv = verify_and_round(hh, config.model['num_bits']-1+model_scaled.quant_feat.num_bits-1)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old stuf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use bjorck1\n",
      "ModuleDict(\n",
      "  (weight): ParametrizationList(\n",
      "    (0): _SpectralNorm()\n",
      "    (1): _BjorckNorm()\n",
      "    (2): _Quantize()\n",
      "  )\n",
      ")\n",
      "True\n",
      "bjorckbbbbbbbbbbbbbbbbbbbbbbbbbbb\n",
      "None {'test_loss': 9.237733320333064e-05}\n",
      "use bjorck1\n",
      "ModuleDict(\n",
      "  (weight): ParametrizationList(\n",
      "    (0): _SpectralNorm()\n",
      "    (1): _BjorckNorm()\n",
      "    (2): _Quantize()\n",
      "  )\n",
      ")\n",
      "True\n",
      "bjorckbbbbbbbbbbbbbbbbbbbbbbbbbbb\n",
      "6 {'test_loss': 0.9153663516044617}\n",
      "use bjorck1\n",
      "ModuleDict(\n",
      "  (weight): ParametrizationList(\n",
      "    (0): _SpectralNorm()\n",
      "    (1): _BjorckNorm()\n",
      "    (2): _Quantize()\n",
      "  )\n",
      ")\n",
      "True\n",
      "bjorckbbbbbbbbbbbbbbbbbbbbbbbbbbb\n",
      "8 {'test_loss': 0.02758418396115303}\n",
      "use bjorck1\n",
      "ModuleDict(\n",
      "  (weight): ParametrizationList(\n",
      "    (0): _SpectralNorm()\n",
      "    (1): _BjorckNorm()\n",
      "    (2): _Quantize()\n",
      "  )\n",
      ")\n",
      "True\n",
      "bjorckbbbbbbbbbbbbbbbbbbbbbbbbbbb\n",
      "10 {'test_loss': 0.00037850963417440653}\n",
      "use bjorck1\n",
      "ModuleDict(\n",
      "  (weight): ParametrizationList(\n",
      "    (0): _SpectralNorm()\n",
      "    (1): _BjorckNorm()\n",
      "    (2): _Quantize()\n",
      "  )\n",
      ")\n",
      "True\n",
      "bjorckbbbbbbbbbbbbbbbbbbbbbbbbbbb\n",
      "12 {'test_loss': 9.876950934994966e-05}\n",
      "None {'test_loss': 9.237733320333064e-05}\n",
      "6 {'test_loss': 0.9153663516044617}\n",
      "8 {'test_loss': 0.02758418396115303}\n",
      "10 {'test_loss': 0.00037850963417440653}\n",
      "12 {'test_loss': 9.876950934994966e-05}\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "for qb in [None,6,8,10,12]:\n",
    "    config.model['num_bits_feat'] = qb #10 #12\n",
    "\n",
    "    model = make_model(**config.model).to(device)\n",
    "    model.load_state_dict(torch.load(weight_file_path))\n",
    "    if qb is not None:\n",
    "        model.quant_input.max_absolute = saved_quant_input_max_absolute  ## set it twice to get 1 as max\n",
    "        model.quant_feat.max_absolute = saved_quant_feat_max_absolute       \n",
    "    stat_test = evaluate(test_ds, test_batch_size, model, loss_fn=config.train['loss_fn'], metrics=config.train['metrics'], kind='test', torch_device=device)\n",
    "    print(qb, stat_test)\n",
    "    results[qb] =stat_test.copy()\n",
    "\n",
    "for q in results:\n",
    "    print(q,results[q])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "config_scaled = copy.deepcopy(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_scaled.model['num_bits']  = None\n",
    "config_scaled.model['num_bits_feat'] = None\n",
    "config_scaled.model['name'] = 'QRNN'\n",
    "config_scaled.model['bias'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation': modrelu(),\n",
       " 'activation_config': {'features': 256},\n",
       " 'activation_final': None,\n",
       " 'activation_final_config': None,\n",
       " 'bias': False,\n",
       " 'bias_final': True,\n",
       " 'hidden_size': 256,\n",
       " 'input_size': 10,\n",
       " 'manytomany': True,\n",
       " 'name': 'QRNN',\n",
       " 'num_bits': None,\n",
       " 'output_size': 9,\n",
       " 'use_bjorck': True,\n",
       " 'reset_type': 'copyb',\n",
       " 'num_bits_feat': None}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_scaled.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_scaled = make_model(**config_scaled.model).to(device)\n",
    "weights = torch.load(weight_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_weights = {}\n",
    "new_weights[\"input_layer.weight\"] = 2**(config.model['num_bits']-1)*model.input_layer.qweight/ saved_alpha_u #NO_PARAM model.input_layer.weight\n",
    "new_weights['recurrent_layer.weight'] = model.recurrent_layer.weight\n",
    "new_weights['activation.b'] = 2**(config.model['num_bits']-1)*weights['activation.b']/ saved_alpha_u\n",
    "new_weights['output_layer.weight'] = saved_alpha_u*weights['output_layer.weight']/2**(config.model['num_bits']-1)\n",
    "\n",
    "for kk in weights.keys():\n",
    "    if (kk not in new_weights) and ('input_layer' not in kk) and ('recurrent_layer' not in kk):\n",
    "        new_weights[kk] = weights[kk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_layer.weight', 'recurrent_layer.weight', 'activation.b', 'output_layer.weight', 'output_layer.bias'])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_weights.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "weights['input_layer.parametrizations.weight.original'] = 2**(config.model['num_bits']-1)*model.input_layer.weight/ saved_alpha_u\n",
    "weights['activation.b'] = 2**(config.model['num_bits']-1)*weights['activation.b']/ saved_alpha_u\n",
    "print(weights['input_layer.parametrizations.weight.original'].unique())\n",
    "weights['output_layer.weight'] = saved_alpha_u*weights['output_layer.weight']/2**(config.model['num_bits']-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QRNN(\n",
       "  (activation): modrelu()\n",
       "  (input_layer): QLinear(in_features=10, out_features=256, bias=False)\n",
       "  (recurrent_layer): QLinear(in_features=256, out_features=256, bias=False)\n",
       "  (output_layer): Linear(in_features=256, out_features=9, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_scaled.load_state_dict(new_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(iter(test_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model(x[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_scaled = model_scaled(x[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 6.6481, -2.6134, -2.7067, -2.9618, -2.5944, -3.0028, -2.7067, -3.3808,\n",
       "        -2.5686], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_scaled[0,1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0199, grad_fn=<CopyBackwards>)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(torch.abs(preds[0,:500,:]-preds_scaled[0,:500,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_loss': 9.224341920344159e-05}\n"
     ]
    }
   ],
   "source": [
    "stat_test = evaluate(test_ds, test_batch_size, model_scaled, loss_fn=config.train['loss_fn'], metrics=config.train['metrics'], kind='test', torch_device=device)\n",
    "print(stat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = x[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0., grad_fn=<CopyBackwards>)\n",
      "tensor(0., grad_fn=<CopyBackwards>)\n",
      "tensor(1.5130e-05, grad_fn=<CopyBackwards>)\n",
      "tensor(0., grad_fn=<CopyBackwards>)\n",
      "tensor(1.5149e-05, grad_fn=<CopyBackwards>)\n",
      "tensor(2.7136e-05, grad_fn=<CopyBackwards>)\n",
      "tensor(0., grad_fn=<CopyBackwards>)\n",
      "tensor(2.7172e-05, grad_fn=<CopyBackwards>)\n",
      "tensor(3.9127e-05, grad_fn=<CopyBackwards>)\n"
     ]
    }
   ],
   "source": [
    "hidden = []\n",
    "hidden_scaled = []\n",
    "for ii in range(3):\n",
    "    print(torch.norm(model.input_layer(x0[0:1,ii])-model_scaled.input_layer(x0[0:1,ii])*saved_alpha_u/2**(config.model['num_bits']-1)))\n",
    "    if len(hidden)==0:\n",
    "        hidden.append(model.input_layer(x0[0:1,ii]))\n",
    "        hidden_scaled.append(model_scaled.input_layer(x0[0:1,ii]))\n",
    "    else:\n",
    "        hidden.append(model.input_layer(x0[0:1,ii])+model.recurrent_layer(hidden[-1]))\n",
    "        hidden_scaled.append(model_scaled.input_layer(x0[0:1,ii])+model_scaled.recurrent_layer(hidden_scaled[-1]))\n",
    "    print(torch.norm(hidden[-1]-hidden_scaled[-1]*saved_alpha_u/2**(config.model['num_bits']-1)))\n",
    "    hidden[-1] = model.activation(hidden[-1])\n",
    "    hidden_scaled[-1] = model_scaled.activation(hidden_scaled[-1])\n",
    "    print(torch.norm(hidden[-1]-hidden_scaled[-1]*saved_alpha_u/2**(config.model['num_bits']-1)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_saved_quant_feat_max_absolute =  2048 #2**(config.model['num_bits']-1)*saved_quant_feat_max_absolute/saved_alpha_u\n",
    "#1042.9763707232705"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2048\n"
     ]
    }
   ],
   "source": [
    "print(new_saved_quant_feat_max_absolute)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expe with nor by 2.alpha_U only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_values(vv, num_bits):\n",
    "    vv = vv*2**num_bits\n",
    "    assert torch.max(torch.abs(vv.round() - vv))<1e-5\n",
    "    vv = vv.round()\n",
    "    vv = vv/2**num_bits\n",
    "    return vv\n",
    "\n",
    "def verify_round_max(vv, num_bits):\n",
    "    eval_numbits = 0\n",
    "    for ss in range(num_bits):\n",
    "        vv = vv*2\n",
    "        if torch.max(torch.abs(vv.round() - vv))<1e-5:\n",
    "            eval_numbits = ss+1\n",
    "            break\n",
    "    assert eval_numbits > 0\n",
    "    print(eval_numbits,\"max\",num_bits)\n",
    "    return eval_numbits\n",
    "    \n",
    "  \n",
    "def verify_and_round(vv, num_bits):  \n",
    "    eval_numbits = verify_round_max(vv, num_bits)\n",
    "    return round_values(vv, eval_numbits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.load(weight_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.296505792551102\n"
     ]
    }
   ],
   "source": [
    "new_weights_normalphaU = {}\n",
    "requant_input_weight = round_values(model.input_layer.qweight/ (2*saved_alpha_u), config.model['num_bits']) #NO_PARAM model.input_layer.weight\n",
    "new_weights_normalphaU[\"input_layer.weight\"] = requant_input_weight #model.input_layer.weight/ (2*saved_alpha_u)  \n",
    "#new_weights_normalphaU['recurrent_layer.weight'] = model.recurrent_layer.weight / saved_alpha_w ## compensate alpha_w in hidden quantization\n",
    "#requant_recurrent_weight = round_values(model.recurrent_layer.weight, config.model['num_bits'])\n",
    "new_weights_normalphaU['recurrent_layer.weight'] = model.recurrent_layer.weight  \n",
    "scaled_quant_feat_max_absolute  = saved_quant_feat_max_absolute/(2*saved_alpha_u)\n",
    "print(scaled_quant_feat_max_absolute) #16.296505792551102\n",
    "new_quant_feat_max_absolute  = 16 #saved_alpha_w*saved_quant_feat_max_absolute/(2*saved_alpha_u)\n",
    "#new_quant_feat_max_absolute  = scaled_quant_feat_max_absolute #8/saved_alpha_w #saved_alpha_w*saved_quant_feat_max_absolute/(2*saved_alpha_u)\n",
    "new_weights_normalphaU['activation.b'] = weights['activation.b']/ (2*saved_alpha_u)\n",
    "new_weights_normalphaU['output_layer.weight'] = 2*saved_alpha_u*weights['output_layer.weight']\n",
    "\n",
    "for kk in weights.keys():\n",
    "    if (kk not in new_weights_normalphaU) and ('input_layer' not in kk) and ('recurrent_layer' not in kk):\n",
    "        new_weights_normalphaU[kk] = weights[kk]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### verif is U and W are in Q_{nbits-1} (+ factor 2 for U) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input  tensor(0., grad_fn=<CopyBackwards>)\n",
      "recurrent  tensor(8.7021e-05, grad_fn=<CopyBackwards>)\n"
     ]
    }
   ],
   "source": [
    "iww = 2**(config.model['num_bits']-1)*2*new_weights_normalphaU[\"input_layer.weight\"].unique()\n",
    "print(\"input \",torch.norm(iww.round() - iww))\n",
    "\n",
    "iww = 2**(config.model['num_bits']-1)*new_weights_normalphaU['recurrent_layer.weight']/saved_alpha_w\n",
    "#print(iww)\n",
    "print(\"recurrent \",torch.norm(iww.round() - iww))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "config_scaled = copy.deepcopy(config)\n",
    "\n",
    "config_scaled.model['num_bits']  = None\n",
    "config_scaled.model['num_bits_feat'] = None\n",
    "config_scaled.model['name'] = 'QRNN'\n",
    "config_scaled.model['bias'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None {'test_loss': 9.133988351095468e-05}\n",
      "6 {'test_loss': 0.900307297706604}\n",
      "8 {'test_loss': 0.024784889072179794}\n",
      "10 {'test_loss': 0.00043290978646837175}\n",
      "12 {'test_loss': 0.00010541886877035722}\n",
      "None {'test_loss': 9.133988351095468e-05}\n",
      "6 {'test_loss': 0.900307297706604}\n",
      "8 {'test_loss': 0.024784889072179794}\n",
      "10 {'test_loss': 0.00043290978646837175}\n",
      "12 {'test_loss': 0.00010541886877035722}\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "for qb in [None,6,8,10,12]:\n",
    "    config_scaled.model['num_bits_feat'] = qb #10 #12\n",
    "\n",
    "    model_scaled = make_model(**config_scaled.model).to(device)\n",
    "    model_scaled.load_state_dict(new_weights_normalphaU)\n",
    "    if qb is not None:\n",
    "        model_scaled.quant_input.max_absolute = saved_quant_input_max_absolute  ## set it twice to get 1 as max\n",
    "        model_scaled.quant_feat.max_absolute = new_quant_feat_max_absolute       \n",
    "        #model_scaled.quant_feat.pre_scaling_factor = saved_alpha_w       \n",
    "    stat_test = evaluate(test_ds, test_batch_size, model_scaled, loss_fn=config.train['loss_fn'], metrics=config.train['metrics'], kind='test', torch_device=device)\n",
    "    print(qb, stat_test)\n",
    "    results[qb] = stat_test.copy()\n",
    "\n",
    "for q in results:\n",
    "    print(q,results[q])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expe with nor by 2.alpha_U , and integration of alpha_W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "weights = torch.load(weight_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.296505792551102\n",
      "4.055123978191649\n"
     ]
    }
   ],
   "source": [
    "new_weights = {}\n",
    "requant_input_weight = round_values(model.input_layer.qweight/ (2*saved_alpha_u), config.model['num_bits']) #NO_PARAM model.input_layer.weight\n",
    "new_weights[\"input_layer.weight\"] = requant_input_weight #model.input_layer.weight/ (2*saved_alpha_u)\n",
    "requant_recurrent_weight = round_values(model.recurrent_layer.weight/ saved_alpha_w, config.model['num_bits'])\n",
    "new_weights['recurrent_layer.weight'] = requant_recurrent_weight #model.recurrent_layer.weight / saved_alpha_w ## compensate alpha_w in hidden quantization\n",
    "#new_weights['recurrent_layer.weight'] = model.recurrent_layer.weight  ## compensate alpha_w in hidden quantization\n",
    "scaled_quant_feat_max_absolute  = saved_quant_feat_max_absolute/(2*saved_alpha_u)\n",
    "print(scaled_quant_feat_max_absolute) #saved_quant_feat_max_absolute*saved_alpha_w/(2*saved_alpha_u))\n",
    "print(scaled_quant_feat_max_absolute*saved_alpha_w)  #4.055123978191649\n",
    "new_quant_feat_max_absolute  = 4 #saved_alpha_w*saved_quant_feat_max_absolute/(2*saved_alpha_u)\n",
    "#new_quant_feat_max_absolute  = scaled_quant_feat_max_absolute #8/saved_alpha_w #saved_alpha_w*saved_quant_feat_max_absolute/(2*saved_alpha_u)\n",
    "new_weights['activation.b'] = weights['activation.b']/ (2*saved_alpha_u)\n",
    "new_weights['output_layer.weight'] = 2*saved_alpha_u*weights['output_layer.weight']\n",
    "\n",
    "for kk in weights.keys():\n",
    "    if (kk not in new_weights) and ('input_layer' not in kk) and ('recurrent_layer' not in kk):\n",
    "        new_weights[kk] = weights[kk]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### verif is U and W are in Q_{nbits-1} (+ factor 2 for U) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input  tensor(0., grad_fn=<CopyBackwards>)\n",
      "recurrent  tensor(0., grad_fn=<CopyBackwards>)\n"
     ]
    }
   ],
   "source": [
    "iww = 2**(config.model['num_bits']-1)*2*new_weights[\"input_layer.weight\"].unique()\n",
    "print(\"input \",torch.norm(iww.round() - iww))\n",
    "\n",
    "iww = 2**(config.model['num_bits']-1)*new_weights['recurrent_layer.weight']\n",
    "#print(iww)\n",
    "print(\"recurrent \",torch.norm(iww.round() - iww))\n",
    "#print(\"recurrent \",(iww.round() - iww).unique())\n",
    "#print(torch.max(model.recurrent_layer.parametrizations['weight'][1](model.recurrent_layer.parametrizations['weight'].original)))\n",
    "#print(torch.min(model.recurrent_layer.parametrizations['weight'][1](model.recurrent_layer.parametrizations['weight'].original)))\n",
    "#vvv = 2**(config.model['num_bits']-1)*model.recurrent_layer.weight / saved_alpha_w\n",
    "#print((vvv.round() - vvv).unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "config_scaled = copy.deepcopy(config)\n",
    "\n",
    "config_scaled.model['num_bits']  = None\n",
    "config_scaled.model['num_bits_feat'] = None\n",
    "config_scaled.model['name'] = 'QRNN'\n",
    "config_scaled.model['bias'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None {'test_loss': 9.133988351095468e-05}\n",
      "6 {'test_loss': 0.8989794850349426}\n",
      "8 {'test_loss': 0.028380531817674637}\n",
      "10 {'test_loss': 0.0004319324507378042}\n",
      "12 {'test_loss': 0.00010572293831501156}\n",
      "None {'test_loss': 9.133988351095468e-05}\n",
      "6 {'test_loss': 0.8989794850349426}\n",
      "8 {'test_loss': 0.028380531817674637}\n",
      "10 {'test_loss': 0.0004319324507378042}\n",
      "12 {'test_loss': 0.00010572293831501156}\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "for qb in [None,6,8,10,12]:\n",
    "    config_scaled.model['num_bits_feat'] = qb #10 #12\n",
    "    model_scaled = make_model(**config_scaled.model).to(device)\n",
    "    if qb is None:   # No rescaling of h with qb=None\n",
    "        model_scaled.load_state_dict(new_weights_normalphaU)\n",
    "    else:\n",
    "        model_scaled.load_state_dict(new_weights)\n",
    "    if qb is not None:\n",
    "        model_scaled.quant_input.max_absolute = saved_quant_input_max_absolute  ## set it twice to get 1 as max\n",
    "        model_scaled.quant_feat.max_absolute = new_quant_feat_max_absolute       \n",
    "        model_scaled.quant_feat.pre_scaling_factor = saved_alpha_w       \n",
    "    stat_test = evaluate(test_ds, test_batch_size, model_scaled, loss_fn=config.train['loss_fn'], metrics=config.train['metrics'], kind='test', torch_device=device)\n",
    "    print(qb, stat_test)\n",
    "    results[qb] =stat_test.copy()\n",
    "\n",
    "for q in results:\n",
    "    print(q,results[q])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expe with quantized bias + nor by 2.alpha_U , and integration of alpha_W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.load(weight_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.296505792551102\n",
      "4.055123978191649\n"
     ]
    }
   ],
   "source": [
    "new_weights = {}\n",
    "requant_input_weight = round_values(model.input_layer.qweight/ (2*saved_alpha_u), config.model['num_bits']) #NO_PARAM model.input_layer.weight\n",
    "new_weights[\"input_layer.weight\"] = requant_input_weight #model.input_layer.weight/ (2*saved_alpha_u)\n",
    "requant_recurrent_weight = round_values(model.recurrent_layer.weight/ saved_alpha_w, config.model['num_bits'])\n",
    "new_weights['recurrent_layer.weight'] = requant_recurrent_weight #model.recurrent_layer.weight / saved_alpha_w ## compensate alpha_w in hidden quantization\n",
    "#new_weights['recurrent_layer.weight'] = model.recurrent_layer.weight  ## compensate alpha_w in hidden quantization\n",
    "scaled_quant_feat_max_absolute  = saved_quant_feat_max_absolute/(2*saved_alpha_u)\n",
    "print(scaled_quant_feat_max_absolute) #saved_quant_feat_max_absolute*saved_alpha_w/(2*saved_alpha_u))\n",
    "print(scaled_quant_feat_max_absolute*saved_alpha_w)  #4.055123978191649\n",
    "new_quant_feat_max_absolute  = 4 #saved_alpha_w*saved_quant_feat_max_absolute/(2*saved_alpha_u)\n",
    "#new_quant_feat_max_absolute  = scaled_quant_feat_max_absolute #8/saved_alpha_w #saved_alpha_w*saved_quant_feat_max_absolute/(2*saved_alpha_u)\n",
    "new_weights['activation.b'] = weights['activation.b']/ (2*saved_alpha_u)\n",
    "new_weights['output_layer.weight'] = 2*saved_alpha_u*weights['output_layer.weight']\n",
    "\n",
    "for kk in weights.keys():\n",
    "    if (kk not in new_weights) and ('input_layer' not in kk) and ('recurrent_layer' not in kk):\n",
    "        new_weights[kk] = weights[kk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified modReLU\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "config_scaled = copy.deepcopy(config)\n",
    "\n",
    "config_scaled.model['num_bits']  = None\n",
    "config_scaled.model['num_bits_feat'] = None\n",
    "config_scaled.model['name'] = 'QRNN'\n",
    "config_scaled.model['bias'] = False\n",
    "config_scaled.model['activation'] = layers.modified4q_modrelu(**config_scaled.model['activation_config'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': modified4q_modrelu(), 'activation_config': {'features': 256}, 'activation_final': None, 'activation_final_config': None, 'bias': False, 'bias_final': True, 'hidden_size': 256, 'input_size': 10, 'manytomany': True, 'name': 'QRNN', 'num_bits': None, 'output_size': 9, 'use_bjorck': True, 'reset_type': 'copyb', 'num_bits_feat': None}\n"
     ]
    }
   ],
   "source": [
    "print(config_scaled.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None {'test_loss': 9.226499969372526e-05}\n",
      "6 {'test_loss': 0.7314143776893616}\n",
      "8 {'test_loss': 0.030396342277526855}\n",
      "10 {'test_loss': 0.00039048207690939307}\n",
      "12 {'test_loss': 0.00010659168765414506}\n",
      "None {'test_loss': 9.226499969372526e-05}\n",
      "6 {'test_loss': 0.7314143776893616}\n",
      "8 {'test_loss': 0.030396342277526855}\n",
      "10 {'test_loss': 0.00039048207690939307}\n",
      "12 {'test_loss': 0.00010659168765414506}\n"
     ]
    }
   ],
   "source": [
    "from quantized_layers import _Quantize_stats\n",
    "results = {}\n",
    "for qb in [None,6,8,10,12]:\n",
    "    config_scaled.model['num_bits_feat'] = qb #10 #12\n",
    "    model_scaled = make_model(**config_scaled.model).to(device)\n",
    "    if qb is None:   # No rescaling of h with qb=None\n",
    "        model_scaled.load_state_dict(new_weights_normalphaU)\n",
    "    else:\n",
    "        model_scaled.load_state_dict(new_weights)\n",
    "    if qb is not None:\n",
    "        model_scaled.quant_input.max_absolute = saved_quant_input_max_absolute  ## set it twice to get 1 as max\n",
    "        model_scaled.quant_feat.max_absolute = new_quant_feat_max_absolute       \n",
    "        model_scaled.quant_feat.pre_scaling_factor = saved_alpha_w     \n",
    "        #quantize biases\n",
    "        quant_bias = _Quantize_stats(weight=None,num_bits= model_scaled.quant_feat.num_bits, max_absolute= model_scaled.quant_feat.max_absolute)\n",
    "        #print(model_scaled.activation.b)\n",
    "        #print(quant_bias(model_scaled.activation.b))\n",
    "        model_scaled.activation.b.data = quant_bias(model_scaled.activation.b) \n",
    "    stat_test = evaluate(test_ds, test_batch_size, model_scaled, loss_fn=config.train['loss_fn'], metrics=config.train['metrics'], kind='test', torch_device=device)\n",
    "    print(qb, stat_test)\n",
    "    results[qb] =stat_test.copy()\n",
    "\n",
    "for q in results:\n",
    "    print(q,results[q])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(iter(test_ds))\n",
    "x0 = x[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation': ReLU(),\n",
       " 'activation_config': None,\n",
       " 'activation_final': None,\n",
       " 'activation_final_config': None,\n",
       " 'basic_block': 4,\n",
       " 'bias': True,\n",
       " 'bias_final': True,\n",
       " 'givens': 2,\n",
       " 'hidden_size': 512,\n",
       " 'input_size': 10,\n",
       " 'manytomany': True,\n",
       " 'multiparameters': False,\n",
       " 'name': 'QSSM',\n",
       " 'num_bits': None,\n",
       " 'output_size': 9,\n",
       " 'seed': 171,\n",
       " 'single_layer': True,\n",
       " 'num_bits_feat': 12}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_scaled.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'training': False, '_parameters': OrderedDict(), '_buffers': OrderedDict(), '_non_persistent_buffers_set': set(), '_backward_hooks': OrderedDict(), '_is_full_backward_hook': None, '_forward_hooks': OrderedDict(), '_forward_pre_hooks': OrderedDict(), '_state_dict_hooks': OrderedDict(), '_load_state_dict_pre_hooks': OrderedDict(), '_modules': OrderedDict(), 'num_bits': 12, 'enforce_true_quantized': True, 'delta_m': None, 'max_absolute': 4, 'stat_max_absolute': 1e-10, 'pre_scaling_factor': 0.24883395433425903}\n"
     ]
    }
   ],
   "source": [
    "print(model_scaled.quant_feat.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0., grad_fn=<CopyBackwards>)\n"
     ]
    }
   ],
   "source": [
    "print(torch.norm(model.recurrent_layer.weight-model_scaled.recurrent_layer.weight*saved_alpha_w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "tensor(0., grad_fn=<CopyBackwards>)\n",
      "tensor(0., grad_fn=<CopyBackwards>)\n",
      "activ tensor(0.0373, grad_fn=<CopyBackwards>)\n",
      "tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]])\n",
      "tensor(0., grad_fn=<CopyBackwards>)\n",
      "tensor(0.0567, grad_fn=<CopyBackwards>)\n",
      "activ tensor(0.0588, grad_fn=<CopyBackwards>)\n",
      "tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]])\n",
      "tensor(0., grad_fn=<CopyBackwards>)\n",
      "tensor(0.0707, grad_fn=<CopyBackwards>)\n",
      "activ tensor(0.0703, grad_fn=<CopyBackwards>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]])\n",
      "tensor(0., grad_fn=<CopyBackwards>)\n",
      "tensor(0.0859, grad_fn=<CopyBackwards>)\n",
      "activ tensor(0.0871, grad_fn=<CopyBackwards>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]])\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]])\n",
      "tensor(0., grad_fn=<CopyBackwards>)\n",
      "tensor(0.0961, grad_fn=<CopyBackwards>)\n",
      "activ tensor(0.0968, grad_fn=<CopyBackwards>)\n",
      "tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]])\n",
      "tensor(0., grad_fn=<CopyBackwards>)\n",
      "tensor(0.1085, grad_fn=<CopyBackwards>)\n",
      "activ tensor(0.1125, grad_fn=<CopyBackwards>)\n",
      "tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "tensor(0., grad_fn=<CopyBackwards>)\n",
      "tensor(0.1185, grad_fn=<CopyBackwards>)\n",
      "activ tensor(0.1185, grad_fn=<CopyBackwards>)\n",
      "tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]])\n",
      "tensor(0., grad_fn=<CopyBackwards>)\n",
      "tensor(0.1257, grad_fn=<CopyBackwards>)\n",
      "activ tensor(0.1268, grad_fn=<CopyBackwards>)\n",
      "tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]])\n",
      "tensor(0., grad_fn=<CopyBackwards>)\n",
      "tensor(0.1387, grad_fn=<CopyBackwards>)\n",
      "activ tensor(0.1418, grad_fn=<CopyBackwards>)\n",
      "tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]])\n",
      "tensor(0., grad_fn=<CopyBackwards>)\n",
      "tensor(0.1442, grad_fn=<CopyBackwards>)\n",
      "activ tensor(0.1443, grad_fn=<CopyBackwards>)\n"
     ]
    }
   ],
   "source": [
    "hidden = []\n",
    "hidden_scaled = []\n",
    "hidden_scaled_input = []\n",
    "hidden_scaled_quant = []\n",
    "hidden_scaled_rec = []\n",
    "for ii in range(10):\n",
    "    print(x0[0:1,ii])\n",
    "    print(model_scaled.quant_input(x0[0:1,ii]))\n",
    "    #print(model.input_layer(x0[0:1,ii]))\n",
    "    #print(model_scaled.input_layer(x0[0:1,ii])*2*saved_alpha_u)\n",
    "    print(torch.norm(model.input_layer(x0[0:1,ii])-model_scaled.input_layer(model_scaled.quant_input(x0[0:1,ii]))*2*saved_alpha_u))\n",
    "    \n",
    "    if len(hidden)==0:\n",
    "        hidden.append(model.input_layer(x0[0:1,ii]))\n",
    "        hidden_scaled.append(model_scaled.input_layer(model_scaled.quant_input(x0[0:1,ii])))\n",
    "        #print(hidden_scaled[0])\n",
    "        #print(hidden_scaled[0]*2**(config.model['num_bits']))\n",
    "        hidden_scaled_quant.append(torch.zeros_like(hidden_scaled[0])) # fake\n",
    "        hidden_scaled_rec.append(torch.zeros_like(hidden_scaled[0])) # fake\n",
    "    else:\n",
    "        #print(model.recurrent_layer(hidden[-1]))\n",
    "        #print(model_scaled.recurrent_layer(hidden_scaled[-1])*2*saved_alpha_u/4.0)\n",
    "        #print(hidden_scaled[-1])\n",
    "        #print(model_scaled.quant_feat(hidden_scaled[-1])/saved_alpha_w)\n",
    "        '''print(\"h -sh\",torch.norm(hidden[-1]-hidden_scaled[-1]*2*saved_alpha_u))\n",
    "        print(hidden[-1][0,0:10])\n",
    "        print(hidden_scaled[-1][0,0:10]*2*saved_alpha_u)\n",
    "        print(model_scaled.quant_feat(hidden_scaled[-1])[0,0:10]*2*saved_alpha_u/saved_alpha_w)A\n",
    "        print(2**(model_scaled.quant_feat.num_bits-1))\n",
    "        print(model_scaled.quant_feat.max_absolute)\n",
    "        print(hidden_scaled[-1])\n",
    "        #hidden_scaled[-1][0,1] = -8\n",
    "        print(2**(model_scaled.quant_feat.num_bits-1)*model_scaled.quant_feat(hidden_scaled[-1])/model_scaled.quant_feat.max_absolute)\n",
    "        print(torch.abs(hidden[-1]-hidden_scaled[-1]*2*saved_alpha_u)[0,0:10])\n",
    "        print(torch.abs(hidden[-1]-model_scaled.quant_feat(hidden_scaled[-1])*2*saved_alpha_u/saved_alpha_w)[0,0:10])\n",
    "        print(torch.abs(hidden[-1]-model_scaled.quant_feat(hidden_scaled[-1])*2*saved_alpha_u/saved_alpha_w)[0,0:10]-torch.abs(hidden[-1]-hidden_scaled[-1]*2*saved_alpha_u)[0,0:10])\n",
    "        print((torch.square(hidden[-1]-model_scaled.quant_feat(hidden_scaled[-1])*2*saved_alpha_u/saved_alpha_w)-torch.square(hidden[-1]-hidden_scaled[-1]*2*saved_alpha_u)).min())\n",
    "        print((torch.square(hidden[-1]-model_scaled.quant_feat(hidden_scaled[-1])*2*saved_alpha_u/saved_alpha_w)-torch.square(hidden[-1]-hidden_scaled[-1]*2*saved_alpha_u)).mean())\n",
    "        print((torch.square(hidden[-1]-model_scaled.quant_feat(hidden_scaled[-1])*2*saved_alpha_u/saved_alpha_w)-torch.square(hidden[-1]-hidden_scaled[-1]*2*saved_alpha_u)).max())\n",
    "\n",
    "        print(torch.sum(torch.square(hidden[-1]-model_scaled.quant_feat(hidden_scaled[-1])*2*saved_alpha_u/saved_alpha_w))-torch.sum(torch.square(hidden[-1]-hidden_scaled[-1]*2*saved_alpha_u)))\n",
    "        #print(torch.norm(hidden_scaled[-1]-model_scaled.quant_feat(hidden_scaled[-1])/saved_alpha_w))\n",
    "        print(torch.sum(torch.square(hidden[-1]-model_scaled.quant_feat(hidden_scaled[-1])*2*saved_alpha_u/saved_alpha_w)))\n",
    "        print(torch.norm(hidden[-1]-model_scaled.quant_feat(hidden_scaled[-1])*2*saved_alpha_u/saved_alpha_w))\n",
    "        print(torch.sum(torch.square(hidden_scaled[-1]*2*saved_alpha_u-model_scaled.quant_feat(hidden_scaled[-1])*2*saved_alpha_u/saved_alpha_w)))\n",
    "        print(\"aaa\",torch.norm(hidden_scaled[-1]*2*saved_alpha_u-model_scaled.quant_feat(hidden_scaled[-1])*2*saved_alpha_u/saved_alpha_w))\n",
    "        print(torch.norm(model.recurrent_layer(hidden[-1])-model_scaled.recurrent_layer(model_scaled.quant_feat(hidden_scaled[-1]))*2*saved_alpha_u))'''\n",
    "        #print(model.recurrent_layer(hidden[-1]))\n",
    "        #print(model_scaled.recurrent_layer(model_scaled.quant_feat(hidden_scaled[-1]))*2*saved_alpha_u)\n",
    "        hidden.append(model.input_layer(x0[0:1,ii])+model.recurrent_layer(hidden[-1]))\n",
    "        hidden_scaled.append(model_scaled.input_layer(model_scaled.quant_input(x0[0:1,ii]))+model_scaled.recurrent_layer(model_scaled.quant_feat(hidden_scaled[-1])))\n",
    "        hidden_scaled_quant.append(model_scaled.quant_feat(hidden_scaled[-1]))\n",
    "        hidden_scaled_rec.append(model_scaled.recurrent_layer(model_scaled.quant_feat(hidden_scaled[-1]))) \n",
    "    \n",
    "    hidden_scaled_input.append(model_scaled.input_layer(model_scaled.quant_input(model_scaled.quant_input(x0[0:1,ii]))))\n",
    "    print(torch.norm(hidden[-1]-hidden_scaled[-1]*2*saved_alpha_u))\n",
    "    hidden[-1] = model.activation(hidden[-1])\n",
    "    hidden_scaled[-1] = model_scaled.activation(hidden_scaled[-1])\n",
    "    print(\"activ\",torch.norm(hidden[-1]-hidden_scaled[-1]*2*saved_alpha_u))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print(len(hidden_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 max 12\n"
     ]
    }
   ],
   "source": [
    "torch.max(torch.abs(model_scaled.activation.b))\n",
    "vvin = verify_and_round(model_scaled.activation.b, model_scaled.quant_feat.num_bits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 max 6\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "6 max 6\n",
      "hhq  tensor(0.) tensor(0.) tensor(0.)\n",
      "1 max 9\n",
      "hhrec  tensor(0.) tensor(0.) tensor(0.)\n",
      "1 max 21\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "9 max 21\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "6 max 6\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "9 max 9\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "14 max 21\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "14 max 21\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "6 max 6\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "9 max 9\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "14 max 21\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "14 max 21\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "6 max 6\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "9 max 9\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "14 max 21\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "14 max 21\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "6 max 6\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "9 max 9\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "14 max 21\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "14 max 21\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "6 max 6\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "9 max 9\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "14 max 21\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "14 max 21\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "6 max 6\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "9 max 9\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "14 max 21\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "14 max 21\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "6 max 6\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "9 max 9\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "14 max 21\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "14 max 21\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "6 max 6\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "9 max 9\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "14 max 21\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "14 max 21\n",
      "hhin  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "6 max 6\n",
      "hhq  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "9 max 9\n",
      "hhrec  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "14 max 21\n",
      "hh  tensor(0., grad_fn=<CopyBackwards>) tensor(0., grad_fn=<MinBackward1>) tensor(0., grad_fn=<MaxBackward1>)\n",
      "14 max 21\n"
     ]
    }
   ],
   "source": [
    "vvin = verify_and_round(model_scaled.recurrent_layer.weight, config.model['num_bits'])\n",
    "\n",
    "for hh, hhin, hhq, hhrec in zip(hidden_scaled,hidden_scaled_input, hidden_scaled_quant,hidden_scaled_rec):\n",
    "    vvin = hhin*(2**(config.model['num_bits'])) #+model_scaled.quant_input.num_bits-1)) \n",
    "    print(\"hhin \",torch.norm(vvin.round() - vvin), torch.min(vvin.round() - vvin), torch.max(vvin.round() - vvin))\n",
    "    vvin = verify_and_round(hhin, config.model['num_bits'])\n",
    "    vvq = hhq*(2**(model_scaled.quant_input.num_bits-1)*model_scaled.quant_input.max_absolute) \n",
    "    print(\"hhq \",torch.norm(vvq.round() - vvq), torch.min(vvq.round() - vvq), torch.max(vvq.round() - vvq))\n",
    "    vvq = verify_and_round(hhq, model_scaled.quant_input.num_bits-3)\n",
    "    vvrec = hhrec*(2**(config.model['num_bits']+model_scaled.quant_input.num_bits-1)*model_scaled.quant_input.max_absolute) \n",
    "    print(\"hhrec \",torch.norm(vvrec.round() - vvrec), torch.min(vvrec.round() - vvrec), torch.max(vvrec.round() - vvrec))\n",
    "    vvrec = verify_and_round(hhrec, config.model['num_bits']+model_scaled.quant_input.num_bits+3)\n",
    "    '''sgn = torch.sgn(hhin + hhrec)\n",
    "    toto = nn.functional.relu(torch.abs(hhin + hhrec) +model_scaled.activation.b)\n",
    "    toto2 = sgn*toto\n",
    "    print(\"toto2\")\n",
    "    toto = verify_and_round(toto2, config.model['num_bits']+model_scaled.quant_input.num_bits+3)\n",
    "    toto = model_scaled.activation(hhin+hhrec) #hhin + hhrec +model_scaled.activation.b\n",
    "    print(toto2)#torch.abs(hhin + hhrec) +model_scaled.activation.b)\n",
    "    print(toto)\n",
    "    print(torch.max(torch.abs(toto-toto2)))\n",
    "    print(\"toto\")\n",
    "    vv = toto2\n",
    "    for ss in range(10):\n",
    "        vv = vv*2\n",
    "        print(ss+1,torch.norm(vv.round() - vv), torch.min(vv.round() - vv), torch.max(vv.round() - vv))\n",
    "        if torch.max(torch.abs(vv.round() - vv))<1e-5:\n",
    "            print(\"ok\",ss+1)\n",
    "            break\n",
    "    toto = verify_and_round(toto, config.model['num_bits']+model_scaled.quant_input.num_bits+3)'''\n",
    "    vv = hh*(2**(config.model['num_bits']+model_scaled.quant_feat.num_bits-1)) \n",
    "    print(\"hh \",torch.norm(vv.round() - vv), torch.min(vv.round() - vv), torch.max(vv.round() - vv))\n",
    "    vv = verify_and_round(hh, config.model['num_bits']+model_scaled.quant_input.num_bits+3)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$U = \\alpha_U.U_k$, $U_k\\in[[-2^{k-1},2^{k-1}-1]]$\n",
    "\n",
    "$W = \\alpha_W.W_k$, $W_k\\in[[-2^{k-1},2^{k-1}-1]]$\n",
    "\n",
    "$x = x_{ka}/2^{ka-2}$  , $x_{ka}\\in{0,2^{ka-2}}$ #$[-2^{ka-1},2^{ka-1}-1]]$\n",
    "\n",
    "$h = \\alpha_h.h_{ka}$, $h_{ka}\\in[[-2^{ka-1},2^{ka-1}-1]]$\n",
    "\n",
    "$h_{t+1} = W.h + U.x = (\\alpha_h*\\alpha_W)*(W_k.h_{ka}) + (\\alpha_U/2^{ka-2})*(U_k*x_{ka})$\n",
    "\n",
    "$h_{t+1} = ReLU(h_{t+1})$\n",
    "\n",
    "On peut eviter la multiplaction par $\\alpha_U$\n",
    "\n",
    "Simplification si \n",
    "\n",
    "$1/\\alpha_h*\\alpha_W = 2^k'$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reminder fixed point notation  $Q_{l,k}$\n",
    "\n",
    "$Q_{l,k} = {p/2^{k},p\\in[[-2^{l-1},2^{l-1}-1]]}\\in [-2^{l-k-1},2^{l-k-1}[$ with $l$ bits\n",
    "\n",
    "in particular : $Q_k = Q_{k+1,k} = {p/2^{k},p\\in[[-2^k,2^k-1]]}\\in [-1,1[$ with $k+1$ bits\n",
    "\n",
    "multiplication $Q_{l,k}.Q_{l',k'} -> Q_{l+l'-1,k+k'}$\n",
    "\n",
    "$Q_{k}.Q_{k'} -> Q_{k+k'+1,k+k'}=Q_{k+k'}$\n",
    "\n",
    "Addition (has to be with the same $k$)\n",
    "\n",
    "$Q_{k} + Q_{k} -> Q_{k+2,k}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Formulation with fixed point variables\n",
    "## matrices and vectors\n",
    "$U = \\alpha_U/2^{k-1}.U_k= \\alpha_U.U'_k$, with $U'_k\\in Q_{k-1}$\n",
    "\n",
    "$W = \\alpha_W/2^{k-1}.W_k= \\alpha_W.W'_k$, with $W'_k\\in Q_{k-1}$\n",
    "\n",
    "$x(t) = 2*x_{ka}(t)$, with $x\\in\\{0,1\\}$ , $x_{ka}\\in\\{0,0.5\\}$ could be $Q_1$ (2 bits), but easier to keep it in $Q_{ka-1}$\n",
    "\n",
    "$h(t) = \\alpha_h/2^{ka-1}.h_{ka}(t)=\\alpha_h.h'_{ka}(t)$ with $h'_{ka}(t)\\in Q_{ka-1}$\n",
    "\n",
    "## recurrent computation\n",
    "\n",
    "$h(t+1) = W.h(t) + U.x(t) = (\\alpha_h*\\alpha_W)*(W'_k.h'_{ka}(t)) + (2\\alpha_U)*(U'_k*x_{ka}(t))$ with $W'_k.h'_{ka}(t)$ and $U'_k*x_{ka}(t)\\in Q_{k+ka-2}$\n",
    "\n",
    "$h(t+1) = ReLU(h(t+1))$\n",
    "\n",
    "or $h(t+1) = ModReLU(h(t+1),b) = sign(h(t+1))*ReLU(|h(t+1)|+b)$ => Could be quantized back to $\\alpha_h.h'_{ka}(t+1)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Scaling hidden vector to reduce floating point multiplications ($2\\alpha_U$)\n",
    "\n",
    "To avoid the multiplication by  par $2\\alpha_U$\n",
    "\n",
    "scaled h : $sh(t) = h(t)/(2\\alpha_U)$, $sh(t) = \\alpha_{sh}.sh'_{ka}(t)$ with $\\alpha_{sh} = \\alpha_h/(2\\alpha_U)$, and $sh'_{ka}(t)=h'_{ka}(t)$\n",
    "\n",
    "$sh(t+1) = (\\alpha_{sh}*\\alpha_W)*(W'_k.sh'_{ka}(t)) + U'_k*x_{ka}(t)$  -> Floating point\n",
    "\n",
    "$sh(t+1) = ReLU(h(t+1))$\n",
    "\n",
    "$sh(t+1) = ModReLU(h(t+1),sb), with $sb = b/(2\\alpha_U)$ => Could be quantized back to $\\alpha_{sh}.sh'_{ka}(t+1)$ (division by $\\alpha_sh$)\n",
    "\n",
    "# Removing the  floating point multiplication  $(\\alpha_{sh}*\\alpha_W)$\n",
    "set an upper bound for quantization: choose $\\alpha_h$ such as \n",
    "$(\\alpha_{sh}*\\alpha_W) = \\alpha_h\\alpha_W/(2\\alpha_U)= 2^{kal}$\n",
    "\n",
    "\n",
    " $U'_k*x_{ka}(t)$ in $Q_{k+ka-2} = Q_{k+ka-1,k+ka-2}$\n",
    "\n",
    " $W'_k.sh'_{ka}(t)$ in $Q_{k+ka-2} = Q_{k+ka-1,k+ka-2}$\n",
    "\n",
    " $2^{kal}W'_k.sh'_{ka}(t)$ in $Q_{k+ka+kal-1,k+ka-2}$\n",
    "\n",
    "$sh(t+1) \\in Q_{k+ka+kal-1,k+ka-2}$ \n",
    "ReLU or ModReLU -> **BUT** division by $\\alpha_{sh}=2^{kal}/\\alpha_W$ before quantizing  to $Q_{ka}$\n",
    "\n",
    "# Removing the  floating point division  $\\alpha_{sh}$ for quantization\n",
    "require to set  $\\alpha_{sh}=2^{kal}$\n",
    "quantize $\\alpha_W \\in Q_{kw,kw'}$\n",
    "\n",
    "\n",
    " $U'_k*x_{ka}(t)$ in $Q_{k+ka-2} = Q_{k+ka-1,k+ka-2}$\n",
    "\n",
    " $W'_k.sh'_{ka}(t)$ in $Q_{k+ka-2} = Q_{k+ka-1,k+ka-2}$\n",
    "\n",
    " $\\alpha_W.W'_k.sh'_{ka}(t)$ in $Q_{k+ka+kw-2,k+ka+kw'-2}$\n",
    "\n",
    "$sh(t+1) \\in Q_{k+ka+kw-2,k+ka+kw'-2}$ \n",
    "ReLU or ModReLU ->  shift by $\\alpha_{sh}=2^{kal}$ before quantizing  to $Q_{ka}$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deel-pt1.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
